{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "magnetic-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler#モデルの評価用に標準化する関数\n",
    "import scipy.stats#モデルの評価用に標準化する関数\n",
    "import warnings\n",
    "#from pycaret.regression import *#setupが回帰と分類で名前が同じだが機能が違うので，使う方のみインポート（後にインポートした方が使われる)\n",
    "from pycaret.classification import *#setupが回帰と分類で名前が同じだが機能が違うので，使う方のみインポート（後にインポートした方が使われる)\n",
    "#自作のモジュールのインポート\n",
    "sys.path.append(\"../..\")\n",
    "import module.master as master\n",
    "import module.modeling_scores as modeling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "recent-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "exempt-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#V4_系列の特殊処理\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/{v_b}_score/{version}/\".format(place_name=place_name,v_b=version_big,version=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-pennsylvania",
   "metadata": {},
   "source": [
    "## 関数内で使用されている関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "later-parallel",
   "metadata": {
    "code_folding": [
     0,
     73,
     80,
     107,
     122,
     138,
     144,
     156,
     164
    ]
   },
   "outputs": [],
   "source": [
    "def get_event_info(result_base_df):\n",
    "    df=result_base_df.copy()\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    df['year']=df['date'].dt.year\n",
    "    df['month']=df['date'].dt.month\n",
    "    df['day']=df['date'].dt.day\n",
    "\n",
    "    num_date=1\n",
    "    num_date_arr=[]\n",
    "    last_race_date=df['date'].values[0]#前レースの日付(処理開始時用にtarainのデータの一番初めのdateを仮に入力しておく)\n",
    "    for index,row in df.iterrows():\n",
    "        today_date=row['date']\n",
    "        if today_date==last_race_date:#同じ日のレースだったらおなじレース日を配列に追加、次の日の日付を出力（ほぼ無操作みたいなもん）\n",
    "            next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "            num_date_arr.append(num_date)\n",
    "        else:#日にちが変わった時\n",
    "            if today_date==next_date:#想定していた日付（次の日のレース）だったら,レース日を一日足して、そのレース日番号を加算\n",
    "                num_date+=1\n",
    "                num_date_arr.append(num_date)\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "                #print(next_date)\n",
    "            else:#想定していた日付でない(違う大会になった)場合はレース日をリセット\n",
    "                num_date=1\n",
    "                num_date_arr.append(num_date)\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "    df['num_date']=num_date_arr\n",
    "\n",
    "    range_races=0#大会中の取得できたレースの数\n",
    "    range_date=1#大会の開催日数\n",
    "    range_date_arr=[]\n",
    "    range_date_arr_2=[]#for文中で繰り返し上書きさせる用の配列\n",
    "    last_race_date=df['date'].values[0]#前レースの日付(処理開始時用にtrainのデータの一番初めのdateを仮に入力しておく)\n",
    "    for index,row in df.iterrows():\n",
    "        today_date=row['date']\n",
    "        if today_date==last_race_date:#同じ日のレースだったらおなじレース日を配列に追加、次の日の日付を出力（ほぼ無操作みたいなもん）\n",
    "            range_races+=1\n",
    "            next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "            #num_date_arr.append(num_date)\n",
    "        else:#日にちが変わった時\n",
    "            if today_date==next_date:#想定していた日付（次の日のレース）だったら,レース日を一日足して終了\n",
    "                range_date+=1\n",
    "                range_races+=1\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日次の日\n",
    "            else:#想定していた日付でない(違う大会になった)場合は現在のrange_dateをもとに前の大会のレースに大会開催日数を持たせる。\n",
    "\n",
    "                range_date_arr_2=[range_date]*range_races\n",
    "                for num in range_date_arr_2:\n",
    "                    range_date_arr.append(num)\n",
    "                range_races=1#大会中の取得できたレースの数\n",
    "                range_date=1#大会の開催日数\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "    range_date_arr_2=[range_date]*range_races#最後の日は日付の変わり絵が発生しないので特別処理\n",
    "    for num in range_date_arr_2:\n",
    "        range_date_arr.append(num)\n",
    "    df['range_date']=range_date_arr\n",
    "\n",
    "    #四半期カラムの作成\n",
    "    df['season']=df['month']\n",
    "    df['season']=df['season'].replace([3,4,5],'sp')#春\n",
    "    df['season']=df['season'].replace([6,7,8],'su')#夏\n",
    "    df['season']=df['season'].replace([9,10,11],'au')#秋\n",
    "    df['season']=df['season'].replace([12,1,2],'wi')#冬\n",
    "    #df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_monthly_analysis(pred_df):#予測に加えて，配当，開催情報が結合されたdfを渡すことで月ごと関連の分析を行ってくれる関数\n",
    "    cols=['month','use','get','income','income_per','num_hit','buy_hit_per','mean_income','median_income']\n",
    "    monthly_analysis_df= pd.DataFrame(columns=cols)#月別収益結果の入る箱\n",
    "    months=pred_df['month'].value_counts(sort=False).index\n",
    "    for month in months:\n",
    "        monthly_df=pred_df[pred_df['month']==month].copy()\n",
    "        use_m=100*monthly_df['pred'].sum()\n",
    "        get_m=monthly_df['gain'].sum()\n",
    "        income=get_m-use_m\n",
    "        income_per=(get_m/use_m)*100\n",
    "        \n",
    "        \n",
    "        num_hit=monthly_df['hit_flag'].sum()\n",
    "        num_pred=pred_df['pred'].sum()\n",
    "        buy_hit_per=(num_hit/num_pred)*100\n",
    "        if num_hit==0:#警告文削除用\n",
    "            mean_income=0\n",
    "            median_income=0\n",
    "        else:\n",
    "            mean_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].mean()#１回の的中あたりの平均配当\n",
    "            median_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].median()#１回の的中あたりの中央配当\n",
    "        \n",
    "        append_arr=[month,use_m,get_m,income,income_per,num_hit,buy_hit_per,mean_income,median_income]\n",
    "        append_s=pd.Series(append_arr,index=cols)\n",
    "        monthly_analysis_df=monthly_analysis_df.append(append_s, ignore_index=True)\n",
    "    return monthly_analysis_df\n",
    "\n",
    "def get_season_date(now_date):#日付(datetime型)を渡すと，その日付で購買予測を行う際に使用するデータの区間を返す関数\n",
    "    use_data_year=now_date.year\n",
    "    if (now_date.month>=1)and(now_date.month<4):\n",
    "        use_data_month=1\n",
    "    elif (now_date.month>=4)and(now_date.month<7):\n",
    "        use_data_month=4\n",
    "    elif (now_date.month>=7)and(now_date.month<10):\n",
    "        use_data_month=7\n",
    "    elif (now_date.month>=10):\n",
    "        use_data_month=10\n",
    "    else:\n",
    "        print('what???????')\n",
    "    use_data_date=datetime.datetime(year=use_data_year, month=use_data_month,day=1)\n",
    "    return use_data_date\n",
    "\n",
    "def trans_result_com(target_com,trans_base_df):#comをターゲットに合わせて0,1の二値に変換する。\n",
    "    #学習データのラベル変換==========================================================\n",
    "    trans_df=trans_base_df.copy()\n",
    "    #result_train_df=trans_base_df.copy()\n",
    "    result_arr=[0]*len(trans_df)\n",
    "    i=0\n",
    "    for result in trans_df['result_com']:#\n",
    "        if ((result==target_com)):\n",
    "            result_arr[i]=1\n",
    "        else:\n",
    "            result_arr[i]=0\n",
    "        i+=1\n",
    "    trans_df['result_com']=result_arr\n",
    "    return trans_df\n",
    "\n",
    "\n",
    "def pred_th_trans_com(pred_df,th,target_com):#指定の組のカラムのみを置換。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_{}'.format(target_com)] >= th, 'pred_{}'.format(target_com)] = 1\n",
    "    trans_df.loc[~(trans_df['pred_{}'.format(target_com)] >=  th), 'pred_{}'.format(target_com)] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_gain(pred_gain_df):#レース単位であたっているか同課の判別と、当たった場合に得られた配当金を計算する関数\n",
    "    pred_true_df=pred_gain_df[(pred_gain_df['pred']==1)&(pred_gain_df['trans_result']==1)].copy()\n",
    "    pred_true_df['hit']=1\n",
    "    calc_base_df=pred_gain_df.copy()\n",
    "    calc_base_df['hit']=pred_true_df['hit']\n",
    "    calc_base_df['gain']=pred_true_df['money']\n",
    "    calc_base_df=calc_base_df.fillna(0)\n",
    "    #\n",
    "    #calc_base_df:予測、変換積みの結果、実際の結果、配当金、収益をすべて表したdf,合計操作は行っていない。\n",
    "    #\n",
    "    return calc_base_df\n",
    "\n",
    "def check_pred_arr(pred1_df,pred2_df):#カラムの中身が同じか比較する関数\n",
    "    pred_1_vals=[pred1_df[col] for col in pred1_df.columns]\n",
    "    pred_2_vals=[pred2_df[col] for col in pred2_df.columns]\n",
    "    for col_name1,col1,col_name2,col2 in zip(pred1_df.columns,pred_1_vals,pred2_df.columns,pred_2_vals):\n",
    "        if list(col1.values)==list(col1.values):\n",
    "            print(col_name1,'  and  ',col_name2,'  is same pred \\n')\n",
    "    return None\n",
    "\n",
    "def pred_th_trans(pred_df,th):#閾値を渡して、その値以上を1、未満を0に置き変える。\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detected-culture",
   "metadata": {},
   "source": [
    "## 前バージョンから持ってきた参考関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "color-compiler",
   "metadata": {
    "code_folding": [
     0,
     120,
     240,
     344,
     419,
     477
    ]
   },
   "outputs": [],
   "source": [
    "def data_making_clustar_section_has_final(df,now_ym,range_test_m,range_final_m):#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）final(実運用バックテスト区間)あり版\n",
    "    #V4系列で使用する加工関数，年と月の情報を使って直近のデータを使って性能検証を行う（区間を使ってデータを作っている）\n",
    "    #validデータを作成するバージョンなので実装する際はこれをそのまま使わず，final_test部分の処理は消してくだちい\n",
    "    #yearが使わないと思うけど一応残してあるから邪魔だと思ったら消して下さい\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22}).copy()#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02}).copy()#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02}).copy()\n",
    "\n",
    "    #result_base_df=result_df.copy()\n",
    "    #result_base_df=get_event_info(result_base_df)#開催の情報について付与する関数(年月日に加えて，何日間の開催かどうかも教えてくれる)\n",
    "\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df.copy()\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr.copy()\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col]).copy()#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val.copy()\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    \n",
    "    #==========================================================================\n",
    "    #result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020].copy()#2020のデータを完全に切り離す。\n",
    "    #==========================================================================\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #now_ym:開始の月\n",
    "    test_st_date = now_ym - relativedelta(months=range_test_m)#テストデータに使用する区間を決める\n",
    "    final_test_en_date = now_ym + relativedelta(months=range_final_m)#最終テストの終了日\n",
    "    result_df_dummie=result_df_dummie[result_df_dummie['date']<final_test_en_date]#おおざっぱに切り分ける\n",
    "\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values.copy()#\n",
    "    money=result_df_dummie['money'].values.copy()#\n",
    "    years=result_df_dummie['year'].values.copy()#\n",
    "    dates=result_df_dummie['date'].values.copy()#\n",
    "    \n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('year',axis=1)\n",
    "    \n",
    "    #いらない情報を削除したらデータの切り分け\n",
    "    clustar_final_test_df=result_df_dummie[(result_df_dummie['date']>=now_ym) & (result_df_dummie['date']<final_test_en_date) ].copy()#今の日付以降を最終チェックデータ(予測のターゲット)に。\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['date']<now_ym) & ((result_df_dummie['date']>=test_st_date) )].copy()#今日に日より前の，指定した区間でのテストデータ\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['date']<test_st_date)].copy()#そのほかを学習データに\n",
    "    #日付の情報だけ切り分けに使ったからここで消す。\n",
    "    clustar_final_test_df=clustar_final_test_df.drop('date',axis=1).copy()\n",
    "    clustar_test_df=clustar_test_df.drop('date',axis=1).copy()\n",
    "    clustar_train_df=clustar_train_df.drop('date',axis=1).copy()\n",
    "    #esult_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "#     clustar_final_test_df['check']='final'#確認用\n",
    "#     clustar_test_df['check']='test'#確認用\n",
    "#     clustar_train_df['check']='train'#確認用\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['date']=dates\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df.copy()\n",
    "    return model_df\n",
    "\n",
    "def data_making_clustar_section(df,now_ym,range_test_m):#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）final(実運用バックテスト区間)無し版\n",
    "    #V4系列で使用する加工関数，年と月の情報を使って直近のデータを使って性能検証を行う（区間を使ってデータを作っている）\n",
    "    #validデータを作成するバージョンなので実装する際はこれをそのまま使わず，final_test部分の処理は消してくだちい\n",
    "    #yearが使わないと思うけど一応残してあるから邪魔だと思ったら消して下さい\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22}).copy()#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02}).copy()#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02}).copy()\n",
    "\n",
    "    #result_base_df=result_df.copy()\n",
    "    #result_base_df=get_event_info(result_base_df)#開催の情報について付与する関数(年月日に加えて，何日間の開催かどうかも教えてくれる)\n",
    "\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df.copy()\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr.copy()\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col]).copy()#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val.copy()\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    \n",
    "    #==========================================================================\n",
    "    #result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020].copy()#2020のデータを完全に切り離す。\n",
    "    #==========================================================================\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #now_ym:開始の月\n",
    "    test_st_date = now_ym - relativedelta(months=range_test_m)#テストデータに使用する区間を決める\n",
    "    result_df_dummie=result_df_dummie[result_df_dummie['date']<now_ym]#おおざっぱに切り分ける\n",
    "    #final_test_en_date = now_ym + relativedelta(months=range_final_m)#最終テストの終了日\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values.copy()#\n",
    "    money=result_df_dummie['money'].values.copy()#\n",
    "    years=result_df_dummie['year'].values.copy()#\n",
    "    dates=result_df_dummie['date'].values.copy()#\n",
    "    \n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('year',axis=1)\n",
    "    \n",
    "    #切り分ける\n",
    "    #clustar_final_test_df=result_df_dummie[(result_df_dummie['date']>=now_ym) & (result_df_dummie['date']<final_test_en_date) ].copy()#今の日付以降を最終チェックデータ(予測のターゲット)に。\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['date']<now_ym) & ((result_df_dummie['date']>=test_st_date) )].copy()#今日より前の，指定した区間でのテストデータ\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['date']<test_st_date)].copy()#そのほかを学習データに\n",
    "    #日付の情報だけ切り分けに使ったからここで消す。\n",
    "    #clustar_final_test_df=clustar_final_test_df.drop('date',axis=1).copy()\n",
    "    clustar_test_df=clustar_test_df.drop('date',axis=1).copy()\n",
    "    clustar_train_df=clustar_train_df.drop('date',axis=1).copy()\n",
    "    #result_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "#     clustar_final_test_df['check']='final'#確認用\n",
    "#     clustar_test_df['check']='test'#確認用\n",
    "#     clustar_train_df['check']='train'#確認用\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df]).copy()\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['date']=dates\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df.copy()\n",
    "    return model_df\n",
    "\n",
    "def data_making_clustar(df,years):#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22}).copy()#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02}).copy()#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02}).copy()\n",
    "\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df.copy()\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr.copy()\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col]).copy()#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val.copy()\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "\n",
    "    #==========================================================================\n",
    "    #result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020].copy()#2020のデータを完全に切り離す。\n",
    "    #==========================================================================\n",
    "\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values.copy()#\n",
    "    money=result_df_dummie['money'].values.copy()#\n",
    "    years=result_df_dummie['year'].values.copy()#\n",
    "\n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    #clustar_final_test_df=result_df_dummie[(result_df_dummie['year']==year3)].copy()#2020のデータを最終チェックデータ(予測のターゲット)に。\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['year']==year1) | ((result_df_dummie['year']==year2) )].copy()#2019,2010のデータを検証用データに。\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['year']!=year1) & (result_df_dummie['year']!=year2)].copy()#そのほかを学習データに\n",
    "\n",
    "    #年の情報だけ切り分けに使ったからここで消す。\n",
    "    #clustar_final_test_df=clustar_final_test_df.drop('year',axis=1).copy()\n",
    "    clustar_test_df=clustar_test_df.drop('year',axis=1).copy()\n",
    "    clustar_train_df=clustar_train_df.drop('year',axis=1).copy()\n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df]).copy()\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df.copy()\n",
    "    return model_df\n",
    "\n",
    "def data_making_mo_bo(df):#クラスタリングなし、ボート、艇番号あり\n",
    "\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22})#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02})#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02})\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col])#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "    #boat もmoterも番号は1~99とする\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        for number in numbers:\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        moter_dummie_df=pd.get_dummies(result_df_dummie[col])#モータ番号をダミー化\n",
    "        for column, val in moter_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    #boat番号をダミー化\n",
    "    for col in boat_cols:\n",
    "        for number in numbers:\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        boat_dummie_df=pd.get_dummies(result_df_dummie[col])#boat番号をダミー化\n",
    "        for column, val in boat_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[8,10]の2個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    target_num_cluster=[8,10]\n",
    "    #test_clustaring_df=train_has_PCA_df\n",
    "    clustar_target_df=result_df_dummie\n",
    "    clustaring_df=clustar_target_df\n",
    "    \"\"\"\n",
    "    for num_cluster in target_num_cluster:\n",
    "        pred = KMeans(random_state=0,n_clusters=num_cluster).fit_predict(clustar_target_df)\n",
    "        clustaring_df['num={}'.format(num_cluster)]=pred\n",
    "    \"\"\"\n",
    "    model_df=clustaring_df\n",
    "    model_df=trans_date_type(model_df)\n",
    "    return model_df\n",
    "\n",
    "def data_making_none_clustar(df):#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22}).copy()#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02}).copy()#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02}).copy()\n",
    "\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df.copy()\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr.copy()\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col]).copy()#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val.copy()\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    return result_df_dummie\n",
    "def model_score_rondom_forest(version,place_name,result_df):#学習データと場所名を渡せば探索を初めて、指定のディレクトリにスコアをまとめたcsvを出力する。\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=test_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    gain_mean=gain_mean.sort_index()\n",
    "\n",
    "    gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    gain_median=gain_median.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "                                'result_com_num':result_com_s.values,\n",
    "                                'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "                                'gain_mean':gain_mean.values,\n",
    "                                'gain_median':gain_median.values,})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "\n",
    "        gain_th=10#利益率の閾値\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,73)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        depths_arr=[5,6,8]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=80+(sum_target_per)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                model_score_s=pd.Series(index=['target_com','depth','target_per','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=1)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1)\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "                target_x_test=result_test_df.drop('money',axis=1)\n",
    "                target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test=result_test_df['result_com']\n",
    "\n",
    "                #テストデータ\n",
    "                RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth,n_jobs=10)\n",
    "                RF = RF.fit(target_x_train,target_y_train)\n",
    "                # 未知データに対する予測値\n",
    "                predict_y_test = RF.predict(target_x_test)\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る\n",
    "                pred_test_df=pd.DataFrame({'pred':predict_y_test\n",
    "                                          , 'test':target_y_test})\n",
    "                num_1=len(pred_test_df[pred_test_df['test']==1])\n",
    "                count=0\n",
    "                #追加　配当金の情報も考慮する。\n",
    "                gain_index=0\n",
    "                model_gain_arr=[0]*len(result_test_df)\n",
    "                for _, s in pred_test_df.iterrows():\n",
    "                    if ((s['pred']==1) and (s['test']==1)):\n",
    "                        count+=1#的中回数\n",
    "                        model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "                    gain_index+=1\n",
    "                #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "                gain_arr[index]=sum(model_gain_arr)\n",
    "                accuracy_arr[index]=(count/num_1)*100\n",
    "                try:\n",
    "                    pred_0[index]=pred_test_df['pred'].value_counts()[0]\n",
    "                except:\n",
    "                    pred_0[index]=0\n",
    "                #scoreのseriesに情報書き込み==================\n",
    "                model_score_s['総収益']=sum(model_gain_arr)\n",
    "                model_score_s['投資金額']=100*sum(predict_y_test)\n",
    "                model_score_s['出現数']=sum(target_y_test)\n",
    "                model_score_s['購買予測数']=sum(predict_y_test)\n",
    "                model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['購買的中率']=(count/sum(predict_y_test))*100\n",
    "                model_score_s['的中数']=count\n",
    "                model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "\n",
    "\n",
    "    #モデルの「スコアを保存\n",
    "    #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "    dir_path = \"../../bot_database/{place_name}/model_score_{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broad-toner",
   "metadata": {},
   "source": [
    "## 実行する関数(今回作成した関数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "august-hierarchy",
   "metadata": {
    "code_folding": [
     0,
     136,
     272,
     408,
     544,
     680
    ]
   },
   "outputs": [],
   "source": [
    "def model_score_pc_et_year(version,years,place_name,result_df):#pycaretを使ってモデリング(et),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('rf')#モデルの生成（Extra Trees Regressor）\n",
    "            tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_pc_et_tuneless_year(version,years,place_name,result_df):#pycaretを使ってモデリング(et),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('rf')#モデルの生成（Extra Trees Regressor）\n",
    "            #tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/tuneless_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_pc_nb_tuneless_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[100,200,500,700,1000,3000,5000]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('nb')#モデルの生成（Extra Trees Regressor）\n",
    "            #tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/tuneless_nb_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_pc_nb_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[100,200,500,700,1000,3000,5000]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('nb')#モデルの生成（Extra Trees Regressor）\n",
    "            tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/{place_name}_nb_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_pc_knn_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('knn')#モデルの生成（Extra Trees Regressor）\n",
    "            tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/{place_name}_knn_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_pc_knn_tuneless_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('knn')#モデルの生成（Extra Trees Regressor）\n",
    "            #tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/tuneless_knn_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "muslim-crawford",
   "metadata": {},
   "source": [
    "## 実行(年単位バージョン)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-samba",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## クラスタリングなし，モータ，艇番号なしバージョン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "color-direction",
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_3e3a6_row10_col0,#T_3e3a6_row10_col1,#T_3e3a6_row10_col2,#T_3e3a6_row10_col3,#T_3e3a6_row10_col4,#T_3e3a6_row10_col5,#T_3e3a6_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_3e3a6_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_3e3a6_row0_col0\" class=\"data row0 col0\" >0.8889</td>\n",
       "                        <td id=\"T_3e3a6_row0_col1\" class=\"data row0 col1\" >0.7799</td>\n",
       "                        <td id=\"T_3e3a6_row0_col2\" class=\"data row0 col2\" >0.3750</td>\n",
       "                        <td id=\"T_3e3a6_row0_col3\" class=\"data row0 col3\" >0.7500</td>\n",
       "                        <td id=\"T_3e3a6_row0_col4\" class=\"data row0 col4\" >0.5000</td>\n",
       "                        <td id=\"T_3e3a6_row0_col5\" class=\"data row0 col5\" >0.4452</td>\n",
       "                        <td id=\"T_3e3a6_row0_col6\" class=\"data row0 col6\" >0.4792</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_3e3a6_row1_col0\" class=\"data row1 col0\" >0.8519</td>\n",
       "                        <td id=\"T_3e3a6_row1_col1\" class=\"data row1 col1\" >0.8152</td>\n",
       "                        <td id=\"T_3e3a6_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "                        <td id=\"T_3e3a6_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "                        <td id=\"T_3e3a6_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "                        <td id=\"T_3e3a6_row1_col5\" class=\"data row1 col5\" >0.0000</td>\n",
       "                        <td id=\"T_3e3a6_row1_col6\" class=\"data row1 col6\" >0.0000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_3e3a6_row2_col0\" class=\"data row2 col0\" >0.8889</td>\n",
       "                        <td id=\"T_3e3a6_row2_col1\" class=\"data row2 col1\" >0.9484</td>\n",
       "                        <td id=\"T_3e3a6_row2_col2\" class=\"data row2 col2\" >0.3750</td>\n",
       "                        <td id=\"T_3e3a6_row2_col3\" class=\"data row2 col3\" >0.7500</td>\n",
       "                        <td id=\"T_3e3a6_row2_col4\" class=\"data row2 col4\" >0.5000</td>\n",
       "                        <td id=\"T_3e3a6_row2_col5\" class=\"data row2 col5\" >0.4452</td>\n",
       "                        <td id=\"T_3e3a6_row2_col6\" class=\"data row2 col6\" >0.4792</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_3e3a6_row3_col0\" class=\"data row3 col0\" >0.8704</td>\n",
       "                        <td id=\"T_3e3a6_row3_col1\" class=\"data row3 col1\" >0.9429</td>\n",
       "                        <td id=\"T_3e3a6_row3_col2\" class=\"data row3 col2\" >0.3750</td>\n",
       "                        <td id=\"T_3e3a6_row3_col3\" class=\"data row3 col3\" >0.6000</td>\n",
       "                        <td id=\"T_3e3a6_row3_col4\" class=\"data row3 col4\" >0.4615</td>\n",
       "                        <td id=\"T_3e3a6_row3_col5\" class=\"data row3 col5\" >0.3923</td>\n",
       "                        <td id=\"T_3e3a6_row3_col6\" class=\"data row3 col6\" >0.4063</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_3e3a6_row4_col0\" class=\"data row4 col0\" >0.8704</td>\n",
       "                        <td id=\"T_3e3a6_row4_col1\" class=\"data row4 col1\" >0.8043</td>\n",
       "                        <td id=\"T_3e3a6_row4_col2\" class=\"data row4 col2\" >0.1250</td>\n",
       "                        <td id=\"T_3e3a6_row4_col3\" class=\"data row4 col3\" >1.0000</td>\n",
       "                        <td id=\"T_3e3a6_row4_col4\" class=\"data row4 col4\" >0.2222</td>\n",
       "                        <td id=\"T_3e3a6_row4_col5\" class=\"data row4 col5\" >0.1957</td>\n",
       "                        <td id=\"T_3e3a6_row4_col6\" class=\"data row4 col6\" >0.3294</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_3e3a6_row5_col0\" class=\"data row5 col0\" >0.8704</td>\n",
       "                        <td id=\"T_3e3a6_row5_col1\" class=\"data row5 col1\" >0.7391</td>\n",
       "                        <td id=\"T_3e3a6_row5_col2\" class=\"data row5 col2\" >0.1250</td>\n",
       "                        <td id=\"T_3e3a6_row5_col3\" class=\"data row5 col3\" >1.0000</td>\n",
       "                        <td id=\"T_3e3a6_row5_col4\" class=\"data row5 col4\" >0.2222</td>\n",
       "                        <td id=\"T_3e3a6_row5_col5\" class=\"data row5 col5\" >0.1957</td>\n",
       "                        <td id=\"T_3e3a6_row5_col6\" class=\"data row5 col6\" >0.3294</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_3e3a6_row6_col0\" class=\"data row6 col0\" >0.9074</td>\n",
       "                        <td id=\"T_3e3a6_row6_col1\" class=\"data row6 col1\" >0.9212</td>\n",
       "                        <td id=\"T_3e3a6_row6_col2\" class=\"data row6 col2\" >0.3750</td>\n",
       "                        <td id=\"T_3e3a6_row6_col3\" class=\"data row6 col3\" >1.0000</td>\n",
       "                        <td id=\"T_3e3a6_row6_col4\" class=\"data row6 col4\" >0.5455</td>\n",
       "                        <td id=\"T_3e3a6_row6_col5\" class=\"data row6 col5\" >0.5055</td>\n",
       "                        <td id=\"T_3e3a6_row6_col6\" class=\"data row6 col6\" >0.5816</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_3e3a6_row7_col0\" class=\"data row7 col0\" >0.8889</td>\n",
       "                        <td id=\"T_3e3a6_row7_col1\" class=\"data row7 col1\" >0.7599</td>\n",
       "                        <td id=\"T_3e3a6_row7_col2\" class=\"data row7 col2\" >0.1429</td>\n",
       "                        <td id=\"T_3e3a6_row7_col3\" class=\"data row7 col3\" >1.0000</td>\n",
       "                        <td id=\"T_3e3a6_row7_col4\" class=\"data row7 col4\" >0.2500</td>\n",
       "                        <td id=\"T_3e3a6_row7_col5\" class=\"data row7 col5\" >0.2249</td>\n",
       "                        <td id=\"T_3e3a6_row7_col6\" class=\"data row7 col6\" >0.3559</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_3e3a6_row8_col0\" class=\"data row8 col0\" >0.8679</td>\n",
       "                        <td id=\"T_3e3a6_row8_col1\" class=\"data row8 col1\" >0.6863</td>\n",
       "                        <td id=\"T_3e3a6_row8_col2\" class=\"data row8 col2\" >0.2857</td>\n",
       "                        <td id=\"T_3e3a6_row8_col3\" class=\"data row8 col3\" >0.5000</td>\n",
       "                        <td id=\"T_3e3a6_row8_col4\" class=\"data row8 col4\" >0.3636</td>\n",
       "                        <td id=\"T_3e3a6_row8_col5\" class=\"data row8 col5\" >0.2960</td>\n",
       "                        <td id=\"T_3e3a6_row8_col6\" class=\"data row8 col6\" >0.3105</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_3e3a6_row9_col0\" class=\"data row9 col0\" >0.9245</td>\n",
       "                        <td id=\"T_3e3a6_row9_col1\" class=\"data row9 col1\" >0.7919</td>\n",
       "                        <td id=\"T_3e3a6_row9_col2\" class=\"data row9 col2\" >0.4286</td>\n",
       "                        <td id=\"T_3e3a6_row9_col3\" class=\"data row9 col3\" >1.0000</td>\n",
       "                        <td id=\"T_3e3a6_row9_col4\" class=\"data row9 col4\" >0.6000</td>\n",
       "                        <td id=\"T_3e3a6_row9_col5\" class=\"data row9 col5\" >0.5656</td>\n",
       "                        <td id=\"T_3e3a6_row9_col6\" class=\"data row9 col6\" >0.6279</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_3e3a6_row10_col0\" class=\"data row10 col0\" >0.8829</td>\n",
       "                        <td id=\"T_3e3a6_row10_col1\" class=\"data row10 col1\" >0.8189</td>\n",
       "                        <td id=\"T_3e3a6_row10_col2\" class=\"data row10 col2\" >0.2607</td>\n",
       "                        <td id=\"T_3e3a6_row10_col3\" class=\"data row10 col3\" >0.7600</td>\n",
       "                        <td id=\"T_3e3a6_row10_col4\" class=\"data row10 col4\" >0.3665</td>\n",
       "                        <td id=\"T_3e3a6_row10_col5\" class=\"data row10 col5\" >0.3266</td>\n",
       "                        <td id=\"T_3e3a6_row10_col6\" class=\"data row10 col6\" >0.3899</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_3e3a6_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_3e3a6_row11_col0\" class=\"data row11 col0\" >0.0202</td>\n",
       "                        <td id=\"T_3e3a6_row11_col1\" class=\"data row11 col1\" >0.0852</td>\n",
       "                        <td id=\"T_3e3a6_row11_col2\" class=\"data row11 col2\" >0.1413</td>\n",
       "                        <td id=\"T_3e3a6_row11_col3\" class=\"data row11 col3\" >0.3097</td>\n",
       "                        <td id=\"T_3e3a6_row11_col4\" class=\"data row11 col4\" >0.1791</td>\n",
       "                        <td id=\"T_3e3a6_row11_col5\" class=\"data row11 col5\" >0.1655</td>\n",
       "                        <td id=\"T_3e3a6_row11_col6\" class=\"data row11 col6\" >0.1663</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c13165b370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [2:54:23<00:00, 373.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_pc_et_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "leading-reward",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_2fe6c_row10_col0,#T_2fe6c_row10_col1,#T_2fe6c_row10_col2,#T_2fe6c_row10_col3,#T_2fe6c_row10_col4,#T_2fe6c_row10_col5,#T_2fe6c_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_2fe6c_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_2fe6c_row0_col0\" class=\"data row0 col0\" >0.8889</td>\n",
       "                        <td id=\"T_2fe6c_row0_col1\" class=\"data row0 col1\" >0.7989</td>\n",
       "                        <td id=\"T_2fe6c_row0_col2\" class=\"data row0 col2\" >0.5000</td>\n",
       "                        <td id=\"T_2fe6c_row0_col3\" class=\"data row0 col3\" >0.6667</td>\n",
       "                        <td id=\"T_2fe6c_row0_col4\" class=\"data row0 col4\" >0.5714</td>\n",
       "                        <td id=\"T_2fe6c_row0_col5\" class=\"data row0 col5\" >0.5091</td>\n",
       "                        <td id=\"T_2fe6c_row0_col6\" class=\"data row0 col6\" >0.5160</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_2fe6c_row1_col0\" class=\"data row1 col0\" >0.8519</td>\n",
       "                        <td id=\"T_2fe6c_row1_col1\" class=\"data row1 col1\" >0.8465</td>\n",
       "                        <td id=\"T_2fe6c_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row1_col5\" class=\"data row1 col5\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row1_col6\" class=\"data row1 col6\" >0.0000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_2fe6c_row2_col0\" class=\"data row2 col0\" >0.8889</td>\n",
       "                        <td id=\"T_2fe6c_row2_col1\" class=\"data row2 col1\" >0.9090</td>\n",
       "                        <td id=\"T_2fe6c_row2_col2\" class=\"data row2 col2\" >0.3750</td>\n",
       "                        <td id=\"T_2fe6c_row2_col3\" class=\"data row2 col3\" >0.7500</td>\n",
       "                        <td id=\"T_2fe6c_row2_col4\" class=\"data row2 col4\" >0.5000</td>\n",
       "                        <td id=\"T_2fe6c_row2_col5\" class=\"data row2 col5\" >0.4452</td>\n",
       "                        <td id=\"T_2fe6c_row2_col6\" class=\"data row2 col6\" >0.4792</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_2fe6c_row3_col0\" class=\"data row3 col0\" >0.9444</td>\n",
       "                        <td id=\"T_2fe6c_row3_col1\" class=\"data row3 col1\" >0.9674</td>\n",
       "                        <td id=\"T_2fe6c_row3_col2\" class=\"data row3 col2\" >0.6250</td>\n",
       "                        <td id=\"T_2fe6c_row3_col3\" class=\"data row3 col3\" >1.0000</td>\n",
       "                        <td id=\"T_2fe6c_row3_col4\" class=\"data row3 col4\" >0.7692</td>\n",
       "                        <td id=\"T_2fe6c_row3_col5\" class=\"data row3 col5\" >0.7395</td>\n",
       "                        <td id=\"T_2fe6c_row3_col6\" class=\"data row3 col6\" >0.7660</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_2fe6c_row4_col0\" class=\"data row4 col0\" >0.8519</td>\n",
       "                        <td id=\"T_2fe6c_row4_col1\" class=\"data row4 col1\" >0.8207</td>\n",
       "                        <td id=\"T_2fe6c_row4_col2\" class=\"data row4 col2\" >0.1250</td>\n",
       "                        <td id=\"T_2fe6c_row4_col3\" class=\"data row4 col3\" >0.5000</td>\n",
       "                        <td id=\"T_2fe6c_row4_col4\" class=\"data row4 col4\" >0.2000</td>\n",
       "                        <td id=\"T_2fe6c_row4_col5\" class=\"data row4 col5\" >0.1496</td>\n",
       "                        <td id=\"T_2fe6c_row4_col6\" class=\"data row4 col6\" >0.1942</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_2fe6c_row5_col0\" class=\"data row5 col0\" >0.8519</td>\n",
       "                        <td id=\"T_2fe6c_row5_col1\" class=\"data row5 col1\" >0.7704</td>\n",
       "                        <td id=\"T_2fe6c_row5_col2\" class=\"data row5 col2\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row5_col3\" class=\"data row5 col3\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row5_col4\" class=\"data row5 col4\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row5_col5\" class=\"data row5 col5\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row5_col6\" class=\"data row5 col6\" >0.0000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_2fe6c_row6_col0\" class=\"data row6 col0\" >0.9074</td>\n",
       "                        <td id=\"T_2fe6c_row6_col1\" class=\"data row6 col1\" >0.9416</td>\n",
       "                        <td id=\"T_2fe6c_row6_col2\" class=\"data row6 col2\" >0.3750</td>\n",
       "                        <td id=\"T_2fe6c_row6_col3\" class=\"data row6 col3\" >1.0000</td>\n",
       "                        <td id=\"T_2fe6c_row6_col4\" class=\"data row6 col4\" >0.5455</td>\n",
       "                        <td id=\"T_2fe6c_row6_col5\" class=\"data row6 col5\" >0.5055</td>\n",
       "                        <td id=\"T_2fe6c_row6_col6\" class=\"data row6 col6\" >0.5816</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_2fe6c_row7_col0\" class=\"data row7 col0\" >0.8704</td>\n",
       "                        <td id=\"T_2fe6c_row7_col1\" class=\"data row7 col1\" >0.8009</td>\n",
       "                        <td id=\"T_2fe6c_row7_col2\" class=\"data row7 col2\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row7_col3\" class=\"data row7 col3\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row7_col4\" class=\"data row7 col4\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row7_col5\" class=\"data row7 col5\" >0.0000</td>\n",
       "                        <td id=\"T_2fe6c_row7_col6\" class=\"data row7 col6\" >0.0000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_2fe6c_row8_col0\" class=\"data row8 col0\" >0.8679</td>\n",
       "                        <td id=\"T_2fe6c_row8_col1\" class=\"data row8 col1\" >0.7267</td>\n",
       "                        <td id=\"T_2fe6c_row8_col2\" class=\"data row8 col2\" >0.1429</td>\n",
       "                        <td id=\"T_2fe6c_row8_col3\" class=\"data row8 col3\" >0.5000</td>\n",
       "                        <td id=\"T_2fe6c_row8_col4\" class=\"data row8 col4\" >0.2222</td>\n",
       "                        <td id=\"T_2fe6c_row8_col5\" class=\"data row8 col5\" >0.1737</td>\n",
       "                        <td id=\"T_2fe6c_row8_col6\" class=\"data row8 col6\" >0.2152</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_2fe6c_row9_col0\" class=\"data row9 col0\" >0.8868</td>\n",
       "                        <td id=\"T_2fe6c_row9_col1\" class=\"data row9 col1\" >0.7748</td>\n",
       "                        <td id=\"T_2fe6c_row9_col2\" class=\"data row9 col2\" >0.1429</td>\n",
       "                        <td id=\"T_2fe6c_row9_col3\" class=\"data row9 col3\" >1.0000</td>\n",
       "                        <td id=\"T_2fe6c_row9_col4\" class=\"data row9 col4\" >0.2500</td>\n",
       "                        <td id=\"T_2fe6c_row9_col5\" class=\"data row9 col5\" >0.2244</td>\n",
       "                        <td id=\"T_2fe6c_row9_col6\" class=\"data row9 col6\" >0.3555</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_2fe6c_row10_col0\" class=\"data row10 col0\" >0.8810</td>\n",
       "                        <td id=\"T_2fe6c_row10_col1\" class=\"data row10 col1\" >0.8357</td>\n",
       "                        <td id=\"T_2fe6c_row10_col2\" class=\"data row10 col2\" >0.2286</td>\n",
       "                        <td id=\"T_2fe6c_row10_col3\" class=\"data row10 col3\" >0.5417</td>\n",
       "                        <td id=\"T_2fe6c_row10_col4\" class=\"data row10 col4\" >0.3058</td>\n",
       "                        <td id=\"T_2fe6c_row10_col5\" class=\"data row10 col5\" >0.2747</td>\n",
       "                        <td id=\"T_2fe6c_row10_col6\" class=\"data row10 col6\" >0.3108</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_2fe6c_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_2fe6c_row11_col0\" class=\"data row11 col0\" >0.0278</td>\n",
       "                        <td id=\"T_2fe6c_row11_col1\" class=\"data row11 col1\" >0.0753</td>\n",
       "                        <td id=\"T_2fe6c_row11_col2\" class=\"data row11 col2\" >0.2135</td>\n",
       "                        <td id=\"T_2fe6c_row11_col3\" class=\"data row11 col3\" >0.3966</td>\n",
       "                        <td id=\"T_2fe6c_row11_col4\" class=\"data row11 col4\" >0.2612</td>\n",
       "                        <td id=\"T_2fe6c_row11_col5\" class=\"data row11 col5\" >0.2466</td>\n",
       "                        <td id=\"T_2fe6c_row11_col6\" class=\"data row11 col6\" >0.2580</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c136704e20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [15:08<00:00, 32.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ハイパーパラメータ探索を行わない\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_pc_et_tuneless_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "preliminary-assault",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_50b2e_row10_col0,#T_50b2e_row10_col1,#T_50b2e_row10_col2,#T_50b2e_row10_col3,#T_50b2e_row10_col4,#T_50b2e_row10_col5,#T_50b2e_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_50b2e_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_50b2e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_50b2e_row0_col0\" class=\"data row0 col0\" >0.3890</td>\n",
       "                        <td id=\"T_50b2e_row0_col1\" class=\"data row0 col1\" >0.7562</td>\n",
       "                        <td id=\"T_50b2e_row0_col2\" class=\"data row0 col2\" >1.0000</td>\n",
       "                        <td id=\"T_50b2e_row0_col3\" class=\"data row0 col3\" >0.0316</td>\n",
       "                        <td id=\"T_50b2e_row0_col4\" class=\"data row0 col4\" >0.0613</td>\n",
       "                        <td id=\"T_50b2e_row0_col5\" class=\"data row0 col5\" >0.0235</td>\n",
       "                        <td id=\"T_50b2e_row0_col6\" class=\"data row0 col6\" >0.1091</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_50b2e_row1_col0\" class=\"data row1 col0\" >0.3741</td>\n",
       "                        <td id=\"T_50b2e_row1_col1\" class=\"data row1 col1\" >0.7261</td>\n",
       "                        <td id=\"T_50b2e_row1_col2\" class=\"data row1 col2\" >0.8750</td>\n",
       "                        <td id=\"T_50b2e_row1_col3\" class=\"data row1 col3\" >0.0272</td>\n",
       "                        <td id=\"T_50b2e_row1_col4\" class=\"data row1 col4\" >0.0528</td>\n",
       "                        <td id=\"T_50b2e_row1_col5\" class=\"data row1 col5\" >0.0147</td>\n",
       "                        <td id=\"T_50b2e_row1_col6\" class=\"data row1 col6\" >0.0696</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_50b2e_row2_col0\" class=\"data row2 col0\" >0.3591</td>\n",
       "                        <td id=\"T_50b2e_row2_col1\" class=\"data row2 col1\" >0.7766</td>\n",
       "                        <td id=\"T_50b2e_row2_col2\" class=\"data row2 col2\" >1.0000</td>\n",
       "                        <td id=\"T_50b2e_row2_col3\" class=\"data row2 col3\" >0.0302</td>\n",
       "                        <td id=\"T_50b2e_row2_col4\" class=\"data row2 col4\" >0.0586</td>\n",
       "                        <td id=\"T_50b2e_row2_col5\" class=\"data row2 col5\" >0.0207</td>\n",
       "                        <td id=\"T_50b2e_row2_col6\" class=\"data row2 col6\" >0.1022</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_50b2e_row3_col0\" class=\"data row3 col0\" >0.3725</td>\n",
       "                        <td id=\"T_50b2e_row3_col1\" class=\"data row3 col1\" >0.9405</td>\n",
       "                        <td id=\"T_50b2e_row3_col2\" class=\"data row3 col2\" >1.0000</td>\n",
       "                        <td id=\"T_50b2e_row3_col3\" class=\"data row3 col3\" >0.0309</td>\n",
       "                        <td id=\"T_50b2e_row3_col4\" class=\"data row3 col4\" >0.0599</td>\n",
       "                        <td id=\"T_50b2e_row3_col5\" class=\"data row3 col5\" >0.0220</td>\n",
       "                        <td id=\"T_50b2e_row3_col6\" class=\"data row3 col6\" >0.1054</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_50b2e_row4_col0\" class=\"data row4 col0\" >0.4075</td>\n",
       "                        <td id=\"T_50b2e_row4_col1\" class=\"data row4 col1\" >0.7923</td>\n",
       "                        <td id=\"T_50b2e_row4_col2\" class=\"data row4 col2\" >1.0000</td>\n",
       "                        <td id=\"T_50b2e_row4_col3\" class=\"data row4 col3\" >0.0327</td>\n",
       "                        <td id=\"T_50b2e_row4_col4\" class=\"data row4 col4\" >0.0632</td>\n",
       "                        <td id=\"T_50b2e_row4_col5\" class=\"data row4 col5\" >0.0255</td>\n",
       "                        <td id=\"T_50b2e_row4_col6\" class=\"data row4 col6\" >0.1136</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_50b2e_row5_col0\" class=\"data row5 col0\" >0.3725</td>\n",
       "                        <td id=\"T_50b2e_row5_col1\" class=\"data row5 col1\" >0.6425</td>\n",
       "                        <td id=\"T_50b2e_row5_col2\" class=\"data row5 col2\" >1.0000</td>\n",
       "                        <td id=\"T_50b2e_row5_col3\" class=\"data row5 col3\" >0.0309</td>\n",
       "                        <td id=\"T_50b2e_row5_col4\" class=\"data row5 col4\" >0.0599</td>\n",
       "                        <td id=\"T_50b2e_row5_col5\" class=\"data row5 col5\" >0.0220</td>\n",
       "                        <td id=\"T_50b2e_row5_col6\" class=\"data row5 col6\" >0.1054</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_50b2e_row6_col0\" class=\"data row6 col0\" >0.3425</td>\n",
       "                        <td id=\"T_50b2e_row6_col1\" class=\"data row6 col1\" >0.8299</td>\n",
       "                        <td id=\"T_50b2e_row6_col2\" class=\"data row6 col2\" >0.8750</td>\n",
       "                        <td id=\"T_50b2e_row6_col3\" class=\"data row6 col3\" >0.0260</td>\n",
       "                        <td id=\"T_50b2e_row6_col4\" class=\"data row6 col4\" >0.0505</td>\n",
       "                        <td id=\"T_50b2e_row6_col5\" class=\"data row6 col5\" >0.0122</td>\n",
       "                        <td id=\"T_50b2e_row6_col6\" class=\"data row6 col6\" >0.0616</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_50b2e_row7_col0\" class=\"data row7 col0\" >0.3750</td>\n",
       "                        <td id=\"T_50b2e_row7_col1\" class=\"data row7 col1\" >0.7601</td>\n",
       "                        <td id=\"T_50b2e_row7_col2\" class=\"data row7 col2\" >1.0000</td>\n",
       "                        <td id=\"T_50b2e_row7_col3\" class=\"data row7 col3\" >0.0272</td>\n",
       "                        <td id=\"T_50b2e_row7_col4\" class=\"data row7 col4\" >0.0530</td>\n",
       "                        <td id=\"T_50b2e_row7_col5\" class=\"data row7 col5\" >0.0196</td>\n",
       "                        <td id=\"T_50b2e_row7_col6\" class=\"data row7 col6\" >0.0996</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_50b2e_row8_col0\" class=\"data row8 col0\" >0.4375</td>\n",
       "                        <td id=\"T_50b2e_row8_col1\" class=\"data row8 col1\" >0.7179</td>\n",
       "                        <td id=\"T_50b2e_row8_col2\" class=\"data row8 col2\" >0.7143</td>\n",
       "                        <td id=\"T_50b2e_row8_col3\" class=\"data row8 col3\" >0.0219</td>\n",
       "                        <td id=\"T_50b2e_row8_col4\" class=\"data row8 col4\" >0.0426</td>\n",
       "                        <td id=\"T_50b2e_row8_col5\" class=\"data row8 col5\" >0.0089</td>\n",
       "                        <td id=\"T_50b2e_row8_col6\" class=\"data row8 col6\" >0.0389</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_50b2e_row9_col0\" class=\"data row9 col0\" >0.3825</td>\n",
       "                        <td id=\"T_50b2e_row9_col1\" class=\"data row9 col1\" >0.7977</td>\n",
       "                        <td id=\"T_50b2e_row9_col2\" class=\"data row9 col2\" >0.8571</td>\n",
       "                        <td id=\"T_50b2e_row9_col3\" class=\"data row9 col3\" >0.0238</td>\n",
       "                        <td id=\"T_50b2e_row9_col4\" class=\"data row9 col4\" >0.0463</td>\n",
       "                        <td id=\"T_50b2e_row9_col5\" class=\"data row9 col5\" >0.0127</td>\n",
       "                        <td id=\"T_50b2e_row9_col6\" class=\"data row9 col6\" >0.0628</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_50b2e_row10_col0\" class=\"data row10 col0\" >0.3812</td>\n",
       "                        <td id=\"T_50b2e_row10_col1\" class=\"data row10 col1\" >0.7740</td>\n",
       "                        <td id=\"T_50b2e_row10_col2\" class=\"data row10 col2\" >0.9321</td>\n",
       "                        <td id=\"T_50b2e_row10_col3\" class=\"data row10 col3\" >0.0282</td>\n",
       "                        <td id=\"T_50b2e_row10_col4\" class=\"data row10 col4\" >0.0548</td>\n",
       "                        <td id=\"T_50b2e_row10_col5\" class=\"data row10 col5\" >0.0182</td>\n",
       "                        <td id=\"T_50b2e_row10_col6\" class=\"data row10 col6\" >0.0868</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_50b2e_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_50b2e_row11_col0\" class=\"data row11 col0\" >0.0248</td>\n",
       "                        <td id=\"T_50b2e_row11_col1\" class=\"data row11 col1\" >0.0742</td>\n",
       "                        <td id=\"T_50b2e_row11_col2\" class=\"data row11 col2\" >0.0934</td>\n",
       "                        <td id=\"T_50b2e_row11_col3\" class=\"data row11 col3\" >0.0034</td>\n",
       "                        <td id=\"T_50b2e_row11_col4\" class=\"data row11 col4\" >0.0065</td>\n",
       "                        <td id=\"T_50b2e_row11_col5\" class=\"data row11 col5\" >0.0053</td>\n",
       "                        <td id=\"T_50b2e_row11_col6\" class=\"data row11 col6\" >0.0247</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c10eb99c40>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [08:10<00:00, 17.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#ナイーブベイズ版,ためしにチューニングなし\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_pc_nb_tuneless_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "subject-bulletin",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_93552_row10_col0,#T_93552_row10_col1,#T_93552_row10_col2,#T_93552_row10_col3,#T_93552_row10_col4,#T_93552_row10_col5,#T_93552_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_93552_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_93552_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_93552_row0_col0\" class=\"data row0 col0\" >0.9277</td>\n",
       "                        <td id=\"T_93552_row0_col1\" class=\"data row0 col1\" >0.8136</td>\n",
       "                        <td id=\"T_93552_row0_col2\" class=\"data row0 col2\" >0.1250</td>\n",
       "                        <td id=\"T_93552_row0_col3\" class=\"data row0 col3\" >0.0435</td>\n",
       "                        <td id=\"T_93552_row0_col4\" class=\"data row0 col4\" >0.0645</td>\n",
       "                        <td id=\"T_93552_row0_col5\" class=\"data row0 col5\" >0.0360</td>\n",
       "                        <td id=\"T_93552_row0_col6\" class=\"data row0 col6\" >0.0415</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_93552_row1_col0\" class=\"data row1 col0\" >0.9576</td>\n",
       "                        <td id=\"T_93552_row1_col1\" class=\"data row1 col1\" >0.8244</td>\n",
       "                        <td id=\"T_93552_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "                        <td id=\"T_93552_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "                        <td id=\"T_93552_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "                        <td id=\"T_93552_row1_col5\" class=\"data row1 col5\" >-0.0216</td>\n",
       "                        <td id=\"T_93552_row1_col6\" class=\"data row1 col6\" >-0.0216</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_93552_row2_col0\" class=\"data row2 col0\" >0.9401</td>\n",
       "                        <td id=\"T_93552_row2_col1\" class=\"data row2 col1\" >0.7564</td>\n",
       "                        <td id=\"T_93552_row2_col2\" class=\"data row2 col2\" >0.1250</td>\n",
       "                        <td id=\"T_93552_row2_col3\" class=\"data row2 col3\" >0.0556</td>\n",
       "                        <td id=\"T_93552_row2_col4\" class=\"data row2 col4\" >0.0769</td>\n",
       "                        <td id=\"T_93552_row2_col5\" class=\"data row2 col5\" >0.0507</td>\n",
       "                        <td id=\"T_93552_row2_col6\" class=\"data row2 col6\" >0.0552</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_93552_row3_col0\" class=\"data row3 col0\" >0.9550</td>\n",
       "                        <td id=\"T_93552_row3_col1\" class=\"data row3 col1\" >0.9302</td>\n",
       "                        <td id=\"T_93552_row3_col2\" class=\"data row3 col2\" >0.5000</td>\n",
       "                        <td id=\"T_93552_row3_col3\" class=\"data row3 col3\" >0.2222</td>\n",
       "                        <td id=\"T_93552_row3_col4\" class=\"data row3 col4\" >0.3077</td>\n",
       "                        <td id=\"T_93552_row3_col5\" class=\"data row3 col5\" >0.2880</td>\n",
       "                        <td id=\"T_93552_row3_col6\" class=\"data row3 col6\" >0.3135</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_93552_row4_col0\" class=\"data row4 col0\" >0.9525</td>\n",
       "                        <td id=\"T_93552_row4_col1\" class=\"data row4 col1\" >0.7793</td>\n",
       "                        <td id=\"T_93552_row4_col2\" class=\"data row4 col2\" >0.2500</td>\n",
       "                        <td id=\"T_93552_row4_col3\" class=\"data row4 col3\" >0.1333</td>\n",
       "                        <td id=\"T_93552_row4_col4\" class=\"data row4 col4\" >0.1739</td>\n",
       "                        <td id=\"T_93552_row4_col5\" class=\"data row4 col5\" >0.1518</td>\n",
       "                        <td id=\"T_93552_row4_col6\" class=\"data row4 col6\" >0.1598</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_93552_row5_col0\" class=\"data row5 col0\" >0.9325</td>\n",
       "                        <td id=\"T_93552_row5_col1\" class=\"data row5 col1\" >0.5456</td>\n",
       "                        <td id=\"T_93552_row5_col2\" class=\"data row5 col2\" >0.1250</td>\n",
       "                        <td id=\"T_93552_row5_col3\" class=\"data row5 col3\" >0.0476</td>\n",
       "                        <td id=\"T_93552_row5_col4\" class=\"data row5 col4\" >0.0690</td>\n",
       "                        <td id=\"T_93552_row5_col5\" class=\"data row5 col5\" >0.0412</td>\n",
       "                        <td id=\"T_93552_row5_col6\" class=\"data row5 col6\" >0.0464</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_93552_row6_col0\" class=\"data row6 col0\" >0.9375</td>\n",
       "                        <td id=\"T_93552_row6_col1\" class=\"data row6 col1\" >0.9008</td>\n",
       "                        <td id=\"T_93552_row6_col2\" class=\"data row6 col2\" >0.5000</td>\n",
       "                        <td id=\"T_93552_row6_col3\" class=\"data row6 col3\" >0.1600</td>\n",
       "                        <td id=\"T_93552_row6_col4\" class=\"data row6 col4\" >0.2424</td>\n",
       "                        <td id=\"T_93552_row6_col5\" class=\"data row6 col5\" >0.2188</td>\n",
       "                        <td id=\"T_93552_row6_col6\" class=\"data row6 col6\" >0.2582</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_93552_row7_col0\" class=\"data row7 col0\" >0.9225</td>\n",
       "                        <td id=\"T_93552_row7_col1\" class=\"data row7 col1\" >0.7754</td>\n",
       "                        <td id=\"T_93552_row7_col2\" class=\"data row7 col2\" >0.1429</td>\n",
       "                        <td id=\"T_93552_row7_col3\" class=\"data row7 col3\" >0.0385</td>\n",
       "                        <td id=\"T_93552_row7_col4\" class=\"data row7 col4\" >0.0606</td>\n",
       "                        <td id=\"T_93552_row7_col5\" class=\"data row7 col5\" >0.0340</td>\n",
       "                        <td id=\"T_93552_row7_col6\" class=\"data row7 col6\" >0.0421</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_93552_row8_col0\" class=\"data row8 col0\" >0.9375</td>\n",
       "                        <td id=\"T_93552_row8_col1\" class=\"data row8 col1\" >0.7783</td>\n",
       "                        <td id=\"T_93552_row8_col2\" class=\"data row8 col2\" >0.2857</td>\n",
       "                        <td id=\"T_93552_row8_col3\" class=\"data row8 col3\" >0.0909</td>\n",
       "                        <td id=\"T_93552_row8_col4\" class=\"data row8 col4\" >0.1379</td>\n",
       "                        <td id=\"T_93552_row8_col5\" class=\"data row8 col5\" >0.1144</td>\n",
       "                        <td id=\"T_93552_row8_col6\" class=\"data row8 col6\" >0.1351</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_93552_row9_col0\" class=\"data row9 col0\" >0.9450</td>\n",
       "                        <td id=\"T_93552_row9_col1\" class=\"data row9 col1\" >0.7419</td>\n",
       "                        <td id=\"T_93552_row9_col2\" class=\"data row9 col2\" >0.5714</td>\n",
       "                        <td id=\"T_93552_row9_col3\" class=\"data row9 col3\" >0.1739</td>\n",
       "                        <td id=\"T_93552_row9_col4\" class=\"data row9 col4\" >0.2667</td>\n",
       "                        <td id=\"T_93552_row9_col5\" class=\"data row9 col5\" >0.2464</td>\n",
       "                        <td id=\"T_93552_row9_col6\" class=\"data row9 col6\" >0.2946</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_93552_row10_col0\" class=\"data row10 col0\" >0.9408</td>\n",
       "                        <td id=\"T_93552_row10_col1\" class=\"data row10 col1\" >0.7846</td>\n",
       "                        <td id=\"T_93552_row10_col2\" class=\"data row10 col2\" >0.2625</td>\n",
       "                        <td id=\"T_93552_row10_col3\" class=\"data row10 col3\" >0.0965</td>\n",
       "                        <td id=\"T_93552_row10_col4\" class=\"data row10 col4\" >0.1400</td>\n",
       "                        <td id=\"T_93552_row10_col5\" class=\"data row10 col5\" >0.1160</td>\n",
       "                        <td id=\"T_93552_row10_col6\" class=\"data row10 col6\" >0.1325</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_93552_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_93552_row11_col0\" class=\"data row11 col0\" >0.0111</td>\n",
       "                        <td id=\"T_93552_row11_col1\" class=\"data row11 col1\" >0.0985</td>\n",
       "                        <td id=\"T_93552_row11_col2\" class=\"data row11 col2\" >0.1868</td>\n",
       "                        <td id=\"T_93552_row11_col3\" class=\"data row11 col3\" >0.0684</td>\n",
       "                        <td id=\"T_93552_row11_col4\" class=\"data row11 col4\" >0.0982</td>\n",
       "                        <td id=\"T_93552_row11_col5\" class=\"data row11 col5\" >0.1002</td>\n",
       "                        <td id=\"T_93552_row11_col6\" class=\"data row11 col6\" >0.1137</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c10ec7a190>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [15:26<00:00, 33.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#ナイーブベイズ版\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_pc_nb_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "shaped-genealogy",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_0a369_row10_col0,#T_0a369_row10_col1,#T_0a369_row10_col2,#T_0a369_row10_col3,#T_0a369_row10_col4,#T_0a369_row10_col5,#T_0a369_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_0a369_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_0a369_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_0a369_row0_col0\" class=\"data row0 col0\" >0.8333</td>\n",
       "                        <td id=\"T_0a369_row0_col1\" class=\"data row0 col1\" >0.4592</td>\n",
       "                        <td id=\"T_0a369_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row0_col4\" class=\"data row0 col4\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row0_col5\" class=\"data row0 col5\" >-0.0340</td>\n",
       "                        <td id=\"T_0a369_row0_col6\" class=\"data row0 col6\" >-0.0573</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_0a369_row1_col0\" class=\"data row1 col0\" >0.8148</td>\n",
       "                        <td id=\"T_0a369_row1_col1\" class=\"data row1 col1\" >0.4565</td>\n",
       "                        <td id=\"T_0a369_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row1_col5\" class=\"data row1 col5\" >-0.0630</td>\n",
       "                        <td id=\"T_0a369_row1_col6\" class=\"data row1 col6\" >-0.0818</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_0a369_row2_col0\" class=\"data row2 col0\" >0.8333</td>\n",
       "                        <td id=\"T_0a369_row2_col1\" class=\"data row2 col1\" >0.7092</td>\n",
       "                        <td id=\"T_0a369_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row2_col5\" class=\"data row2 col5\" >-0.0340</td>\n",
       "                        <td id=\"T_0a369_row2_col6\" class=\"data row2 col6\" >-0.0573</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_0a369_row3_col0\" class=\"data row3 col0\" >0.7963</td>\n",
       "                        <td id=\"T_0a369_row3_col1\" class=\"data row3 col1\" >0.5652</td>\n",
       "                        <td id=\"T_0a369_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row3_col5\" class=\"data row3 col5\" >-0.0879</td>\n",
       "                        <td id=\"T_0a369_row3_col6\" class=\"data row3 col6\" >-0.1011</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_0a369_row4_col0\" class=\"data row4 col0\" >0.8704</td>\n",
       "                        <td id=\"T_0a369_row4_col1\" class=\"data row4 col1\" >0.3967</td>\n",
       "                        <td id=\"T_0a369_row4_col2\" class=\"data row4 col2\" >0.1250</td>\n",
       "                        <td id=\"T_0a369_row4_col3\" class=\"data row4 col3\" >1.0000</td>\n",
       "                        <td id=\"T_0a369_row4_col4\" class=\"data row4 col4\" >0.2222</td>\n",
       "                        <td id=\"T_0a369_row4_col5\" class=\"data row4 col5\" >0.1957</td>\n",
       "                        <td id=\"T_0a369_row4_col6\" class=\"data row4 col6\" >0.3294</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_0a369_row5_col0\" class=\"data row5 col0\" >0.8333</td>\n",
       "                        <td id=\"T_0a369_row5_col1\" class=\"data row5 col1\" >0.5299</td>\n",
       "                        <td id=\"T_0a369_row5_col2\" class=\"data row5 col2\" >0.1250</td>\n",
       "                        <td id=\"T_0a369_row5_col3\" class=\"data row5 col3\" >0.3333</td>\n",
       "                        <td id=\"T_0a369_row5_col4\" class=\"data row5 col4\" >0.1818</td>\n",
       "                        <td id=\"T_0a369_row5_col5\" class=\"data row5 col5\" >0.1099</td>\n",
       "                        <td id=\"T_0a369_row5_col6\" class=\"data row5 col6\" >0.1264</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_0a369_row6_col0\" class=\"data row6 col0\" >0.8704</td>\n",
       "                        <td id=\"T_0a369_row6_col1\" class=\"data row6 col1\" >0.7717</td>\n",
       "                        <td id=\"T_0a369_row6_col2\" class=\"data row6 col2\" >0.1250</td>\n",
       "                        <td id=\"T_0a369_row6_col3\" class=\"data row6 col3\" >1.0000</td>\n",
       "                        <td id=\"T_0a369_row6_col4\" class=\"data row6 col4\" >0.2222</td>\n",
       "                        <td id=\"T_0a369_row6_col5\" class=\"data row6 col5\" >0.1957</td>\n",
       "                        <td id=\"T_0a369_row6_col6\" class=\"data row6 col6\" >0.3294</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_0a369_row7_col0\" class=\"data row7 col0\" >0.8519</td>\n",
       "                        <td id=\"T_0a369_row7_col1\" class=\"data row7 col1\" >0.4149</td>\n",
       "                        <td id=\"T_0a369_row7_col2\" class=\"data row7 col2\" >0.1429</td>\n",
       "                        <td id=\"T_0a369_row7_col3\" class=\"data row7 col3\" >0.3333</td>\n",
       "                        <td id=\"T_0a369_row7_col4\" class=\"data row7 col4\" >0.2000</td>\n",
       "                        <td id=\"T_0a369_row7_col5\" class=\"data row7 col5\" >0.1325</td>\n",
       "                        <td id=\"T_0a369_row7_col6\" class=\"data row7 col6\" >0.1471</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_0a369_row8_col0\" class=\"data row8 col0\" >0.7925</td>\n",
       "                        <td id=\"T_0a369_row8_col1\" class=\"data row8 col1\" >0.3276</td>\n",
       "                        <td id=\"T_0a369_row8_col2\" class=\"data row8 col2\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row8_col3\" class=\"data row8 col3\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row8_col4\" class=\"data row8 col4\" >0.0000</td>\n",
       "                        <td id=\"T_0a369_row8_col5\" class=\"data row8 col5\" >-0.1063</td>\n",
       "                        <td id=\"T_0a369_row8_col6\" class=\"data row8 col6\" >-0.1115</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_0a369_row9_col0\" class=\"data row9 col0\" >0.8868</td>\n",
       "                        <td id=\"T_0a369_row9_col1\" class=\"data row9 col1\" >0.4037</td>\n",
       "                        <td id=\"T_0a369_row9_col2\" class=\"data row9 col2\" >0.1429</td>\n",
       "                        <td id=\"T_0a369_row9_col3\" class=\"data row9 col3\" >1.0000</td>\n",
       "                        <td id=\"T_0a369_row9_col4\" class=\"data row9 col4\" >0.2500</td>\n",
       "                        <td id=\"T_0a369_row9_col5\" class=\"data row9 col5\" >0.2244</td>\n",
       "                        <td id=\"T_0a369_row9_col6\" class=\"data row9 col6\" >0.3555</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_0a369_row10_col0\" class=\"data row10 col0\" >0.8383</td>\n",
       "                        <td id=\"T_0a369_row10_col1\" class=\"data row10 col1\" >0.5035</td>\n",
       "                        <td id=\"T_0a369_row10_col2\" class=\"data row10 col2\" >0.0661</td>\n",
       "                        <td id=\"T_0a369_row10_col3\" class=\"data row10 col3\" >0.3667</td>\n",
       "                        <td id=\"T_0a369_row10_col4\" class=\"data row10 col4\" >0.1076</td>\n",
       "                        <td id=\"T_0a369_row10_col5\" class=\"data row10 col5\" >0.0533</td>\n",
       "                        <td id=\"T_0a369_row10_col6\" class=\"data row10 col6\" >0.0879</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_0a369_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_0a369_row11_col0\" class=\"data row11 col0\" >0.0301</td>\n",
       "                        <td id=\"T_0a369_row11_col1\" class=\"data row11 col1\" >0.1353</td>\n",
       "                        <td id=\"T_0a369_row11_col2\" class=\"data row11 col2\" >0.0664</td>\n",
       "                        <td id=\"T_0a369_row11_col3\" class=\"data row11 col3\" >0.4333</td>\n",
       "                        <td id=\"T_0a369_row11_col4\" class=\"data row11 col4\" >0.1089</td>\n",
       "                        <td id=\"T_0a369_row11_col5\" class=\"data row11 col5\" >0.1239</td>\n",
       "                        <td id=\"T_0a369_row11_col6\" class=\"data row11 col6\" >0.1843</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c13cba06a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [33:25<00:00, 71.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#knnn版\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_pc_knn_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "written-algebra",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_9e222_row10_col0,#T_9e222_row10_col1,#T_9e222_row10_col2,#T_9e222_row10_col3,#T_9e222_row10_col4,#T_9e222_row10_col5,#T_9e222_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_9e222_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_9e222_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_9e222_row0_col0\" class=\"data row0 col0\" >0.8333</td>\n",
       "                        <td id=\"T_9e222_row0_col1\" class=\"data row0 col1\" >0.4470</td>\n",
       "                        <td id=\"T_9e222_row0_col2\" class=\"data row0 col2\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row0_col3\" class=\"data row0 col3\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row0_col4\" class=\"data row0 col4\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row0_col5\" class=\"data row0 col5\" >-0.0340</td>\n",
       "                        <td id=\"T_9e222_row0_col6\" class=\"data row0 col6\" >-0.0573</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_9e222_row1_col0\" class=\"data row1 col0\" >0.8148</td>\n",
       "                        <td id=\"T_9e222_row1_col1\" class=\"data row1 col1\" >0.5027</td>\n",
       "                        <td id=\"T_9e222_row1_col2\" class=\"data row1 col2\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row1_col3\" class=\"data row1 col3\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row1_col4\" class=\"data row1 col4\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row1_col5\" class=\"data row1 col5\" >-0.0630</td>\n",
       "                        <td id=\"T_9e222_row1_col6\" class=\"data row1 col6\" >-0.0818</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_9e222_row2_col0\" class=\"data row2 col0\" >0.8333</td>\n",
       "                        <td id=\"T_9e222_row2_col1\" class=\"data row2 col1\" >0.6644</td>\n",
       "                        <td id=\"T_9e222_row2_col2\" class=\"data row2 col2\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row2_col3\" class=\"data row2 col3\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row2_col4\" class=\"data row2 col4\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row2_col5\" class=\"data row2 col5\" >-0.0340</td>\n",
       "                        <td id=\"T_9e222_row2_col6\" class=\"data row2 col6\" >-0.0573</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_9e222_row3_col0\" class=\"data row3 col0\" >0.7963</td>\n",
       "                        <td id=\"T_9e222_row3_col1\" class=\"data row3 col1\" >0.5571</td>\n",
       "                        <td id=\"T_9e222_row3_col2\" class=\"data row3 col2\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row3_col3\" class=\"data row3 col3\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row3_col4\" class=\"data row3 col4\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row3_col5\" class=\"data row3 col5\" >-0.0879</td>\n",
       "                        <td id=\"T_9e222_row3_col6\" class=\"data row3 col6\" >-0.1011</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_9e222_row4_col0\" class=\"data row4 col0\" >0.8704</td>\n",
       "                        <td id=\"T_9e222_row4_col1\" class=\"data row4 col1\" >0.3940</td>\n",
       "                        <td id=\"T_9e222_row4_col2\" class=\"data row4 col2\" >0.1250</td>\n",
       "                        <td id=\"T_9e222_row4_col3\" class=\"data row4 col3\" >1.0000</td>\n",
       "                        <td id=\"T_9e222_row4_col4\" class=\"data row4 col4\" >0.2222</td>\n",
       "                        <td id=\"T_9e222_row4_col5\" class=\"data row4 col5\" >0.1957</td>\n",
       "                        <td id=\"T_9e222_row4_col6\" class=\"data row4 col6\" >0.3294</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_9e222_row5_col0\" class=\"data row5 col0\" >0.8333</td>\n",
       "                        <td id=\"T_9e222_row5_col1\" class=\"data row5 col1\" >0.5394</td>\n",
       "                        <td id=\"T_9e222_row5_col2\" class=\"data row5 col2\" >0.1250</td>\n",
       "                        <td id=\"T_9e222_row5_col3\" class=\"data row5 col3\" >0.3333</td>\n",
       "                        <td id=\"T_9e222_row5_col4\" class=\"data row5 col4\" >0.1818</td>\n",
       "                        <td id=\"T_9e222_row5_col5\" class=\"data row5 col5\" >0.1099</td>\n",
       "                        <td id=\"T_9e222_row5_col6\" class=\"data row5 col6\" >0.1264</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_9e222_row6_col0\" class=\"data row6 col0\" >0.8704</td>\n",
       "                        <td id=\"T_9e222_row6_col1\" class=\"data row6 col1\" >0.8071</td>\n",
       "                        <td id=\"T_9e222_row6_col2\" class=\"data row6 col2\" >0.1250</td>\n",
       "                        <td id=\"T_9e222_row6_col3\" class=\"data row6 col3\" >1.0000</td>\n",
       "                        <td id=\"T_9e222_row6_col4\" class=\"data row6 col4\" >0.2222</td>\n",
       "                        <td id=\"T_9e222_row6_col5\" class=\"data row6 col5\" >0.1957</td>\n",
       "                        <td id=\"T_9e222_row6_col6\" class=\"data row6 col6\" >0.3294</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_9e222_row7_col0\" class=\"data row7 col0\" >0.8519</td>\n",
       "                        <td id=\"T_9e222_row7_col1\" class=\"data row7 col1\" >0.4027</td>\n",
       "                        <td id=\"T_9e222_row7_col2\" class=\"data row7 col2\" >0.1429</td>\n",
       "                        <td id=\"T_9e222_row7_col3\" class=\"data row7 col3\" >0.3333</td>\n",
       "                        <td id=\"T_9e222_row7_col4\" class=\"data row7 col4\" >0.2000</td>\n",
       "                        <td id=\"T_9e222_row7_col5\" class=\"data row7 col5\" >0.1325</td>\n",
       "                        <td id=\"T_9e222_row7_col6\" class=\"data row7 col6\" >0.1471</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_9e222_row8_col0\" class=\"data row8 col0\" >0.7925</td>\n",
       "                        <td id=\"T_9e222_row8_col1\" class=\"data row8 col1\" >0.3571</td>\n",
       "                        <td id=\"T_9e222_row8_col2\" class=\"data row8 col2\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row8_col3\" class=\"data row8 col3\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row8_col4\" class=\"data row8 col4\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row8_col5\" class=\"data row8 col5\" >-0.1063</td>\n",
       "                        <td id=\"T_9e222_row8_col6\" class=\"data row8 col6\" >-0.1115</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_9e222_row9_col0\" class=\"data row9 col0\" >0.8679</td>\n",
       "                        <td id=\"T_9e222_row9_col1\" class=\"data row9 col1\" >0.4022</td>\n",
       "                        <td id=\"T_9e222_row9_col2\" class=\"data row9 col2\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row9_col3\" class=\"data row9 col3\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row9_col4\" class=\"data row9 col4\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row9_col5\" class=\"data row9 col5\" >0.0000</td>\n",
       "                        <td id=\"T_9e222_row9_col6\" class=\"data row9 col6\" >0.0000</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_9e222_row10_col0\" class=\"data row10 col0\" >0.8364</td>\n",
       "                        <td id=\"T_9e222_row10_col1\" class=\"data row10 col1\" >0.5074</td>\n",
       "                        <td id=\"T_9e222_row10_col2\" class=\"data row10 col2\" >0.0518</td>\n",
       "                        <td id=\"T_9e222_row10_col3\" class=\"data row10 col3\" >0.2667</td>\n",
       "                        <td id=\"T_9e222_row10_col4\" class=\"data row10 col4\" >0.0826</td>\n",
       "                        <td id=\"T_9e222_row10_col5\" class=\"data row10 col5\" >0.0309</td>\n",
       "                        <td id=\"T_9e222_row10_col6\" class=\"data row10 col6\" >0.0523</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_9e222_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_9e222_row11_col0\" class=\"data row11 col0\" >0.0275</td>\n",
       "                        <td id=\"T_9e222_row11_col1\" class=\"data row11 col1\" >0.1338</td>\n",
       "                        <td id=\"T_9e222_row11_col2\" class=\"data row11 col2\" >0.0636</td>\n",
       "                        <td id=\"T_9e222_row11_col3\" class=\"data row11 col3\" >0.3887</td>\n",
       "                        <td id=\"T_9e222_row11_col4\" class=\"data row11 col4\" >0.1018</td>\n",
       "                        <td id=\"T_9e222_row11_col5\" class=\"data row11 col5\" >0.1105</td>\n",
       "                        <td id=\"T_9e222_row11_col6\" class=\"data row11 col6\" >0.1622</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c13cb95d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [11:45<00:00, 25.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#knn版\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_pc_knn_tuneless_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invalid-lucas",
   "metadata": {},
   "source": [
    "## クラスタリングなし，ボート，モータ番号あり版"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "knowing-gospel",
   "metadata": {
    "code_folding": [
     0,
     136,
     272,
     408,
     544,
     680
    ]
   },
   "outputs": [],
   "source": [
    "def model_score_number_pc_et_year(version,years,place_name,result_df):#pycaretを使ってモデリング(et),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('rf')#モデルの生成（Extra Trees Regressor）\n",
    "            tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/num_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_number_pc_et_tuneless_year(version,years,place_name,result_df):#pycaretを使ってモデリング(et),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('rf')#モデルの生成（Extra Trees Regressor）\n",
    "            #tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/num_tuneless_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_number_pc_nb_tuneless_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[100,200,500,700,1000,3000,5000]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('nb')#モデルの生成（Extra Trees Regressor）\n",
    "            #tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/num_tuneless_nb_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_number_pc_nb_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[100,200,500,700,1000,3000,5000]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('nb')#モデルの生成（Extra Trees Regressor）\n",
    "            tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/num_{place_name}_nb_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_number_pc_knn_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('knn')#モデルの生成（Extra Trees Regressor）\n",
    "            tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/num_{place_name}_knn_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_number_pc_knn_tuneless_year(version,years,place_name,result_df):#pycaretを使ってモデリング(Naive beise),年単位で検証\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "    year1=years[0]\n",
    "    year2=years[1]\n",
    "    test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "    test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "    train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "    test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "    train_df=train_df.drop(['year'],axis=1).copy()\n",
    "    #金額の情報は横によけておく\n",
    "    test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "    test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "    train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "        result_test_year1_df=test_year1_df.copy()\n",
    "        result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_test_year2_df=test_year2_df.copy()\n",
    "        result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_year1_df['money']=test_year1_money\n",
    "        result_test_year2_df['money']=test_year2_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        target_per_arr=[5,10,25,50,100,200,500]\n",
    "        #target_per_arr=[20]\n",
    "        for sum_target_per in target_per_arr:\n",
    "            #index=sum_target_per-1\n",
    "            target_per=100+sum_target_per#学習データを増やす\n",
    "            #target_per_arr[index]=target_per\n",
    "            #======================\n",
    "            #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "            # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "            target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "            target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "            len_1=len(target_1_df)\n",
    "\n",
    "            target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "            len_0=len(target_0_df)\n",
    "            target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "            target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "            #学習＆予測ぱーと========================================================================\n",
    "            #==========================================================================================================================================\n",
    "            #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "            target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "    #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "            target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "            target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "            target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "            target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "            target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "            #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "            #exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "            exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10,silent=True)#学習環境の作成\n",
    "\n",
    "            #compare_models( errors=\"raise\")\n",
    "            model=create_model('knn')#モデルの生成（Extra Trees Regressor）\n",
    "            #tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "            #predict_model(tuned_model)\n",
    "            # 未知データに対する予測値\n",
    "            predict_year1_df = predict_model(model, data=target_x_year1_test)\n",
    "            predict_year2_df = predict_model(model, data=target_x_year2_test)\n",
    "            #==========================================================================================================================================\n",
    "            #[1]の正答率を見る\n",
    "            pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year1_test})\n",
    "            pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "                                             , 'trans_result':target_y_year2_test})\n",
    "            year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "            year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "            #/////収益計算の項\n",
    "            year1_trans_df['money']=test_year1_money\n",
    "            year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "            year2_trans_df['money']=test_year2_money\n",
    "            year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "            #配当金の情報も考慮する。\n",
    "            #result_gain_base_df=calc_gain(trans_df)\n",
    "            year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "            year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "            #scoreのseriesに情報書き込み==================\n",
    "            model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "            model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "            model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "            #model_score_s['threshold']=th\n",
    "\n",
    "            result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "            year_labels=[1,2]\n",
    "            #年のごとのスコア情報を横に展開していく\n",
    "            for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "                #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "                model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "                #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "                model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "                #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "                model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "                #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "                model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "                #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "                #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "                model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "                #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "                model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "            model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/num_tuneless_knn_{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-bryan",
   "metadata": {},
   "outputs": [],
   "source": [
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_number_pc_et_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-portfolio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ハイパーパラメータ探索を行わない\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_number_pc_et_tuneless_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "three-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ナイーブベイズ版,ためしにチューニングなし\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_number_pc_nb_tuneless_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interstate-louisville",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ナイーブベイズ版\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_number_pc_nb_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#knnn版\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_number_pc_knn_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secondary-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#knn版\n",
    "#設定する変数=================================================================\n",
    "version_big='v5'#バージョン(大のくくり)\n",
    "version='v5_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "place_name='asiya'\n",
    "#設定する変数=================================================================\n",
    "\n",
    "\n",
    "result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "years=[2019,2020]\n",
    "result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "result_df=result_df.drop('date',axis=1)\n",
    "model_score_number_pc_knn_tuneless_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriented-bottle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-merit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-quarterly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-tuner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interracial-republican",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "south-foundation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-device",
   "metadata": {},
   "source": [
    "## 関数作成用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "julian-judgment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "#T_1942f_row10_col0,#T_1942f_row10_col1,#T_1942f_row10_col2,#T_1942f_row10_col3,#T_1942f_row10_col4,#T_1942f_row10_col5,#T_1942f_row10_col6{\n",
       "            background:  yellow;\n",
       "        }</style><table id=\"T_1942f_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Accuracy</th>        <th class=\"col_heading level0 col1\" >AUC</th>        <th class=\"col_heading level0 col2\" >Recall</th>        <th class=\"col_heading level0 col3\" >Prec.</th>        <th class=\"col_heading level0 col4\" >F1</th>        <th class=\"col_heading level0 col5\" >Kappa</th>        <th class=\"col_heading level0 col6\" >MCC</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_1942f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_1942f_row0_col0\" class=\"data row0 col0\" >0.6462</td>\n",
       "                        <td id=\"T_1942f_row0_col1\" class=\"data row0 col1\" >0.7943</td>\n",
       "                        <td id=\"T_1942f_row0_col2\" class=\"data row0 col2\" >0.2203</td>\n",
       "                        <td id=\"T_1942f_row0_col3\" class=\"data row0 col3\" >1.0000</td>\n",
       "                        <td id=\"T_1942f_row0_col4\" class=\"data row0 col4\" >0.3611</td>\n",
       "                        <td id=\"T_1942f_row0_col5\" class=\"data row0 col5\" >0.2359</td>\n",
       "                        <td id=\"T_1942f_row0_col6\" class=\"data row0 col6\" >0.3657</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_1942f_row1_col0\" class=\"data row1 col0\" >0.6154</td>\n",
       "                        <td id=\"T_1942f_row1_col1\" class=\"data row1 col1\" >0.7641</td>\n",
       "                        <td id=\"T_1942f_row1_col2\" class=\"data row1 col2\" >0.1610</td>\n",
       "                        <td id=\"T_1942f_row1_col3\" class=\"data row1 col3\" >0.9500</td>\n",
       "                        <td id=\"T_1942f_row1_col4\" class=\"data row1 col4\" >0.2754</td>\n",
       "                        <td id=\"T_1942f_row1_col5\" class=\"data row1 col5\" >0.1656</td>\n",
       "                        <td id=\"T_1942f_row1_col6\" class=\"data row1 col6\" >0.2877</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_1942f_row2_col0\" class=\"data row2 col0\" >0.6178</td>\n",
       "                        <td id=\"T_1942f_row2_col1\" class=\"data row2 col1\" >0.7786</td>\n",
       "                        <td id=\"T_1942f_row2_col2\" class=\"data row2 col2\" >0.1695</td>\n",
       "                        <td id=\"T_1942f_row2_col3\" class=\"data row2 col3\" >0.9524</td>\n",
       "                        <td id=\"T_1942f_row2_col4\" class=\"data row2 col4\" >0.2878</td>\n",
       "                        <td id=\"T_1942f_row2_col5\" class=\"data row2 col5\" >0.1741</td>\n",
       "                        <td id=\"T_1942f_row2_col6\" class=\"data row2 col6\" >0.2963</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_1942f_row3_col0\" class=\"data row3 col0\" >0.6062</td>\n",
       "                        <td id=\"T_1942f_row3_col1\" class=\"data row3 col1\" >0.7644</td>\n",
       "                        <td id=\"T_1942f_row3_col2\" class=\"data row3 col2\" >0.1780</td>\n",
       "                        <td id=\"T_1942f_row3_col3\" class=\"data row3 col3\" >0.8077</td>\n",
       "                        <td id=\"T_1942f_row3_col4\" class=\"data row3 col4\" >0.2917</td>\n",
       "                        <td id=\"T_1942f_row3_col5\" class=\"data row3 col5\" >0.1522</td>\n",
       "                        <td id=\"T_1942f_row3_col6\" class=\"data row3 col6\" >0.2362</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_1942f_row4_col0\" class=\"data row4 col0\" >0.5869</td>\n",
       "                        <td id=\"T_1942f_row4_col1\" class=\"data row4 col1\" >0.7678</td>\n",
       "                        <td id=\"T_1942f_row4_col2\" class=\"data row4 col2\" >0.1271</td>\n",
       "                        <td id=\"T_1942f_row4_col3\" class=\"data row4 col3\" >0.7895</td>\n",
       "                        <td id=\"T_1942f_row4_col4\" class=\"data row4 col4\" >0.2190</td>\n",
       "                        <td id=\"T_1942f_row4_col5\" class=\"data row4 col5\" >0.1060</td>\n",
       "                        <td id=\"T_1942f_row4_col6\" class=\"data row4 col6\" >0.1886</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "                        <td id=\"T_1942f_row5_col0\" class=\"data row5 col0\" >0.5869</td>\n",
       "                        <td id=\"T_1942f_row5_col1\" class=\"data row5 col1\" >0.7532</td>\n",
       "                        <td id=\"T_1942f_row5_col2\" class=\"data row5 col2\" >0.1102</td>\n",
       "                        <td id=\"T_1942f_row5_col3\" class=\"data row5 col3\" >0.8667</td>\n",
       "                        <td id=\"T_1942f_row5_col4\" class=\"data row5 col4\" >0.1955</td>\n",
       "                        <td id=\"T_1942f_row5_col5\" class=\"data row5 col5\" >0.1033</td>\n",
       "                        <td id=\"T_1942f_row5_col6\" class=\"data row5 col6\" >0.2047</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "                        <td id=\"T_1942f_row6_col0\" class=\"data row6 col0\" >0.5830</td>\n",
       "                        <td id=\"T_1942f_row6_col1\" class=\"data row6 col1\" >0.7016</td>\n",
       "                        <td id=\"T_1942f_row6_col2\" class=\"data row6 col2\" >0.1356</td>\n",
       "                        <td id=\"T_1942f_row6_col3\" class=\"data row6 col3\" >0.7273</td>\n",
       "                        <td id=\"T_1942f_row6_col4\" class=\"data row6 col4\" >0.2286</td>\n",
       "                        <td id=\"T_1942f_row6_col5\" class=\"data row6 col5\" >0.0997</td>\n",
       "                        <td id=\"T_1942f_row6_col6\" class=\"data row6 col6\" >0.1662</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "                        <td id=\"T_1942f_row7_col0\" class=\"data row7 col0\" >0.5174</td>\n",
       "                        <td id=\"T_1942f_row7_col1\" class=\"data row7 col1\" >0.4925</td>\n",
       "                        <td id=\"T_1942f_row7_col2\" class=\"data row7 col2\" >0.1610</td>\n",
       "                        <td id=\"T_1942f_row7_col3\" class=\"data row7 col3\" >0.4222</td>\n",
       "                        <td id=\"T_1942f_row7_col4\" class=\"data row7 col4\" >0.2331</td>\n",
       "                        <td id=\"T_1942f_row7_col5\" class=\"data row7 col5\" >-0.0246</td>\n",
       "                        <td id=\"T_1942f_row7_col6\" class=\"data row7 col6\" >-0.0307</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "                        <td id=\"T_1942f_row8_col0\" class=\"data row8 col0\" >0.5521</td>\n",
       "                        <td id=\"T_1942f_row8_col1\" class=\"data row8 col1\" >0.4783</td>\n",
       "                        <td id=\"T_1942f_row8_col2\" class=\"data row8 col2\" >0.0763</td>\n",
       "                        <td id=\"T_1942f_row8_col3\" class=\"data row8 col3\" >0.5625</td>\n",
       "                        <td id=\"T_1942f_row8_col4\" class=\"data row8 col4\" >0.1343</td>\n",
       "                        <td id=\"T_1942f_row8_col5\" class=\"data row8 col5\" >0.0286</td>\n",
       "                        <td id=\"T_1942f_row8_col6\" class=\"data row8 col6\" >0.0551</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "                        <td id=\"T_1942f_row9_col0\" class=\"data row9 col0\" >0.5212</td>\n",
       "                        <td id=\"T_1942f_row9_col1\" class=\"data row9 col1\" >0.4801</td>\n",
       "                        <td id=\"T_1942f_row9_col2\" class=\"data row9 col2\" >0.0855</td>\n",
       "                        <td id=\"T_1942f_row9_col3\" class=\"data row9 col3\" >0.3704</td>\n",
       "                        <td id=\"T_1942f_row9_col4\" class=\"data row9 col4\" >0.1389</td>\n",
       "                        <td id=\"T_1942f_row9_col5\" class=\"data row9 col5\" >-0.0367</td>\n",
       "                        <td id=\"T_1942f_row9_col6\" class=\"data row9 col6\" >-0.0558</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "                        <td id=\"T_1942f_row10_col0\" class=\"data row10 col0\" >0.5833</td>\n",
       "                        <td id=\"T_1942f_row10_col1\" class=\"data row10 col1\" >0.6775</td>\n",
       "                        <td id=\"T_1942f_row10_col2\" class=\"data row10 col2\" >0.1424</td>\n",
       "                        <td id=\"T_1942f_row10_col3\" class=\"data row10 col3\" >0.7449</td>\n",
       "                        <td id=\"T_1942f_row10_col4\" class=\"data row10 col4\" >0.2365</td>\n",
       "                        <td id=\"T_1942f_row10_col5\" class=\"data row10 col5\" >0.1004</td>\n",
       "                        <td id=\"T_1942f_row10_col6\" class=\"data row10 col6\" >0.1714</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_1942f_level0_row11\" class=\"row_heading level0 row11\" >SD</th>\n",
       "                        <td id=\"T_1942f_row11_col0\" class=\"data row11 col0\" >0.0399</td>\n",
       "                        <td id=\"T_1942f_row11_col1\" class=\"data row11 col1\" >0.1289</td>\n",
       "                        <td id=\"T_1942f_row11_col2\" class=\"data row11 col2\" >0.0421</td>\n",
       "                        <td id=\"T_1942f_row11_col3\" class=\"data row11 col3\" >0.2119</td>\n",
       "                        <td id=\"T_1942f_row11_col4\" class=\"data row11 col4\" >0.0669</td>\n",
       "                        <td id=\"T_1942f_row11_col5\" class=\"data row11 col5\" >0.0839</td>\n",
       "                        <td id=\"T_1942f_row11_col6\" class=\"data row11 col6\" >0.1336</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c108c39a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|██▉                                                                                | 1/28 [01:12<32:24, 72.02s/it]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e0e63f349fd46eca34f6785ac435883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Processing: ', max=3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>09:31:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Finalizing Model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>K Neighbors Classifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      \n",
       "                                                                      \n",
       "Initiated  . . . . . . . . . . . . . . . . . .                09:31:22\n",
       "Status     . . . . . . . . . . . . . . . . . .        Finalizing Model\n",
       "Estimator  . . . . . . . . . . . . . . . . . .  K Neighbors Classifier"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10a9cc34d114ab082301e92e1ef1d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value=\"Following data types have been inferred automatically, if they are correct press enter to continue…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>racer_1_rank</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_1_age</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_1_doub</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_1_ave_st</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_2_rank</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_2_age</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_2_doub</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_2_ave_st</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_3_rank</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_3_age</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_3_doub</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_3_ave_st</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_4_rank</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_4_age</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_4_doub</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_4_ave_st</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_5_rank</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_5_age</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_5_doub</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_5_ave_st</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_6_rank</th>\n",
       "      <td>Categorical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_6_age</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_6_doub</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_6_ave_st</th>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_2</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_4</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_5</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_6</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_7</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_8</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_9</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_10</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_11</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>race_12</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_1_male_0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_1_male_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_2_male_0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_2_male_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_3_male_0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_3_male_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_4_male_0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_4_male_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_5_male_0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_5_male_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_6_male_0</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>racer_6_male_1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>result_com</th>\n",
       "      <td>Label</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Data Type\n",
       "racer_1_rank    Categorical\n",
       "racer_1_age         Numeric\n",
       "racer_1_doub        Numeric\n",
       "racer_1_ave_st      Numeric\n",
       "racer_2_rank    Categorical\n",
       "racer_2_age         Numeric\n",
       "racer_2_doub        Numeric\n",
       "racer_2_ave_st      Numeric\n",
       "racer_3_rank    Categorical\n",
       "racer_3_age         Numeric\n",
       "racer_3_doub        Numeric\n",
       "racer_3_ave_st      Numeric\n",
       "racer_4_rank    Categorical\n",
       "racer_4_age         Numeric\n",
       "racer_4_doub        Numeric\n",
       "racer_4_ave_st      Numeric\n",
       "racer_5_rank    Categorical\n",
       "racer_5_age         Numeric\n",
       "racer_5_doub        Numeric\n",
       "racer_5_ave_st      Numeric\n",
       "racer_6_rank    Categorical\n",
       "racer_6_age         Numeric\n",
       "racer_6_doub        Numeric\n",
       "racer_6_ave_st      Numeric\n",
       "race_1                     \n",
       "race_2                     \n",
       "race_3                     \n",
       "race_4                     \n",
       "race_5                     \n",
       "race_6                     \n",
       "race_7                     \n",
       "race_8                     \n",
       "race_9                     \n",
       "race_10                    \n",
       "race_11                    \n",
       "race_12                    \n",
       "racer_1_male_0             \n",
       "racer_1_male_1             \n",
       "racer_2_male_0             \n",
       "racer_2_male_1             \n",
       "racer_3_male_0             \n",
       "racer_3_male_1             \n",
       "racer_4_male_0             \n",
       "racer_4_male_1             \n",
       "racer_5_male_0             \n",
       "racer_5_male_1             \n",
       "racer_6_male_0             \n",
       "racer_6_male_1             \n",
       "result_com            Label"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▉                                                                               | 1/28 [01:49<49:08, 109.20s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-df8a8d51a69d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m#モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_com_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'result_com'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99999\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata_split_shuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#学習環境の作成\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;31m#compare_models( errors=\"raise\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'rf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#モデルの生成（Extra Trees Regressor）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\classification.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, fix_imbalance, fix_imbalance_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile, profile_kwargs)\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[0mlog_plots\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"auc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"confusion_matrix\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"feature\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m     return pycaret.internal.tabular.setup(\n\u001b[0m\u001b[0;32m    582\u001b[0m         \u001b[0mml_usecase\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"classification\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mavailable_plots\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mavailable_plots\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\internal\\tabular.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(data, target, ml_usecase, available_plots, train_size, test_data, preprocess, imputation_type, iterative_imputation_iters, categorical_features, categorical_imputation, categorical_iterative_imputer, ordinal_features, high_cardinality_features, high_cardinality_method, numeric_features, numeric_imputation, numeric_iterative_imputer, date_features, ignore_features, normalize, normalize_method, transformation, transformation_method, handle_unknown_categorical, unknown_categorical_method, pca, pca_method, pca_components, ignore_low_variance, combine_rare_levels, rare_level_threshold, bin_numeric_features, remove_outliers, outliers_threshold, remove_multicollinearity, multicollinearity_threshold, remove_perfect_collinearity, create_clusters, cluster_iter, polynomial_features, polynomial_degree, trigonometry_features, polynomial_threshold, group_features, group_names, feature_selection, feature_selection_threshold, feature_selection_method, feature_interaction, feature_ratio, interaction_threshold, fix_imbalance, fix_imbalance_method, transform_target, transform_target_method, data_split_shuffle, data_split_stratify, fold_strategy, fold, fold_shuffle, fold_groups, n_jobs, use_gpu, custom_pipeline, html, session_id, log_experiment, experiment_name, log_plots, log_profile, log_data, silent, verbose, profile, profile_kwargs, display)\u001b[0m\n\u001b[0;32m   1349\u001b[0m             \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1351\u001b[1;33m         \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprep_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1352\u001b[0m         \u001b[1;31m# workaround to also transform target\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m         \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_training_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    365\u001b[0m         \"\"\"\n\u001b[0;32m    366\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 367\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\internal\\preprocess.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, dataset, y)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[1;31m# since this is for training , we dont nees any transformation since it has already been transformed in fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;31m# additionally we just need to treat the target variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\internal\\preprocess.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, y)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_print_out\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Data Type\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 323\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             if self.response in [\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m    855\u001b[0m                 \u001b[1;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    856\u001b[0m             )\n\u001b[1;32m--> 857\u001b[1;33m         return self._input_request(str(prompt),\n\u001b[0m\u001b[0;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\ipykernel\\kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m    899\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m                 \u001b[1;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 901\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Interrupted by user\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    902\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    903\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid Message:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# #設定する変数=================================================================\n",
    "# version='V5'#バージョン\n",
    "\n",
    "# place_master=master.get_place_master()\n",
    "# place_name='asiya'\n",
    "# #設定する変数=================================================================\n",
    "\n",
    "\n",
    "# result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "# result_base_df=pd.read_csv(result_filepath)\n",
    "# result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# years=[2019,2020]\n",
    "# result_df=data_making_none_clustar(result_base_df)#モデル関連に使用するdfの作成関数，要は前処理を一気にやってくれる関数(クラスタリング無し、モータ番号、艇番号なし)\n",
    "# result_df=result_df.drop('date',axis=1)\n",
    "# #model_score_pc_et_year(version,years,place_name,result_df)#pycaretを使ってモデリング(et),年単位で検証\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(place_name)\n",
    "# #result_dfは加工関数にて分けられたものを渡す。\n",
    "# model_score_df=pd.DataFrame(columns=['target_com','target_per','threshold','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'])#スコアを格納するdf\n",
    "\n",
    "# year1=years[0]\n",
    "# year2=years[1]\n",
    "# test_year1_df= result_df[(result_df['year']==year1)].copy()#2019のデータ\n",
    "# test_year2_df= result_df[(result_df['year']==year2)].copy()#2020のデータ\n",
    "\n",
    "# train_df =  result_df[(result_df['year']!=year1) & (result_df['year']!=year2)].copy()#そのほかを学習データに\n",
    "# #学習データを切り分けたらyearはいらないから削除する\n",
    "# test_year1_df=test_year1_df.drop(['year'],axis=1).copy()\n",
    "# test_year2_df=test_year2_df.drop(['year'],axis=1).copy()\n",
    "\n",
    "# train_df=train_df.drop(['year'],axis=1).copy()\n",
    "# #金額の情報は横によけておく\n",
    "# test_year1_money=pd.Series(test_year1_df['money']).copy()\n",
    "# test_year2_money=pd.Series(test_year2_df['money']).copy()\n",
    "# train_money=pd.Series(train_df['money']).copy()\n",
    "\n",
    "# #出現数の分布\n",
    "# result_com_s=train_df['result_com'].value_counts()\n",
    "# result_com_s=result_com_s.sort_index()\n",
    "# result_com_df=pd.DataFrame({'result_com':result_com_s.index})\n",
    "# result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "# for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "#     result_com=result_com_number\n",
    "\n",
    "#     #result_comごとの閾値の決定========================================================================\n",
    "#     result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "#     #===============================================================================\n",
    "#     #学習データのラベル変換==========================================================\n",
    "#     result_train_df=train_df.copy()\n",
    "#     result_train_df=trans_result_com(result_com,result_train_df)\n",
    "#     result_test_year1_df=test_year1_df.copy()\n",
    "#     result_test_year1_df=trans_result_com(result_com,result_test_year1_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "#     result_test_year2_df=test_year2_df.copy()\n",
    "#     result_test_year2_df=trans_result_com(result_com,result_test_year2_df)#対象のラベルを１、それ以外を０に変換する関数\n",
    "#     result_train_df['money']=train_money\n",
    "#     result_test_year1_df['money']=test_year1_money\n",
    "#     result_test_year2_df['money']=test_year2_money\n",
    "#     #学習データラベル変換終わり============================================\n",
    "\n",
    "#     #target_per_arr=[5,10,25,50,100,200,500]\n",
    "#     target_per_arr=[20]\n",
    "#     for sum_target_per in target_per_arr:\n",
    "#         #index=sum_target_per-1\n",
    "#         target_per=100+sum_target_per#学習データを増やす\n",
    "#         #target_per_arr[index]=target_per\n",
    "#         #======================\n",
    "#         #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "#         # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "#         target_df=result_train_df.copy()#ベースのデータフレームをコピー\n",
    "#         target_1_df=target_df[target_df['result_com']==1].copy()\n",
    "#         len_1=len(target_1_df)\n",
    "        \n",
    "#         target_0_df=target_df[target_df['result_com']==0].copy()\n",
    "#         len_0=len(target_0_df)\n",
    "#         target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0].copy()#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "#         target_train_df=pd.concat([target_1_df, target_0_df]).copy()\n",
    "#         #学習＆予測ぱーと========================================================================\n",
    "#         #==========================================================================================================================================\n",
    "#         #データの切り分け(pycaretは引数でyを指定するので，予測対象のresult_comは分別する必要はない，収益moneyは予測の変数として　用意できないので消す.)\n",
    "#         target_com_train=target_train_df.drop('money',axis=1).copy()\n",
    "# #             target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "# #             target_y_train=target_train_df['result_com'].copy()\n",
    "\n",
    "#         target_x_year1_test=result_test_year1_df.drop('money',axis=1).copy()\n",
    "#         target_x_year1_test=target_x_year1_test.drop('result_com',axis=1)\n",
    "\n",
    "#         target_x_year2_test=result_test_year2_df.drop('money',axis=1).copy()\n",
    "#         target_x_year2_test=target_x_year2_test.drop('result_com',axis=1)\n",
    "\n",
    "#         target_y_year1_test=result_test_year1_df['result_com'].copy()\n",
    "#         target_y_year2_test=result_test_year2_df['result_com'].copy()\n",
    "\n",
    "#         #モデリングを行う，行う対象はtargetのcomを1,0でラベル変換したdfに対して．\n",
    "#         exp = setup(target_com_train,target='result_com',use_gpu=True,train_size=0.99999,data_split_shuffle=False,n_jobs=10)#学習環境の作成\n",
    "#         #compare_models( errors=\"raise\")\n",
    "#         model=create_model('rf')#モデルの生成（Extra Trees Regressor）\n",
    "#         tuned_model=tune_model(model,optimize ='Precision',n_iter = 100)#生成したモデルをもとにハイパーパラメータチューニングを行う\n",
    "#         #predict_model(tuned_model)\n",
    "#         # 未知データに対する予測値\n",
    "#         predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "#         predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "#         #==========================================================================================================================================\n",
    "#         #[1]の正答率を見る\n",
    "#         pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "#                                          , 'trans_result':target_y_year1_test})\n",
    "#         pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "#                                          , 'trans_result':target_y_year2_test})\n",
    "#         year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "#         year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "#         #/////収益計算の項\n",
    "#         year1_trans_df['money']=test_year1_money\n",
    "#         year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "#         year2_trans_df['money']=test_year2_money\n",
    "#         year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "#         #配当金の情報も考慮する。\n",
    "#         #result_gain_base_df=calc_gain(trans_df)\n",
    "#         year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "#         year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "#         #scoreのseriesに情報書き込み==================\n",
    "#         model_score_s=pd.Series(index=['target_com','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "#         model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "#         model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "#         #model_score_s['threshold']=th\n",
    "\n",
    "#         result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "#         year_labels=[1,2]\n",
    "#         #年のごとのスコア情報を横に展開していく\n",
    "#         for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "#             #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "#             model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "#             #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "#             model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "#             #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "#             model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "#             #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "#             model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "#             #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "#             model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "#             #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "#             model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "#             #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "#             model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "#         model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "#     #モデルの「スコアを保存\n",
    "#     dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "#     model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "# #return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "public-asset",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataTypes_Auto_infer' object has no attribute 'final_training_columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-5ff997a8cec5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 未知データに対する予測値\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpredict_year1_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuned_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_x_year1_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mpredict_year2_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtuned_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_x_year2_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#==========================================================================================================================================\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#[1]の正答率を見る\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\classification.py\u001b[0m in \u001b[0;36mpredict_model\u001b[1;34m(estimator, data, probability_threshold, encoded_labels, raw_score, round, verbose)\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \"\"\"\n\u001b[0;32m   2004\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2005\u001b[1;33m     return pycaret.internal.tabular.predict_model(\n\u001b[0m\u001b[0;32m   2006\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\internal\\tabular.py\u001b[0m in \u001b[0;36mpredict_model\u001b[1;34m(estimator, data, probability_threshold, encoded_labels, raw_score, round, verbose, ml_usecase, display)\u001b[0m\n\u001b[0;32m   8747\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_estimator_from_meta_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 8749\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan_to_num\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   8750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8751\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\pycaret\\internal\\preprocess.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, dataset, y)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;31m# drop any columns that were asked to drop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures_todrop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinal_training_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;31m# also make sure that all the column names are string\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataTypes_Auto_infer' object has no attribute 'final_training_columns'"
     ]
    }
   ],
   "source": [
    "# # 未知データに対する予測値\n",
    "# predict_year1_df = predict_model(tuned_model, data=target_x_year1_test)\n",
    "# predict_year2_df = predict_model(tuned_model, data=target_x_year2_test)\n",
    "# #==========================================================================================================================================\n",
    "# #[1]の正答率を見る\n",
    "# pred_year1_test_df=pd.DataFrame({'pred':predict_year1_df['Label'].values#確率分布での出力\n",
    "#                                  , 'trans_result':target_y_year1_test})\n",
    "# pred_year2_test_df=pd.DataFrame({'pred':predict_year2_df['Label'].values#確率分布での出力\n",
    "#                                  , 'trans_result':target_y_year2_test})\n",
    "# year1_trans_df=pred_year1_test_df.copy()#閾値での変換は行わない\n",
    "# year2_trans_df=pred_year2_test_df.copy()#閾値での変換は行わない\n",
    "# #/////収益計算の項\n",
    "# year1_trans_df['money']=test_year1_money\n",
    "# year1_trans_df['true_result']=test_year1_df['result_com']\n",
    "# year2_trans_df['money']=test_year2_money\n",
    "# year2_trans_df['true_result']=test_year2_df['result_com']\n",
    "# #配当金の情報も考慮する。\n",
    "# #result_gain_base_df=calc_gain(trans_df)\n",
    "# year1_result_gain_base_df=calc_gain(year1_trans_df)\n",
    "# year2_result_gain_base_df=calc_gain(year2_trans_df)\n",
    "\n",
    "# #scoreのseriesに情報書き込み==================\n",
    "# model_score_s=pd.Series(index=['target_com','depth','target_per','total_get_year1', 'total_use_year1','num_com_year1','num_pred_year1','num_hit_year1','buy_hit_per_year1','gain_year1','total_get_year2', 'total_use_year2','num_com_year2','num_pred_year2','num_hit_year2','buy_hit_per_year2','gain_year2','gain_year3'],dtype='float64')\n",
    "# model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "# model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "# model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "# #model_score_s['threshold']=th\n",
    "\n",
    "# result_gain_df_arr=[year1_result_gain_base_df,year2_result_gain_base_df]\n",
    "# year_labels=[1,2]\n",
    "# #年のごとのスコア情報を横に展開していく\n",
    "# for year_df,label in zip(result_gain_df_arr,year_labels):\n",
    "#     #model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "#     model_score_s['total_get_year{year}'.format(year=label)]=year_df[\"gain\"].sum()\n",
    "#     #model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "#     model_score_s['total_use_year{year}'.format(year=label)]=100*year_df[\"pred\"].sum()\n",
    "#     #model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "#     model_score_s['num_com_year{year}'.format(year=label)]=year_df['trans_result'].sum()\n",
    "#     #model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "#     model_score_s['num_pred_year{year}'.format(year=label)]=year_df['pred'].sum()\n",
    "#     #model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "#     model_score_s['gain_year{year}'.format(year=label)]=(model_score_s['total_get_year{year}'.format(year=label)]/model_score_s['total_use_year{year}'.format(year=label)])*100\n",
    "#     #model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "#     model_score_s['num_hit_year{year}'.format(year=label)]=year_df['hit'].sum()\n",
    "#     #model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "#     model_score_s['buy_hit_per_year{year}'.format(year=label)]=(model_score_s['num_hit_year{year}'.format(year=label)]/ model_score_s['num_pred_year{year}'.format(year=label)])*100\n",
    "# #             model_score_df=model_score_df.append(model_score_s,ignore_index=True,sort=False)\n",
    "# #     #モデルの「スコアを保存\n",
    "# #     dir_path =  \"../../bot_database/{place_name}/model_score_{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "# #     model_score_df.to_csv(dir_path, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dramatic-solomon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_573b8_ th {\n",
       "          text-align: left;\n",
       "    }#T_573b8_row0_col0,#T_573b8_row0_col2,#T_573b8_row0_col3,#T_573b8_row0_col4,#T_573b8_row0_col5,#T_573b8_row0_col6,#T_573b8_row0_col7,#T_573b8_row1_col0,#T_573b8_row1_col1,#T_573b8_row1_col2,#T_573b8_row1_col3,#T_573b8_row1_col4,#T_573b8_row1_col5,#T_573b8_row1_col6,#T_573b8_row1_col7,#T_573b8_row2_col0,#T_573b8_row2_col1,#T_573b8_row2_col2,#T_573b8_row2_col3,#T_573b8_row2_col4,#T_573b8_row3_col0,#T_573b8_row3_col1,#T_573b8_row3_col3,#T_573b8_row3_col4,#T_573b8_row3_col5,#T_573b8_row3_col6,#T_573b8_row3_col7,#T_573b8_row4_col0,#T_573b8_row4_col1,#T_573b8_row4_col2,#T_573b8_row4_col3,#T_573b8_row4_col4,#T_573b8_row4_col5,#T_573b8_row4_col6,#T_573b8_row4_col7,#T_573b8_row5_col0,#T_573b8_row5_col1,#T_573b8_row5_col2,#T_573b8_row5_col3,#T_573b8_row5_col5,#T_573b8_row5_col6,#T_573b8_row5_col7,#T_573b8_row6_col0,#T_573b8_row6_col1,#T_573b8_row6_col2,#T_573b8_row6_col3,#T_573b8_row6_col4,#T_573b8_row6_col5,#T_573b8_row6_col6,#T_573b8_row6_col7,#T_573b8_row7_col0,#T_573b8_row7_col1,#T_573b8_row7_col2,#T_573b8_row7_col3,#T_573b8_row7_col4,#T_573b8_row7_col5,#T_573b8_row7_col6,#T_573b8_row7_col7,#T_573b8_row8_col0,#T_573b8_row8_col1,#T_573b8_row8_col2,#T_573b8_row8_col3,#T_573b8_row8_col4,#T_573b8_row8_col5,#T_573b8_row8_col6,#T_573b8_row8_col7,#T_573b8_row9_col0,#T_573b8_row9_col1,#T_573b8_row9_col2,#T_573b8_row9_col3,#T_573b8_row9_col4,#T_573b8_row9_col5,#T_573b8_row9_col6,#T_573b8_row9_col7,#T_573b8_row10_col0,#T_573b8_row10_col1,#T_573b8_row10_col2,#T_573b8_row10_col3,#T_573b8_row10_col4,#T_573b8_row10_col5,#T_573b8_row10_col6,#T_573b8_row10_col7,#T_573b8_row11_col0,#T_573b8_row11_col1,#T_573b8_row11_col2,#T_573b8_row11_col3,#T_573b8_row11_col4,#T_573b8_row11_col5,#T_573b8_row11_col6,#T_573b8_row11_col7,#T_573b8_row12_col0,#T_573b8_row12_col1,#T_573b8_row12_col2,#T_573b8_row12_col3,#T_573b8_row12_col4,#T_573b8_row12_col5,#T_573b8_row12_col6,#T_573b8_row12_col7,#T_573b8_row13_col0,#T_573b8_row13_col1,#T_573b8_row13_col2,#T_573b8_row13_col3,#T_573b8_row13_col4,#T_573b8_row13_col5,#T_573b8_row13_col6,#T_573b8_row13_col7,#T_573b8_row14_col0,#T_573b8_row14_col1,#T_573b8_row14_col2,#T_573b8_row14_col4,#T_573b8_row14_col5,#T_573b8_row14_col6,#T_573b8_row14_col7{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "        }#T_573b8_row0_col1,#T_573b8_row2_col5,#T_573b8_row2_col6,#T_573b8_row2_col7,#T_573b8_row3_col2,#T_573b8_row5_col4,#T_573b8_row14_col3{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  yellow;\n",
       "        }#T_573b8_row0_col8,#T_573b8_row1_col8,#T_573b8_row2_col8,#T_573b8_row3_col8,#T_573b8_row4_col8,#T_573b8_row5_col8,#T_573b8_row6_col8,#T_573b8_row7_col8,#T_573b8_row8_col8,#T_573b8_row10_col8,#T_573b8_row11_col8,#T_573b8_row12_col8,#T_573b8_row13_col8,#T_573b8_row14_col8{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  lightgrey;\n",
       "        }#T_573b8_row9_col8{\n",
       "            text-align:  left;\n",
       "            text-align:  left;\n",
       "            background-color:  yellow;\n",
       "            background-color:  lightgrey;\n",
       "        }</style><table id=\"T_573b8_\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >Model</th>        <th class=\"col_heading level0 col1\" >Accuracy</th>        <th class=\"col_heading level0 col2\" >AUC</th>        <th class=\"col_heading level0 col3\" >Recall</th>        <th class=\"col_heading level0 col4\" >Prec.</th>        <th class=\"col_heading level0 col5\" >F1</th>        <th class=\"col_heading level0 col6\" >Kappa</th>        <th class=\"col_heading level0 col7\" >MCC</th>        <th class=\"col_heading level0 col8\" >TT (Sec)</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_573b8_level0_row0\" class=\"row_heading level0 row0\" >gbc</th>\n",
       "                        <td id=\"T_573b8_row0_col0\" class=\"data row0 col0\" >Gradient Boosting Classifier</td>\n",
       "                        <td id=\"T_573b8_row0_col1\" class=\"data row0 col1\" >0.7401</td>\n",
       "                        <td id=\"T_573b8_row0_col2\" class=\"data row0 col2\" >0.7572</td>\n",
       "                        <td id=\"T_573b8_row0_col3\" class=\"data row0 col3\" >0.4435</td>\n",
       "                        <td id=\"T_573b8_row0_col4\" class=\"data row0 col4\" >0.6639</td>\n",
       "                        <td id=\"T_573b8_row0_col5\" class=\"data row0 col5\" >0.5223</td>\n",
       "                        <td id=\"T_573b8_row0_col6\" class=\"data row0 col6\" >0.3583</td>\n",
       "                        <td id=\"T_573b8_row0_col7\" class=\"data row0 col7\" >0.3752</td>\n",
       "                        <td id=\"T_573b8_row0_col8\" class=\"data row0 col8\" >0.5480</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row1\" class=\"row_heading level0 row1\" >lda</th>\n",
       "                        <td id=\"T_573b8_row1_col0\" class=\"data row1 col0\" >Linear Discriminant Analysis</td>\n",
       "                        <td id=\"T_573b8_row1_col1\" class=\"data row1 col1\" >0.7348</td>\n",
       "                        <td id=\"T_573b8_row1_col2\" class=\"data row1 col2\" >0.7497</td>\n",
       "                        <td id=\"T_573b8_row1_col3\" class=\"data row1 col3\" >0.4625</td>\n",
       "                        <td id=\"T_573b8_row1_col4\" class=\"data row1 col4\" >0.6357</td>\n",
       "                        <td id=\"T_573b8_row1_col5\" class=\"data row1 col5\" >0.5276</td>\n",
       "                        <td id=\"T_573b8_row1_col6\" class=\"data row1 col6\" >0.3537</td>\n",
       "                        <td id=\"T_573b8_row1_col7\" class=\"data row1 col7\" >0.3653</td>\n",
       "                        <td id=\"T_573b8_row1_col8\" class=\"data row1 col8\" >0.0180</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row2\" class=\"row_heading level0 row2\" >xgboost</th>\n",
       "                        <td id=\"T_573b8_row2_col0\" class=\"data row2 col0\" >Extreme Gradient Boosting</td>\n",
       "                        <td id=\"T_573b8_row2_col1\" class=\"data row2 col1\" >0.7333</td>\n",
       "                        <td id=\"T_573b8_row2_col2\" class=\"data row2 col2\" >0.7549</td>\n",
       "                        <td id=\"T_573b8_row2_col3\" class=\"data row2 col3\" >0.5276</td>\n",
       "                        <td id=\"T_573b8_row2_col4\" class=\"data row2 col4\" >0.6121</td>\n",
       "                        <td id=\"T_573b8_row2_col5\" class=\"data row2 col5\" >0.5588</td>\n",
       "                        <td id=\"T_573b8_row2_col6\" class=\"data row2 col6\" >0.3726</td>\n",
       "                        <td id=\"T_573b8_row2_col7\" class=\"data row2 col7\" >0.3784</td>\n",
       "                        <td id=\"T_573b8_row2_col8\" class=\"data row2 col8\" >0.6400</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row3\" class=\"row_heading level0 row3\" >lightgbm</th>\n",
       "                        <td id=\"T_573b8_row3_col0\" class=\"data row3 col0\" >Light Gradient Boosting Machine</td>\n",
       "                        <td id=\"T_573b8_row3_col1\" class=\"data row3 col1\" >0.7330</td>\n",
       "                        <td id=\"T_573b8_row3_col2\" class=\"data row3 col2\" >0.7587</td>\n",
       "                        <td id=\"T_573b8_row3_col3\" class=\"data row3 col3\" >0.5018</td>\n",
       "                        <td id=\"T_573b8_row3_col4\" class=\"data row3 col4\" >0.6139</td>\n",
       "                        <td id=\"T_573b8_row3_col5\" class=\"data row3 col5\" >0.5451</td>\n",
       "                        <td id=\"T_573b8_row3_col6\" class=\"data row3 col6\" >0.3629</td>\n",
       "                        <td id=\"T_573b8_row3_col7\" class=\"data row3 col7\" >0.3693</td>\n",
       "                        <td id=\"T_573b8_row3_col8\" class=\"data row3 col8\" >0.0680</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row4\" class=\"row_heading level0 row4\" >et</th>\n",
       "                        <td id=\"T_573b8_row4_col0\" class=\"data row4 col0\" >Extra Trees Classifier</td>\n",
       "                        <td id=\"T_573b8_row4_col1\" class=\"data row4 col1\" >0.7263</td>\n",
       "                        <td id=\"T_573b8_row4_col2\" class=\"data row4 col2\" >0.7493</td>\n",
       "                        <td id=\"T_573b8_row4_col3\" class=\"data row4 col3\" >0.3450</td>\n",
       "                        <td id=\"T_573b8_row4_col4\" class=\"data row4 col4\" >0.6501</td>\n",
       "                        <td id=\"T_573b8_row4_col5\" class=\"data row4 col5\" >0.4430</td>\n",
       "                        <td id=\"T_573b8_row4_col6\" class=\"data row4 col6\" >0.2928</td>\n",
       "                        <td id=\"T_573b8_row4_col7\" class=\"data row4 col7\" >0.3177</td>\n",
       "                        <td id=\"T_573b8_row4_col8\" class=\"data row4 col8\" >0.2700</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row5\" class=\"row_heading level0 row5\" >rf</th>\n",
       "                        <td id=\"T_573b8_row5_col0\" class=\"data row5 col0\" >Random Forest Classifier</td>\n",
       "                        <td id=\"T_573b8_row5_col1\" class=\"data row5 col1\" >0.7203</td>\n",
       "                        <td id=\"T_573b8_row5_col2\" class=\"data row5 col2\" >0.7468</td>\n",
       "                        <td id=\"T_573b8_row5_col3\" class=\"data row5 col3\" >0.3315</td>\n",
       "                        <td id=\"T_573b8_row5_col4\" class=\"data row5 col4\" >0.6655</td>\n",
       "                        <td id=\"T_573b8_row5_col5\" class=\"data row5 col5\" >0.4329</td>\n",
       "                        <td id=\"T_573b8_row5_col6\" class=\"data row5 col6\" >0.2787</td>\n",
       "                        <td id=\"T_573b8_row5_col7\" class=\"data row5 col7\" >0.3110</td>\n",
       "                        <td id=\"T_573b8_row5_col8\" class=\"data row5 col8\" >0.2600</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row6\" class=\"row_heading level0 row6\" >ada</th>\n",
       "                        <td id=\"T_573b8_row6_col0\" class=\"data row6 col0\" >Ada Boost Classifier</td>\n",
       "                        <td id=\"T_573b8_row6_col1\" class=\"data row6 col1\" >0.7188</td>\n",
       "                        <td id=\"T_573b8_row6_col2\" class=\"data row6 col2\" >0.7325</td>\n",
       "                        <td id=\"T_573b8_row6_col3\" class=\"data row6 col3\" >0.4817</td>\n",
       "                        <td id=\"T_573b8_row6_col4\" class=\"data row6 col4\" >0.5880</td>\n",
       "                        <td id=\"T_573b8_row6_col5\" class=\"data row6 col5\" >0.5255</td>\n",
       "                        <td id=\"T_573b8_row6_col6\" class=\"data row6 col6\" >0.3311</td>\n",
       "                        <td id=\"T_573b8_row6_col7\" class=\"data row6 col7\" >0.3358</td>\n",
       "                        <td id=\"T_573b8_row6_col8\" class=\"data row6 col8\" >0.1570</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row7\" class=\"row_heading level0 row7\" >ridge</th>\n",
       "                        <td id=\"T_573b8_row7_col0\" class=\"data row7 col0\" >Ridge Classifier</td>\n",
       "                        <td id=\"T_573b8_row7_col1\" class=\"data row7 col1\" >0.7139</td>\n",
       "                        <td id=\"T_573b8_row7_col2\" class=\"data row7 col2\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row7_col3\" class=\"data row7 col3\" >0.3495</td>\n",
       "                        <td id=\"T_573b8_row7_col4\" class=\"data row7 col4\" >0.6371</td>\n",
       "                        <td id=\"T_573b8_row7_col5\" class=\"data row7 col5\" >0.4453</td>\n",
       "                        <td id=\"T_573b8_row7_col6\" class=\"data row7 col6\" >0.2758</td>\n",
       "                        <td id=\"T_573b8_row7_col7\" class=\"data row7 col7\" >0.3007</td>\n",
       "                        <td id=\"T_573b8_row7_col8\" class=\"data row7 col8\" >0.0080</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row8\" class=\"row_heading level0 row8\" >lr</th>\n",
       "                        <td id=\"T_573b8_row8_col0\" class=\"data row8 col0\" >Logistic Regression</td>\n",
       "                        <td id=\"T_573b8_row8_col1\" class=\"data row8 col1\" >0.6911</td>\n",
       "                        <td id=\"T_573b8_row8_col2\" class=\"data row8 col2\" >0.7084</td>\n",
       "                        <td id=\"T_573b8_row8_col3\" class=\"data row8 col3\" >0.3092</td>\n",
       "                        <td id=\"T_573b8_row8_col4\" class=\"data row8 col4\" >0.5834</td>\n",
       "                        <td id=\"T_573b8_row8_col5\" class=\"data row8 col5\" >0.3997</td>\n",
       "                        <td id=\"T_573b8_row8_col6\" class=\"data row8 col6\" >0.2173</td>\n",
       "                        <td id=\"T_573b8_row8_col7\" class=\"data row8 col7\" >0.2396</td>\n",
       "                        <td id=\"T_573b8_row8_col8\" class=\"data row8 col8\" >0.3100</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row9\" class=\"row_heading level0 row9\" >dummy</th>\n",
       "                        <td id=\"T_573b8_row9_col0\" class=\"data row9 col0\" >Dummy Classifier</td>\n",
       "                        <td id=\"T_573b8_row9_col1\" class=\"data row9 col1\" >0.6665</td>\n",
       "                        <td id=\"T_573b8_row9_col2\" class=\"data row9 col2\" >0.5000</td>\n",
       "                        <td id=\"T_573b8_row9_col3\" class=\"data row9 col3\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row9_col4\" class=\"data row9 col4\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row9_col5\" class=\"data row9 col5\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row9_col6\" class=\"data row9 col6\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row9_col7\" class=\"data row9 col7\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row9_col8\" class=\"data row9 col8\" >0.0050</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row10\" class=\"row_heading level0 row10\" >dt</th>\n",
       "                        <td id=\"T_573b8_row10_col0\" class=\"data row10 col0\" >Decision Tree Classifier</td>\n",
       "                        <td id=\"T_573b8_row10_col1\" class=\"data row10 col1\" >0.6336</td>\n",
       "                        <td id=\"T_573b8_row10_col2\" class=\"data row10 col2\" >0.5920</td>\n",
       "                        <td id=\"T_573b8_row10_col3\" class=\"data row10 col3\" >0.4673</td>\n",
       "                        <td id=\"T_573b8_row10_col4\" class=\"data row10 col4\" >0.4527</td>\n",
       "                        <td id=\"T_573b8_row10_col5\" class=\"data row10 col5\" >0.4582</td>\n",
       "                        <td id=\"T_573b8_row10_col6\" class=\"data row10 col6\" >0.1823</td>\n",
       "                        <td id=\"T_573b8_row10_col7\" class=\"data row10 col7\" >0.1831</td>\n",
       "                        <td id=\"T_573b8_row10_col8\" class=\"data row10 col8\" >0.0280</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row11\" class=\"row_heading level0 row11\" >svm</th>\n",
       "                        <td id=\"T_573b8_row11_col0\" class=\"data row11 col0\" >SVM - Linear Kernel</td>\n",
       "                        <td id=\"T_573b8_row11_col1\" class=\"data row11 col1\" >0.6111</td>\n",
       "                        <td id=\"T_573b8_row11_col2\" class=\"data row11 col2\" >0.0000</td>\n",
       "                        <td id=\"T_573b8_row11_col3\" class=\"data row11 col3\" >0.4055</td>\n",
       "                        <td id=\"T_573b8_row11_col4\" class=\"data row11 col4\" >0.3458</td>\n",
       "                        <td id=\"T_573b8_row11_col5\" class=\"data row11 col5\" >0.2926</td>\n",
       "                        <td id=\"T_573b8_row11_col6\" class=\"data row11 col6\" >0.1122</td>\n",
       "                        <td id=\"T_573b8_row11_col7\" class=\"data row11 col7\" >0.1341</td>\n",
       "                        <td id=\"T_573b8_row11_col8\" class=\"data row11 col8\" >0.0340</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row12\" class=\"row_heading level0 row12\" >nb</th>\n",
       "                        <td id=\"T_573b8_row12_col0\" class=\"data row12 col0\" >Naive Bayes</td>\n",
       "                        <td id=\"T_573b8_row12_col1\" class=\"data row12 col1\" >0.5907</td>\n",
       "                        <td id=\"T_573b8_row12_col2\" class=\"data row12 col2\" >0.6421</td>\n",
       "                        <td id=\"T_573b8_row12_col3\" class=\"data row12 col3\" >0.6231</td>\n",
       "                        <td id=\"T_573b8_row12_col4\" class=\"data row12 col4\" >0.4263</td>\n",
       "                        <td id=\"T_573b8_row12_col5\" class=\"data row12 col5\" >0.5042</td>\n",
       "                        <td id=\"T_573b8_row12_col6\" class=\"data row12 col6\" >0.1781</td>\n",
       "                        <td id=\"T_573b8_row12_col7\" class=\"data row12 col7\" >0.1878</td>\n",
       "                        <td id=\"T_573b8_row12_col8\" class=\"data row12 col8\" >0.0080</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row13\" class=\"row_heading level0 row13\" >knn</th>\n",
       "                        <td id=\"T_573b8_row13_col0\" class=\"data row13 col0\" >K Neighbors Classifier</td>\n",
       "                        <td id=\"T_573b8_row13_col1\" class=\"data row13 col1\" >0.5876</td>\n",
       "                        <td id=\"T_573b8_row13_col2\" class=\"data row13 col2\" >0.5229</td>\n",
       "                        <td id=\"T_573b8_row13_col3\" class=\"data row13 col3\" >0.3250</td>\n",
       "                        <td id=\"T_573b8_row13_col4\" class=\"data row13 col4\" >0.3816</td>\n",
       "                        <td id=\"T_573b8_row13_col5\" class=\"data row13 col5\" >0.3473</td>\n",
       "                        <td id=\"T_573b8_row13_col6\" class=\"data row13 col6\" >0.0499</td>\n",
       "                        <td id=\"T_573b8_row13_col7\" class=\"data row13 col7\" >0.0511</td>\n",
       "                        <td id=\"T_573b8_row13_col8\" class=\"data row13 col8\" >0.0660</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_573b8_level0_row14\" class=\"row_heading level0 row14\" >qda</th>\n",
       "                        <td id=\"T_573b8_row14_col0\" class=\"data row14 col0\" >Quadratic Discriminant Analysis</td>\n",
       "                        <td id=\"T_573b8_row14_col1\" class=\"data row14 col1\" >0.5405</td>\n",
       "                        <td id=\"T_573b8_row14_col2\" class=\"data row14 col2\" >0.5999</td>\n",
       "                        <td id=\"T_573b8_row14_col3\" class=\"data row14 col3\" >0.6361</td>\n",
       "                        <td id=\"T_573b8_row14_col4\" class=\"data row14 col4\" >0.3871</td>\n",
       "                        <td id=\"T_573b8_row14_col5\" class=\"data row14 col5\" >0.4704</td>\n",
       "                        <td id=\"T_573b8_row14_col6\" class=\"data row14 col6\" >0.1092</td>\n",
       "                        <td id=\"T_573b8_row14_col7\" class=\"data row14 col7\" >0.1322</td>\n",
       "                        <td id=\"T_573b8_row14_col8\" class=\"data row14 col8\" >0.0180</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1c099344b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=2772, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare_models( errors=\"raise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "retained-flexibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>racer_1_rank</th>\n",
       "      <th>racer_1_age</th>\n",
       "      <th>racer_1_doub</th>\n",
       "      <th>racer_1_ave_st</th>\n",
       "      <th>racer_2_rank</th>\n",
       "      <th>racer_2_age</th>\n",
       "      <th>racer_2_doub</th>\n",
       "      <th>racer_2_ave_st</th>\n",
       "      <th>racer_3_rank</th>\n",
       "      <th>racer_3_age</th>\n",
       "      <th>...</th>\n",
       "      <th>racer_2_male_0</th>\n",
       "      <th>racer_2_male_1</th>\n",
       "      <th>racer_3_male_0</th>\n",
       "      <th>racer_3_male_1</th>\n",
       "      <th>racer_4_male_0</th>\n",
       "      <th>racer_4_male_1</th>\n",
       "      <th>racer_5_male_0</th>\n",
       "      <th>racer_5_male_1</th>\n",
       "      <th>racer_6_male_0</th>\n",
       "      <th>racer_6_male_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15171</th>\n",
       "      <td>4</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.453</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2</td>\n",
       "      <td>51.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15172</th>\n",
       "      <td>3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.328</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.21</td>\n",
       "      <td>4</td>\n",
       "      <td>23.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15173</th>\n",
       "      <td>4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.525</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.22</td>\n",
       "      <td>4</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15174</th>\n",
       "      <td>3</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.19</td>\n",
       "      <td>4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15175</th>\n",
       "      <td>2</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.20</td>\n",
       "      <td>3</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.15</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17285</th>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17286</th>\n",
       "      <td>3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17287</th>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.398</td>\n",
       "      <td>0.19</td>\n",
       "      <td>3</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.18</td>\n",
       "      <td>3</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17288</th>\n",
       "      <td>4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.16</td>\n",
       "      <td>3</td>\n",
       "      <td>52.0</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.17</td>\n",
       "      <td>3</td>\n",
       "      <td>45.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17289</th>\n",
       "      <td>4</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.431</td>\n",
       "      <td>0.17</td>\n",
       "      <td>4</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2119 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       racer_1_rank  racer_1_age  racer_1_doub  racer_1_ave_st  racer_2_rank  \\\n",
       "15171             4         31.0         0.453            0.16             1   \n",
       "15172             3         52.0         0.328            0.18             1   \n",
       "15173             4         38.0         0.525            0.16             2   \n",
       "15174             3         31.0         0.344            0.19             4   \n",
       "15175             2         32.0         0.245            0.20             3   \n",
       "...             ...          ...           ...             ...           ...   \n",
       "17285             2         45.0         0.234            0.16             2   \n",
       "17286             3         34.0         0.404            0.18             2   \n",
       "17287             3         32.0         0.398            0.19             3   \n",
       "17288             4         32.0         0.483            0.16             3   \n",
       "17289             4         29.0         0.449            0.14             3   \n",
       "\n",
       "       racer_2_age  racer_2_doub  racer_2_ave_st  racer_3_rank  racer_3_age  \\\n",
       "15171         27.0         0.170            0.16             2         51.0   \n",
       "15172         34.0         0.121            0.21             4         23.0   \n",
       "15173         57.0         0.078            0.22             4         34.0   \n",
       "15174         38.0         0.450            0.18             2         34.0   \n",
       "15175         47.0         0.388            0.15             3         32.0   \n",
       "...            ...           ...             ...           ...          ...   \n",
       "17285         39.0         0.250            0.18             1         45.0   \n",
       "17286         31.0         0.279            0.16             2         54.0   \n",
       "17287         38.0         0.349            0.18             3         44.0   \n",
       "17288         52.0         0.426            0.17             3         45.0   \n",
       "17289         48.0         0.431            0.17             4         50.0   \n",
       "\n",
       "       ...  racer_2_male_0  racer_2_male_1  racer_3_male_0  racer_3_male_1  \\\n",
       "15171  ...               1               0               1               0   \n",
       "15172  ...               1               0               1               0   \n",
       "15173  ...               1               0               1               0   \n",
       "15174  ...               1               0               1               0   \n",
       "15175  ...               1               0               1               0   \n",
       "...    ...             ...             ...             ...             ...   \n",
       "17285  ...               0               1               0               1   \n",
       "17286  ...               0               1               0               1   \n",
       "17287  ...               0               1               0               1   \n",
       "17288  ...               0               1               0               1   \n",
       "17289  ...               0               1               0               1   \n",
       "\n",
       "       racer_4_male_0  racer_4_male_1  racer_5_male_0  racer_5_male_1  \\\n",
       "15171               1               0               1               0   \n",
       "15172               1               0               1               0   \n",
       "15173               1               0               1               0   \n",
       "15174               1               0               1               0   \n",
       "15175               1               0               1               0   \n",
       "...               ...             ...             ...             ...   \n",
       "17285               0               1               0               1   \n",
       "17286               0               1               0               1   \n",
       "17287               0               1               0               1   \n",
       "17288               0               1               0               1   \n",
       "17289               0               1               0               1   \n",
       "\n",
       "       racer_6_male_0  racer_6_male_1  \n",
       "15171               1               0  \n",
       "15172               1               0  \n",
       "15173               1               0  \n",
       "15174               1               0  \n",
       "15175               1               0  \n",
       "...               ...             ...  \n",
       "17285               0               1  \n",
       "17286               0               1  \n",
       "17287               0               1  \n",
       "17288               0               1  \n",
       "17289               0               1  \n",
       "\n",
       "[2119 rows x 48 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_x_year1_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "hollow-recruitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path =  \"../../../bot_database/{place_name}/model_score_{place_name}/v5_score/{place_name}_model_score_{V}_test.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "model_score_df.to_csv(dir_path, encoding='utf_8_sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "biblical-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_year1_test_df.to_csv(\"ex_year1.csv\")\n",
    "pred_year2_test_df.to_csv(\"ex_year2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "sound-dance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight={},\n",
       "                       criterion='entropy', max_depth=11, max_features='log2',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.002, min_impurity_split=None,\n",
       "                       min_samples_leaf=6, min_samples_split=10,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=250,\n",
       "                       n_jobs=10, oob_score=False, random_state=3514, verbose=0,\n",
       "                       warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-north",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
