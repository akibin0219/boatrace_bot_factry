{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# version2_1のバグ探索用\n",
    "方針:testを2017,2018として検証する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import time\n",
    "import datetime\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "# #自作のモジュールのインポート\n",
    "# import module.master as master\n",
    "# import module.graph as graph\n",
    "# import module.trans_text_code as trans\n",
    "# import module.data_making as making\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習、精度検証用データ作成関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_making_clustar(df):#クラスタリングあり、モータ番号、艇番号なし\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22})#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02})#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02})\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col])#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values#\n",
    "    money=result_df_dummie['money'].values#\n",
    "    years=result_df_dummie['year'].values#\n",
    "\n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['year']==2018) | ((result_df_dummie['year']==2019) )].copy()#2019,2020のデータを検証用データに。\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['year']!=2018) & ((result_df_dummie['year']!=2019) )].copy()#そのほかを学習データに\n",
    "\n",
    "    #年の情報だけ切り分けに使ったからここで消す。\n",
    "    clustar_test_df=clustar_test_df.drop('year',axis=1)\n",
    "    clustar_train_df=clustar_train_df.drop('year',axis=1)\n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df])\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "\n",
    "    model_df=clustar_df\n",
    "    return model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パラメータ探索関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルのパラメータ探索関数(XGboost)\n",
    "def model_score_XGboost_th(version,place_name,result_df):#XGboostの出力を確率のやつを使用したバージョン、閾値の探索も行う。\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2018) | ((result_df['year']==2019) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2018) & ((result_df['year']!=2019) )]#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=test_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    gain_mean=gain_mean.sort_index()\n",
    "\n",
    "    gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    gain_median=gain_median.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "                                'result_com_num':result_com_s.values,\n",
    "                                'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "                                'gain_mean':gain_mean.values,\n",
    "                                'gain_median':gain_median.values,})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "\n",
    "        gain_th=10#利益率の閾値\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        #depths_arr=[5,6,8]\n",
    "        depths_arr=[5,8]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=100+(sum_target_per*2)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1)\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "                target_x_test=result_test_df.drop('money',axis=1)\n",
    "                target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test=result_test_df['result_com']\n",
    "                train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2, shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "                #XGboostのデータ型に変換する\n",
    "                train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "                valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "                test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "\n",
    "                #xgb.config_context(verbosity=0)\n",
    "                param = {'max_depth': depth, #パラメータの設定\n",
    "                                 #'eta': 1.8,\n",
    "                                 #'eta': 0.8,\n",
    "                                 'eta': 1.3,\n",
    "                                 #'eta': 0.2,\n",
    "                                 #'objective': 'binary:hinge',\n",
    "                                 'objective': 'binary:logistic',#確率で出力\n",
    "                                 'eval_metric': 'logloss',\n",
    "                                 'verbosity':0,\n",
    "                                 'subsample':0.8,\n",
    "                                 'nthread':10,\n",
    "                                 'gpu_id':0,\n",
    "                                 'seed':7,\n",
    "                                 'tree_method':'gpu_hist'\n",
    "                                }\n",
    "                evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "                #bst = xgb.train(param, train,num_boost_round=1000,early_stopping_rounds=30)\n",
    "                #num_round = 10000\n",
    "                num_round = 400\n",
    "                bst = xgb.train(param, train,num_round,evallist, early_stopping_rounds=30, verbose_eval=0 )\n",
    "                #bst = xgb.train(param, train,num_round,evallist, verbose=100,early_stopping_rounds=30 )\n",
    "                #RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth)\n",
    "                #RF = RF.fit(target_x_train,target_y_train)\n",
    "\n",
    "\n",
    "                # 未知データに対する予測値\n",
    "                #predict_y_test = RF.predict(target_x_test)\n",
    "                predict_y_test=bst.predict(test)\n",
    "\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る\n",
    "                pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                          , 'test':target_y_test})\n",
    "\n",
    "                #th_arr=[0.1,0.3,0.5,0.6,0.7,0.8,0.9]\n",
    "                #th_arr=[0.01,0.03,0.05,0.07,0.9,0.1,0.13]#探索結果待ち、、、、、\n",
    "                th_arr=[0.85,0.9,0.92]\n",
    "                for th in th_arr:\n",
    "                    trans_df=pred_th_trans(pred_test_df,th)\n",
    "                    num_1=len(trans_df[trans_df['test']==1])\n",
    "                    count=0\n",
    "                    #追加　配当金の情報も考慮する。\n",
    "                    gain_index=0\n",
    "                    model_gain_arr=[0]*len(result_test_df)\n",
    "                    for _, s in trans_df.iterrows():\n",
    "                        if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count+=1#的中回数\n",
    "                            model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "                        gain_index+=1\n",
    "                    #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "                    gain_arr[index]=sum(model_gain_arr)\n",
    "                    accuracy_arr[index]=(count/num_1)*100\n",
    "                    try:\n",
    "                        pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "                    except:\n",
    "                        pred_0[index]=0\n",
    "                    #scoreのseriesに情報書き込み==================\n",
    "                    model_score_s['threshold']=th\n",
    "                    model_score_s['総収益']=sum(model_gain_arr)\n",
    "                    #model_score_s['投資金額']=100*sum(predict_y_test)\n",
    "                    model_score_s['投資金額']=100*trans_df['pred'].sum()\n",
    "                    model_score_s['出現数']=sum(target_y_test)\n",
    "                    #model_score_s['購買予測数']=sum(predict_y_test)\n",
    "                    model_score_s['購買予測数']=trans_df['pred'].sum()\n",
    "                    model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                    model_score_s['購買的中率']=(count/trans_df['pred'].sum())*100\n",
    "                    model_score_s['的中数']=count\n",
    "                    model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "    #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "    dir_path = \"/check_csv/{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "検証用データ格納用ディレクトリ作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     27
    ]
   },
   "outputs": [],
   "source": [
    "def get_place_master():\n",
    "    place_dict={\"01\":\"kiryu\"\n",
    "                ,\"02\":\"toda\"\n",
    "                ,\"03\":\"edogawa\"\n",
    "                ,\"04\":\"heiwazima\"\n",
    "                ,\"05\":\"tamagawa\"\n",
    "                ,\"06\":\"hamanako\"\n",
    "                ,\"07\":\"gamagori\"\n",
    "                ,\"08\":\"tokoname\"\n",
    "                ,\"09\":\"tu\"\n",
    "                ,\"10\":\"mikuni\"\n",
    "                ,\"11\":\"biwako\"\n",
    "                ,\"12\":\"suminoe\"\n",
    "                ,\"13\":\"amagasaki\"\n",
    "                ,\"14\":\"naruto\"\n",
    "                ,\"15\":\"marugame\"\n",
    "                ,\"16\":\"kozima\"\n",
    "                ,\"17\":\"miyazima\"\n",
    "                ,\"18\":\"tokuyama\"\n",
    "                ,\"19\":\"simonoseki\"\n",
    "                ,\"20\":\"wakamatu\"\n",
    "                ,\"21\":\"asiya\"\n",
    "                ,\"22\":\"fukuoka\"\n",
    "                ,\"23\":\"karatu\"\n",
    "                ,\"24\":\"omura\"}\n",
    "    return place_dict\n",
    "\n",
    "def get_place_num_master():\n",
    "    place_num_dict={1:\"kiryu\"\n",
    "                ,2:\"toda\"\n",
    "                ,3:\"edogawa\"\n",
    "                ,4:\"heiwazima\"\n",
    "                ,5:\"tamagawa\"\n",
    "                ,6:\"hamanako\"\n",
    "                ,7:\"gamagori\"\n",
    "                ,8:\"tokoname\"\n",
    "                ,9:\"tu\"\n",
    "                ,10:\"mikuni\"\n",
    "                ,11:\"biwako\"\n",
    "                ,12:\"suminoe\"\n",
    "                ,13:\"amagasaki\"\n",
    "                ,14:\"naruto\"\n",
    "                ,15:\"marugame\"\n",
    "                ,16:\"kozima\"\n",
    "                ,17:\"miyazima\"\n",
    "                ,18:\"tokuyama\"\n",
    "                ,19:\"simonoseki\"\n",
    "                ,20:\"wakamatu\"\n",
    "                ,21:\"asiya\"\n",
    "                ,22:\"fukuoka\"\n",
    "                ,23:\"karatu\"\n",
    "                ,24:\"omura\"}\n",
    "    return place_num_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"check_csv/{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "     dir_path = \"check_pickle/{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元データ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_2_1_2(version,place_name,base_df):#閾値で予測を変えるバージョン\n",
    "    result_df=data_making_clustar(base_df)\n",
    "    model_score_XGboost_th(version,place_name,result_df)#閾値を決めて変換するver\n",
    "\n",
    "    \n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "\n",
    "\n",
    "# place_master=master.get_place_master()\n",
    "# for place in place_master.items():\n",
    "#place_name=place[1]\n",
    "place_name='tokuyama'\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "#各会場の学習データの読み込みと加工\n",
    "result_filepath=\"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "result_base_df=pd.read_csv(result_filepath)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "result_base_df=result_base_df[result_base_df['year']!=2020]#2020のデータを完全に切り離す。\n",
    "version_2_1_2(version,place_name,result_base_df):\n",
    "\n",
    "\n",
    "# #データ格納用のディレクトリ作り\n",
    "# version='V2_1_2'#バージョン\n",
    "\n",
    "\n",
    "# place_master=master.get_place_master()\n",
    "# for place in place_master.items():\n",
    "#     place_name=place[1]\n",
    "#     ################################################========================================================================================================================\n",
    "#     ################################################========================================================================================================================\n",
    "#     ################################################========================================================================================================================\n",
    "#     ################################################========================================================================================================================\n",
    "#     #各会場の学習データの読み込みと加工\n",
    "#     result_filepath=\"../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "#     result_base_df=pd.read_csv(result_filepath)\n",
    "#     result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "#     making.version_2_2(version,place_name,result_base_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### そのまま同ノートブックでモデルのpickle保存と予測を行う。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析対象のモデルスコアシート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "place_master=master.get_place_master()\n",
    "for place in tqdm(place_master.items()):\n",
    "    place_name=place[1]\n",
    "    ################################################========================================================================================================================\n",
    "    ################################################========================================================================================================================\n",
    "    ################################################========================================================================================================================\n",
    "    ################################################========================================================================================================================\n",
    "    #各会場の学習したモデルのスコアの読み込み\n",
    "    score_file_path= \"/check_csv/{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    score_df=pd.read_csv(score_file_path)\n",
    "    score_df=score_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "    \n",
    "    \n",
    "    #analysis.model_analysis(score_df,place_name,version)\n",
    "    analysis.model_analysis_2_2(score_df,place_name,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
