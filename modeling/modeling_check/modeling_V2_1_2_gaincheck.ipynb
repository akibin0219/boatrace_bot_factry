{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#収益計算の部分のバグを探す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.decomposition import PCA  #次元削減用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import time\n",
    "import datetime\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler#モデルの評価用に標準化する関数\n",
    "import scipy.stats#モデルの評価用に標準化する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下収益計算部分のチェックを行うコードの雑処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [
     1,
     12,
     26,
     53,
     173
    ]
   },
   "outputs": [],
   "source": [
    "#バージョンとは関係のない、データの加工関数内で使う関数====================================================================================================================================================================================================\n",
    "def make_PCA_df(PCA_arr):#PCAで削減したものは二次元配列で帰ってくるので、それをデータフレームにして返す関数\n",
    "    X=[0]*len(PCA_arr)\n",
    "    Y=[0]*len(PCA_arr)\n",
    "    index=0\n",
    "    for arr in PCA_arr:\n",
    "        X[index]=arr[0]\n",
    "        Y[index]=arr[1]\n",
    "        index+=1\n",
    "    return pd.DataFrame({'X':X,'Y':Y})\n",
    "\n",
    "#dateのカラムを年だけに変換するやつ\n",
    "def trans_date_type(df):\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    df['year']=df['date'].dt.year\n",
    "    df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "#閾値を渡して、その値以上を1、未満を0に置き変える。\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def get_place_master():\n",
    "    place_dict={\"01\":\"kiryu\"\n",
    "                ,\"02\":\"toda\"\n",
    "                ,\"03\":\"edogawa\"\n",
    "                ,\"04\":\"heiwazima\"\n",
    "                ,\"05\":\"tamagawa\"\n",
    "                ,\"06\":\"hamanako\"\n",
    "                ,\"07\":\"gamagori\"\n",
    "                ,\"08\":\"tokoname\"\n",
    "                ,\"09\":\"tu\"\n",
    "                ,\"10\":\"mikuni\"\n",
    "                ,\"11\":\"biwako\"\n",
    "                ,\"12\":\"suminoe\"\n",
    "                ,\"13\":\"amagasaki\"\n",
    "                ,\"14\":\"naruto\"\n",
    "                ,\"15\":\"marugame\"\n",
    "                ,\"16\":\"kozima\"\n",
    "                ,\"17\":\"miyazima\"\n",
    "                ,\"18\":\"tokuyama\"\n",
    "                ,\"19\":\"simonoseki\"\n",
    "                ,\"20\":\"wakamatu\"\n",
    "                ,\"21\":\"asiya\"\n",
    "                ,\"22\":\"fukuoka\"\n",
    "                ,\"23\":\"karatu\"\n",
    "                ,\"24\":\"omura\"}\n",
    "    return place_dict\n",
    "\n",
    "def get_place_num_master():\n",
    "    place_num_dict={1:\"kiryu\"\n",
    "                ,2:\"toda\"\n",
    "                ,3:\"edogawa\"\n",
    "                ,4:\"heiwazima\"\n",
    "                ,5:\"tamagawa\"\n",
    "                ,6:\"hamanako\"\n",
    "                ,7:\"gamagori\"\n",
    "                ,8:\"tokoname\"\n",
    "                ,9:\"tu\"\n",
    "                ,10:\"mikuni\"\n",
    "                ,11:\"biwako\"\n",
    "                ,12:\"suminoe\"\n",
    "                ,13:\"amagasaki\"\n",
    "                ,14:\"naruto\"\n",
    "                ,15:\"marugame\"\n",
    "                ,16:\"kozima\"\n",
    "                ,17:\"miyazima\"\n",
    "                ,18:\"tokuyama\"\n",
    "                ,19:\"simonoseki\"\n",
    "                ,20:\"wakamatu\"\n",
    "                ,21:\"asiya\"\n",
    "                ,22:\"fukuoka\"\n",
    "                ,23:\"karatu\"\n",
    "                ,24:\"omura\"}\n",
    "    return place_num_dict\n",
    "\n",
    "def data_making_clustar(df):#クラスタリングあり、モータ番号、艇番号なし\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22})#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02})#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02})\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col])#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020]#2020のデータを完全に切り離す。\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values#\n",
    "    money=result_df_dummie['money'].values#\n",
    "    years=result_df_dummie['year'].values#\n",
    "\n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['year']==2019) | ((result_df_dummie['year']==2020) )].copy()#2019,2020のデータを検証用データに。\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['year']!=2019) & ((result_df_dummie['year']!=2020) )].copy()#そのほかを学習データに\n",
    "\n",
    "    #年の情報だけ切り分けに使ったからここで消す。\n",
    "    clustar_test_df=clustar_test_df.drop('year',axis=1)\n",
    "    clustar_train_df=clustar_train_df.drop('year',axis=1)\n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df])\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df\n",
    "    return model_df\n",
    "\n",
    "#モデルのパラメータ探索関数(XGboost)\n",
    "def model_score_XGboost_th(version,place_name,result_df):#XGboostの出力を確率のやつを使用したバージョン、閾値の探索も行う。\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=test_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    gain_mean=gain_mean.sort_index()\n",
    "\n",
    "    gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    gain_median=gain_median.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "                                'result_com_num':result_com_s.values,\n",
    "                                'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "                                'gain_mean':gain_mean.values,\n",
    "                                'gain_median':gain_median.values,})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "\n",
    "        gain_th=10#利益率の閾値\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        #depths_arr=[5,6,8]\n",
    "        depths_arr=[5,8]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=100+(sum_target_per*2)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1)\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "                target_x_test=result_test_df.drop('money',axis=1)\n",
    "                target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test=result_test_df['result_com']\n",
    "                train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2, shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "                #XGboostのデータ型に変換する\n",
    "                train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "                valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "                test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "\n",
    "                #xgb.config_context(verbosity=0)\n",
    "                param = {'max_depth': depth, #パラメータの設定\n",
    "                                 #'eta': 1.8,\n",
    "                                 #'eta': 0.8,\n",
    "                                 'eta': 1.3,\n",
    "                                 #'eta': 0.2,\n",
    "                                 #'objective': 'binary:hinge',\n",
    "                                 'objective': 'binary:logistic',#確率で出力\n",
    "                                 'eval_metric': 'logloss',\n",
    "                                 'verbosity':0,\n",
    "                                 'subsample':0.8,\n",
    "                                 'nthread':10,\n",
    "                                 'gpu_id':0,\n",
    "                                 'seed':7,\n",
    "                                 'tree_method':'gpu_hist'\n",
    "                                }\n",
    "                evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "                #bst = xgb.train(param, train,num_boost_round=1000,early_stopping_rounds=30)\n",
    "                #num_round = 10000\n",
    "                num_round = 400\n",
    "                bst = xgb.train(param, train,num_round,evallist, early_stopping_rounds=30, verbose_eval=0 )\n",
    "                #bst = xgb.train(param, train,num_round,evallist, verbose=100,early_stopping_rounds=30 )\n",
    "                #RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth)\n",
    "                #RF = RF.fit(target_x_train,target_y_train)\n",
    "\n",
    "\n",
    "                # 未知データに対する予測値\n",
    "                #predict_y_test = RF.predict(target_x_test)\n",
    "                predict_y_test=bst.predict(test)\n",
    "\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る\n",
    "                pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                          , 'test':target_y_test})\n",
    "\n",
    "                #th_arr=[0.1,0.3,0.5,0.6,0.7,0.8,0.9]\n",
    "                #th_arr=[0.01,0.03,0.05,0.07,0.9,0.1,0.13]#探索結果待ち、、、、、\n",
    "                th_arr=[0.85,0.9,0.92]\n",
    "                for th in th_arr:\n",
    "                    trans_df=pred_th_trans(pred_test_df,th)\n",
    "                    num_1=len(trans_df[trans_df['test']==1])\n",
    "                    count=0\n",
    "                    #追加　配当金の情報も考慮する。\n",
    "                    gain_index=0\n",
    "                    model_gain_arr=[0]*len(result_test_df)\n",
    "                    for _, s in trans_df.iterrows():\n",
    "                        if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count+=1#的中回数\n",
    "                            model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "                        gain_index+=1\n",
    "                    #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "                    gain_arr[index]=sum(model_gain_arr)\n",
    "                    accuracy_arr[index]=(count/num_1)*100\n",
    "                    try:\n",
    "                        pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "                    except:\n",
    "                        pred_0[index]=0\n",
    "                    #scoreのseriesに情報書き込み==================\n",
    "                    model_score_s['threshold']=th\n",
    "                    model_score_s['総収益']=sum(model_gain_arr)\n",
    "                    #model_score_s['投資金額']=100*sum(predict_y_test)\n",
    "                    model_score_s['投資金額']=100*trans_df['pred'].sum()\n",
    "                    model_score_s['出現数']=sum(target_y_test)\n",
    "                    #model_score_s['購買予測数']=sum(predict_y_test)\n",
    "                    model_score_s['購買予測数']=trans_df['pred'].sum()\n",
    "                    model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                    model_score_s['購買的中率']=(count/trans_df['pred'].sum())*100\n",
    "                    model_score_s['的中数']=count\n",
    "                    model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "    #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "    dir_path = \"check_csv/{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "#　簡素化用の関数集\n",
    "#target_comとresultのdfを渡すとcomを0,1に変換する関数\n",
    "def trans_result_com(target_com,trans_base_df):#comをターゲットに合わせて0,1の二値に変換する。\n",
    "    #学習データのラベル変換==========================================================\n",
    "    trans_df=trans_base_df.copy()\n",
    "    #result_train_df=trans_base_df.copy()\n",
    "    result_arr=[0]*len(trans_df)\n",
    "    i=0\n",
    "    for result in trans_df['result_com']:#\n",
    "        if ((result==target_com)):\n",
    "            result_arr[i]=1\n",
    "        else:\n",
    "            result_arr[i]=0\n",
    "        i+=1\n",
    "    trans_df['result_com']=result_arr\n",
    "    return trans_df\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "\n",
    "\n",
    "def pred_th_trans_com(pred_df,th,target_com):#指定の組のカラムのみを置換。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_{}'.format(target_com)] >= th, 'pred_{}'.format(target_com)] = 1\n",
    "    trans_df.loc[~(trans_df['pred_{}'.format(target_com)] >=  th), 'pred_{}'.format(target_com)] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_gain(pred_gain_df):\n",
    "    pred_true_df=pred_gain_df[(pred_gain_df['pred']==1)&(pred_gain_df['trans_result']==1)].copy()\n",
    "    pred_true_df['hit']=1\n",
    "    calc_base_df=pred_gain_df.copy()\n",
    "    calc_base_df['hit']=pred_true_df['hit']\n",
    "    calc_base_df['gain']=pred_true_df['money']\n",
    "    calc_base_df=calc_base_df.fillna(0)\n",
    "    #\n",
    "    #calc_base_df:予測、変換積みの結果、実際の結果、配当金、収益をすべて表したdf,合計操作は行っていない。\n",
    "    #\n",
    "    return calc_base_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"check_csv/{place_name}/pred/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実際のpickleモデルを使い、かつ収益計算部分に手を加えないで予測を行う。また予測もcsv/pred内に1/0で出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_check_V2_1_2(result_base_df,use_model_df,place_name,version):#pickleを使った時の予測内容のチェックをする\n",
    "    print(place_name)\n",
    "\n",
    "    #==============================================================================\n",
    "    #学習関数で場所ごとにバージョンに対応した学習データを作る\n",
    "    result_df=data_making_clustar(result_base_df)\n",
    "    #==============================================================================\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "    pred_concat_df=pd.DataFrame(columns=use_model_df['target_com'].values,index=test_df.index)#予測データをまとめて持つdf\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "    for index, model_row in use_model_df.iterrows():\n",
    "        #パラメータ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "        #======================================================================================\n",
    "        #result_com=int(model_row['target_com'])\n",
    "        result_com=int(model_row['target_com'])\n",
    "        depth=int(model_row['depth'])\n",
    "        target_per=int(model_row['target_per'])\n",
    "        th=float(model_row['threshold'])\n",
    "\n",
    "        #======================================================================================\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_train_df=trans_result_com(result_com,result_train_df)\n",
    "\n",
    "        result_test_df=test_df.copy()\n",
    "        result_test_df=trans_result_com(result_com,result_test_df)\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "\n",
    "\n",
    "        #モデルの評価指標値を格納するseries======================\n",
    "        model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "        model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "        model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "        model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "        #======================\n",
    "        #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "        # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "        target_df=result_train_df#ベースのデータフレームをコピー\n",
    "        target_df=target_df.sample(frac=1,random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "        target_1_df=target_df[target_df['result_com']==1]\n",
    "        len_1=len(target_1_df)\n",
    "        target_0_df=target_df[target_df['result_com']==0]\n",
    "        len_0=len(target_0_df)\n",
    "        target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "        target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "        #学習＆予測ぱーと========================================================================\n",
    "        #==========================================================================================================================================\n",
    "        #データの切り分け\n",
    "        target_x_train=target_train_df.drop('money',axis=1)\n",
    "        target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "        target_x_test=result_test_df.drop('money',axis=1)\n",
    "        target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "        target_y_train=target_train_df['result_com']\n",
    "        target_y_test=result_test_df['result_com']\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2,shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "        #XGboostのデータ型に変換する\n",
    "        train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "        valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "        test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "    #     #xgb.config_context(verbosity=0)\n",
    "    #     param = {'max_depth': depth, #パラメータの設定\n",
    "    #                      #'eta': 1.8,\n",
    "    #                      #'eta': 0.8,\n",
    "    #                      'eta': 1.3,\n",
    "    #                      #'eta': 0.2,\n",
    "    #                      #'objective': 'binary:hinge',\n",
    "    #                      'objective': 'binary:logistic',#確率で出力\n",
    "    #                      'eval_metric': 'logloss',\n",
    "    #                      'verbosity':0,\n",
    "    #                      'subsample':0.8,\n",
    "    #                      'nthread':10,\n",
    "    #                      'gpu_id':0,\n",
    "    #                      'seed':7,\n",
    "    #                      'tree_method':'gpu_hist'\n",
    "    #                     }\n",
    "    #     evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "    #     num_round = 400\n",
    "\n",
    "        pickle_path=\"check_pickle/{place_name}/com{com}_{depth}_{target_per}_{th}_{place_name}.sav\".format(place_name=place_name,com=result_com,depth=depth,target_per=target_per,th=th)#モデルのdirs\n",
    "        bst = pickle.load(open(pickle_path, 'rb'))\n",
    "        # 未知データに対する予測値\n",
    "        #predict_y_test = RF.predict(target_x_test)\n",
    "        predict_y_test=bst.predict(test)\n",
    "\n",
    "        #[1]の正答率を見る\n",
    "        pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                  , 'trans_result':target_y_test})\n",
    "        trans_df=pred_th_trans(pred_test_df,th)\n",
    "        #num_1=len(trans_df[trans_df['test']==1])\n",
    "        count=0\n",
    "\n",
    "        #/////収益計算の項\n",
    "        trans_df['money']=test_money\n",
    "        #trans_df['trans_result']=target_y_test\n",
    "        trans_df['true_result']=test_df['result_com']\n",
    "\n",
    "        #/////\n",
    "\n",
    "\n",
    "        #収益計算部分======================================\n",
    "        #追加　配当金の情報も考慮する。\n",
    "        result_gain_base_df=calc_gain(trans_df)\n",
    "        dir_path = \"check_csv/{place_name}/pred/check_pred_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "        result_gain_base_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "        \n",
    "        pred_concat_df[result_com]=trans_df['trans_result'].values#組の予測を結合\n",
    "\n",
    "        #scoreのseriesに情報書き込み==================\n",
    "        model_score_s['threshold']=th\n",
    "        model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "        model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "        model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "        model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "        model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "        model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "        model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "        model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "\n",
    "    #モデルの「スコアを保存\n",
    "    #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "    dir_path = \"check_csv/{place_name}/check_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "\n",
    "    dir_path = \"check_csv/{place_name}/pred/pred_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "    pred_concat_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[11:29:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[11:29:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "\n",
    "place_name=\"kiryu\"\n",
    "place_master=get_place_master()\n",
    "#for place in tqdm(place_master.items()):\n",
    "    #place_name=place[1]\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "dir_path = \"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#学習のためのベースになるリザルトデータ\n",
    "result_base_df=pd.read_csv(dir_path)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "#各会場の成績の良かったモデルのスコアの読み込み\n",
    "model_dir_path = \"check_csv/{place_name}/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "use_model_df=pd.read_csv(model_dir_path)\n",
    "use_model_df=use_model_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "pickle_check_V2_1_2(result_base_df,use_model_df,place_name,version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>result_com</th>\n",
       "      <th>money</th>\n",
       "      <th>number_race</th>\n",
       "      <th>racer_1_ID</th>\n",
       "      <th>racer_2_ID</th>\n",
       "      <th>racer_3_ID</th>\n",
       "      <th>racer_4_ID</th>\n",
       "      <th>racer_5_ID</th>\n",
       "      <th>racer_6_ID</th>\n",
       "      <th>...</th>\n",
       "      <th>racer_2_bo</th>\n",
       "      <th>racer_2_mo</th>\n",
       "      <th>racer_3_bo</th>\n",
       "      <th>racer_3_mo</th>\n",
       "      <th>racer_4_bo</th>\n",
       "      <th>racer_4_mo</th>\n",
       "      <th>racer_5_bo</th>\n",
       "      <th>racer_5_mo</th>\n",
       "      <th>racer_6_bo</th>\n",
       "      <th>racer_6_mo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19083</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>9</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4173</td>\n",
       "      <td>4091</td>\n",
       "      <td>3382</td>\n",
       "      <td>3448</td>\n",
       "      <td>3550</td>\n",
       "      <td>4327</td>\n",
       "      <td>...</td>\n",
       "      <td>36.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>68.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19084</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>29</td>\n",
       "      <td>8470.0</td>\n",
       "      <td>9</td>\n",
       "      <td>4180</td>\n",
       "      <td>4128</td>\n",
       "      <td>3741</td>\n",
       "      <td>4761</td>\n",
       "      <td>4581</td>\n",
       "      <td>5103</td>\n",
       "      <td>...</td>\n",
       "      <td>23.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19085</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>4</td>\n",
       "      <td>1220.0</td>\n",
       "      <td>10</td>\n",
       "      <td>4305</td>\n",
       "      <td>3463</td>\n",
       "      <td>4404</td>\n",
       "      <td>4802</td>\n",
       "      <td>3920</td>\n",
       "      <td>3505</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19086</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>5</td>\n",
       "      <td>840.0</td>\n",
       "      <td>11</td>\n",
       "      <td>4593</td>\n",
       "      <td>3997</td>\n",
       "      <td>4173</td>\n",
       "      <td>4756</td>\n",
       "      <td>4363</td>\n",
       "      <td>4382</td>\n",
       "      <td>...</td>\n",
       "      <td>24.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19087</th>\n",
       "      <td>2020-03-22</td>\n",
       "      <td>41</td>\n",
       "      <td>6860.0</td>\n",
       "      <td>12</td>\n",
       "      <td>4061</td>\n",
       "      <td>4544</td>\n",
       "      <td>4584</td>\n",
       "      <td>3489</td>\n",
       "      <td>3412</td>\n",
       "      <td>3406</td>\n",
       "      <td>...</td>\n",
       "      <td>74.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  result_com   money  number_race  racer_1_ID  racer_2_ID  \\\n",
       "19083  2020-03-22           9  1340.0            8        4173        4091   \n",
       "19084  2020-03-22          29  8470.0            9        4180        4128   \n",
       "19085  2020-03-22           4  1220.0           10        4305        3463   \n",
       "19086  2020-03-22           5   840.0           11        4593        3997   \n",
       "19087  2020-03-22          41  6860.0           12        4061        4544   \n",
       "\n",
       "       racer_3_ID  racer_4_ID  racer_5_ID  racer_6_ID  ...  racer_2_bo  \\\n",
       "19083        3382        3448        3550        4327  ...        36.0   \n",
       "19084        3741        4761        4581        5103  ...        23.0   \n",
       "19085        4404        4802        3920        3505  ...        45.0   \n",
       "19086        4173        4756        4363        4382  ...        24.0   \n",
       "19087        4584        3489        3412        3406  ...        74.0   \n",
       "\n",
       "       racer_2_mo  racer_3_bo  racer_3_mo  racer_4_bo  racer_4_mo  racer_5_bo  \\\n",
       "19083        45.0        15.0        65.0        68.0        80.0        66.0   \n",
       "19084        29.0        48.0        66.0        38.0        53.0        31.0   \n",
       "19085        58.0        11.0        38.0        64.0        50.0        72.0   \n",
       "19086        67.0        71.0        78.0        65.0        51.0        75.0   \n",
       "19087        27.0        16.0        55.0        20.0        71.0        14.0   \n",
       "\n",
       "       racer_5_mo  racer_6_bo  racer_6_mo  \n",
       "19083        62.0        49.0        76.0  \n",
       "19084        17.0        27.0        24.0  \n",
       "19085        15.0        62.0        46.0  \n",
       "19086        32.0        47.0        42.0  \n",
       "19087        47.0        43.0        39.0  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_base_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickleを使わずにtrainして予測を出力、正常にpickleが作動しているか同課の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_check_V2_1_2(result_base_df,use_model_df,place_name,version):\n",
    "    #探査結果から学習したモデルを保存する関数、\n",
    "    print(place_name)\n",
    "    #==============================================================================\n",
    "    #学習関数で場所ごとにバージョンに対応した学習データを作る\n",
    "    result_df=data_making_clustar(result_base_df)\n",
    "    #==============================================================================\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "    pred_concat_df=pd.DataFrame(columns=use_model_df['target_com'].values,index=test_df.index)#予測データをまとめて持つdf\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "\n",
    "    # #x,yへの切り分け\n",
    "    # #出現数の分布\n",
    "    # result_com_s=test_df['result_com'].value_counts()\n",
    "    # result_com_s=result_com_s.sort_index()\n",
    "    # gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    # gain_mean=gain_mean.sort_index()\n",
    "    #\n",
    "    # gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    # gain_median=gain_median.sort_index()\n",
    "    # result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "    #                             'result_com_num':result_com_s.values,\n",
    "    #                             'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "    #                             'gain_mean':gain_mean.values,\n",
    "    #                             'gain_median':gain_median.values,})\n",
    "    # result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "    for index, model_row in use_model_df.iterrows():\n",
    "        #パラメータ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "        #======================================================================================\n",
    "        #result_com=int(model_row['target_com'])\n",
    "        result_com=int(model_row['target_com'])\n",
    "        depth=int(model_row['depth'])\n",
    "        target_per=int(model_row['target_per'])\n",
    "        th=float(model_row['threshold'])\n",
    "\n",
    "        #======================================================================================\n",
    "        #======================================================================================\n",
    "        #======================================================================================\n",
    "\n",
    "\n",
    "        # gain_th=10#利益率の閾値\n",
    "        # result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        # buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        # num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "\n",
    "\n",
    "        #モデルの評価指標値を格納するseries======================\n",
    "        model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "        model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "        model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "        model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "        #======================\n",
    "        #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "        # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "        target_df=result_train_df#ベースのデータフレームをコピー\n",
    "        target_df=target_df.sample(frac=1,random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "        target_1_df=target_df[target_df['result_com']==1]\n",
    "        len_1=len(target_1_df)\n",
    "        target_0_df=target_df[target_df['result_com']==0]\n",
    "        len_0=len(target_0_df)\n",
    "        target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "        target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "        #学習＆予測ぱーと========================================================================\n",
    "        #==========================================================================================================================================\n",
    "        #データの切り分け\n",
    "        target_x_train=target_train_df.drop('money',axis=1)\n",
    "        target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "        target_x_test=result_test_df.drop('money',axis=1)\n",
    "        target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "        target_y_train=target_train_df['result_com']\n",
    "        target_y_test=result_test_df['result_com']\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2,shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "        #XGboostのデータ型に変換する\n",
    "        train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "        valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "        test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "\n",
    "        #xgb.config_context(verbosity=0)\n",
    "        param = {'max_depth': depth, #パラメータの設定\n",
    "                         #'eta': 1.8,\n",
    "                         #'eta': 0.8,\n",
    "                         'eta': 1.3,\n",
    "                         #'eta': 0.2,\n",
    "                         #'objective': 'binary:hinge',\n",
    "                         'objective': 'binary:logistic',#確率で出力\n",
    "                         'eval_metric': 'logloss',\n",
    "                         'verbosity':0,\n",
    "                         'subsample':0.8,\n",
    "                         'nthread':10,\n",
    "                         'gpu_id':0,\n",
    "                         'seed':7,\n",
    "                         'tree_method':'gpu_hist'\n",
    "                        }\n",
    "        evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "        num_round = 400\n",
    "        bst = xgb.train(param, train,num_round,evallist, early_stopping_rounds=30, verbose_eval=0 )\n",
    "\n",
    "        # 未知データに対する予測値\n",
    "        #predict_y_test = RF.predict(target_x_test)\n",
    "        predict_y_test=bst.predict(test)\n",
    "        #[1]の正答率を見る\n",
    "        pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                  , 'trans_result':target_y_test})\n",
    "        trans_df=pred_th_trans(pred_test_df,th)\n",
    "        #num_1=len(trans_df[trans_df['test']==1])\n",
    "        count=0\n",
    "\n",
    "        #/////収益計算の項\n",
    "        trans_df['money']=test_money\n",
    "        #trans_df['trans_result']=target_y_test\n",
    "        trans_df['true_result']=test_df['result_com']\n",
    "\n",
    "        #/////\n",
    "\n",
    "\n",
    "        #収益計算部分======================================\n",
    "        #追加　配当金の情報も考慮する。\n",
    "        result_gain_base_df=calc_gain(trans_df)\n",
    "        \n",
    "        dir_path = \"check_csv/pred/{place_name}/check_pred_train_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "        result_gain_base_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "        \n",
    "        pred_concat_df[result_com]=trans_df['trans_result'].values#組の予測を結合\n",
    "        \n",
    "        #scoreのseriesに情報書き込み==================\n",
    "        model_score_s['threshold']=th\n",
    "        model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "        model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "        model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "        model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "        model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "        model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "        model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "        model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "\n",
    "    \n",
    "    dir_path = \"check_csv/{place_name}/check_train_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    \n",
    "    dir_path = \"check_csv/{place_name}/pred/pred_train_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "    pred_concat_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "\n",
    "place_name=\"kiryu\"\n",
    "place_master=get_place_master()\n",
    "#for place in tqdm(place_master.items()):\n",
    "    #place_name=place[1]\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "dir_path = \"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#学習のためのベースになるリザルトデータ\n",
    "result_base_df=pd.read_csv(dir_path)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "#各会場の成績の良かったモデルのスコアの読み込み\n",
    "model_dir_path = \"check_csv/{place_name}/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "use_model_df=pd.read_csv(model_dir_path)\n",
    "use_model_df=use_model_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "train_check_V2_1_2(result_base_df,use_model_df,place_name,version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測比較用の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_arr(pred1_df,pred2_df):\n",
    "    pred_1_vals=[pred1_df[col] for col in pred1_df.columns]\n",
    "    pred_2_vals=[pred2_df[col] for col in pred2_df.columns]\n",
    "    for col_name1,col1,col_name2,col2 in zip(pred1_df.columns,pred_1_vals,pred2_df.columns,pred_2_vals):\n",
    "        if list(col1.values)==list(col1.values):\n",
    "            print(col_name1,'  and  ',col_name2,'  is same pred \\n')\n",
    "    return None\n",
    "model_dir_path = \"check_csv/{place_name}/pred/pred_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "pred_1=pd.read_csv(model_dir_path)\n",
    "pred_1=pred_1.drop([\"Unnamed: 0\"],axis=1)\n",
    "model_dir_path = \"check_csv/{place_name}/pred/pred_train_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "pred_2=pd.read_csv(model_dir_path)\n",
    "pred_2=pred_2.drop([\"Unnamed: 0\"],axis=1)\n",
    "check_pred_arr(pred_1,pred_2)\n",
    "\n",
    "#閾値を渡して、その値以上を1、未満を0に置き変える。\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まさかの予測が全く一緒だったが、収益計算がいかれていた。収益が合うまでチェックを行う。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#　簡素化用の関数集\n",
    "#target_comとresultのdfを渡すとcomを0,1に変換する関数\n",
    "def trans_result_com(target_com,trans_base_df):#comをターゲットに合わせて0,1の二値に変換する。\n",
    "    #学習データのラベル変換==========================================================\n",
    "    trans_df=trans_base_df.copy()\n",
    "    #result_train_df=trans_base_df.copy()\n",
    "    result_arr=[0]*len(trans_df)\n",
    "    i=0\n",
    "    for result in trans_df['result_com']:#\n",
    "        if ((result==target_com)):\n",
    "            result_arr[i]=1\n",
    "        else:\n",
    "            result_arr[i]=0\n",
    "        i+=1\n",
    "    trans_df['result_com']=result_arr\n",
    "    return trans_df\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "\n",
    "\n",
    "def pred_th_trans_com(pred_df,th,target_com):#指定の組のカラムのみを置換。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_{}'.format(target_com)] >= th, 'pred_{}'.format(target_com)] = 1\n",
    "    trans_df.loc[~(trans_df['pred_{}'.format(target_com)] >=  th), 'pred_{}'.format(target_com)] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_gain(pred_gain_df):\n",
    "    pred_true_df=pred_gain_df[(pred_gain_df['pred']==1)&(pred_gain_df['trans_result']==1)].copy()\n",
    "    pred_true_df['hit']=1\n",
    "    calc_base_df=pred_gain_df.copy()\n",
    "    calc_base_df['hit']=pred_true_df['hit']\n",
    "    calc_base_df['gain']=pred_true_df['money']\n",
    "    calc_base_df=calc_base_df.fillna(0)\n",
    "    #\n",
    "    #calc_base_df:予測、変換積みの結果、実際の結果、配当金、収益をすべて表したdf,合計操作は行っていない。\n",
    "    #\n",
    "    return calc_base_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "\n",
    "place_name=\"kiryu\"\n",
    "place_master=get_place_master()\n",
    "#for place in tqdm(place_master.items()):\n",
    "    #place_name=place[1]\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "dir_path = \"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#学習のためのベースになるリザルトデータ\n",
    "result_base_df=pd.read_csv(dir_path)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "#各会場の成績の良かったモデルのスコアの読み込み\n",
    "model_dir_path = \"check_csv/{place_name}/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "use_model_df=pd.read_csv(model_dir_path)\n",
    "use_model_df=use_model_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "#def pickle_check_V2_1_2(result_base_df,use_model_df,place_name,version):#pickleを使った時の予測内容のチェックをする\n",
    "#探査結果から学習したモデルを保存する関数、\n",
    "print(place_name)\n",
    "\n",
    "#==============================================================================\n",
    "#学習関数で場所ごとにバージョンに対応した学習データを作る\n",
    "result_df=data_making_clustar(result_base_df)\n",
    "#==============================================================================\n",
    "#result_dfは加工関数にて分けられたものを渡す。\n",
    "model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "#学習データの切り分け\n",
    "test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "pred_concat_df=pd.DataFrame(columns=use_model_df['target_com'].values,index=test_df.index)#予測データをまとめて持つdf\n",
    "#学習データを切り分けたらyearはいらないから削除する\n",
    "test_df=test_df.drop(['year'],axis=1)\n",
    "train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "train_money=pd.Series(train_df['money'])\n",
    "test_money=pd.Series(test_df['money'])\n",
    "for index, model_row in use_model_df.iterrows():\n",
    "    #パラメータ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "    #======================================================================================\n",
    "    #result_com=int(model_row['target_com'])\n",
    "    result_com=int(model_row['target_com'])\n",
    "    depth=int(model_row['depth'])\n",
    "    target_per=int(model_row['target_per'])\n",
    "    th=float(model_row['threshold'])\n",
    "\n",
    "    #======================================================================================\n",
    "    #===============================================================================\n",
    "    #学習データのラベル変換==========================================================\n",
    "    result_train_df=train_df.copy()\n",
    "    result_train_df=trans_result_com(result_com,result_train_df)\n",
    "    \n",
    "    result_test_df=test_df.copy()\n",
    "    result_test_df=trans_result_com(result_com,result_test_df)\n",
    "    \n",
    "    result_train_df['money']=train_money\n",
    "    result_test_df['money']=test_money\n",
    "    #学習データラベル変換終わり============================================\n",
    "\n",
    "    for_arr=np.arange(1,85)\n",
    "    #for_arr=np.arange(1,100,1)\n",
    "    accuracy_arr=[0]*len(for_arr)\n",
    "    target_per_arr=[0]*len(for_arr)\n",
    "    pred_0=[0]*len(for_arr)\n",
    "    gain_arr=[0]*len(for_arr)\n",
    "    model_gain_arr=[0]*len(result_test_df)\n",
    "    test_gain_arr=test_money.values\n",
    "\n",
    "\n",
    "    #モデルの評価指標値を格納するseries======================\n",
    "    model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "    model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "    model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "    model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "    #======================\n",
    "    #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "    # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "    target_df=result_train_df#ベースのデータフレームをコピー\n",
    "    target_df=target_df.sample(frac=1,random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "    target_1_df=target_df[target_df['result_com']==1]\n",
    "    len_1=len(target_1_df)\n",
    "    target_0_df=target_df[target_df['result_com']==0]\n",
    "    len_0=len(target_0_df)\n",
    "    target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "    target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "    #学習＆予測ぱーと========================================================================\n",
    "    #==========================================================================================================================================\n",
    "    #データの切り分け\n",
    "    target_x_train=target_train_df.drop('money',axis=1)\n",
    "    target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "    target_x_test=result_test_df.drop('money',axis=1)\n",
    "    target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "    target_y_train=target_train_df['result_com']\n",
    "    target_y_test=result_test_df['result_com']\n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2,shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "    #XGboostのデータ型に変換する\n",
    "    train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "    valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "    test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "#     #xgb.config_context(verbosity=0)\n",
    "#     param = {'max_depth': depth, #パラメータの設定\n",
    "#                      #'eta': 1.8,\n",
    "#                      #'eta': 0.8,\n",
    "#                      'eta': 1.3,\n",
    "#                      #'eta': 0.2,\n",
    "#                      #'objective': 'binary:hinge',\n",
    "#                      'objective': 'binary:logistic',#確率で出力\n",
    "#                      'eval_metric': 'logloss',\n",
    "#                      'verbosity':0,\n",
    "#                      'subsample':0.8,\n",
    "#                      'nthread':10,\n",
    "#                      'gpu_id':0,\n",
    "#                      'seed':7,\n",
    "#                      'tree_method':'gpu_hist'\n",
    "#                     }\n",
    "#     evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "#     num_round = 400\n",
    "\n",
    "    pickle_path=\"check_pickle/{place_name}/com{com}_{depth}_{target_per}_{th}_{place_name}.sav\".format(place_name=place_name,com=result_com,depth=depth,target_per=target_per,th=th)#モデルのdirs\n",
    "    bst = pickle.load(open(pickle_path, 'rb'))\n",
    "    # 未知データに対する予測値\n",
    "    #predict_y_test = RF.predict(target_x_test)\n",
    "    predict_y_test=bst.predict(test)\n",
    "\n",
    "    #[1]の正答率を見る\n",
    "    pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                              , 'trans_result':target_y_test})\n",
    "    trans_df=pred_th_trans(pred_test_df,th)\n",
    "    #num_1=len(trans_df[trans_df['test']==1])\n",
    "    Wcount=0\n",
    "    \n",
    "    #/////新規関数にはここの情報が必要なので追加する。\n",
    "    trans_df['money']=test_money\n",
    "    #trans_df['trans_result']=target_y_test\n",
    "    trans_df['true_result']=test_df['result_com']\n",
    "        \n",
    "    dir_path = \"ex.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "    trans_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    #/////\n",
    "    \n",
    "    \n",
    "    #収益計算部分======================================\n",
    "    #追加　配当金の情報も考慮する。\n",
    "    result_gain_base_df=calc_gain(trans_df)\n",
    "\n",
    "    pred_concat_df[result_com]=trans_df['trans_result'].values#組の予測を結合\n",
    "\n",
    "    #scoreのseriesに情報書き込み==================\n",
    "    model_score_s['threshold']=th\n",
    "    model_score_s['総収益']=result_gain_base_df[\"gain\"].sum()\n",
    "    model_score_s['投資金額']=100*result_gain_base_df[\"pred\"].sum()\n",
    "    model_score_s['出現数']=result_gain_base_df['trans_result'].sum()\n",
    "    model_score_s['購買予測数']=result_gain_base_df['pred'].sum()\n",
    "    model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "    model_score_s['的中数']=result_gain_base_df['hit'].sum()\n",
    "    model_score_s['購買的中率']=(model_score_s['的中数']/ model_score_s['購買予測数'])*100\n",
    "    model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    dir_path = \"ex.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#モデルの「スコアを保存\n",
    "# #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "# dir_path = \"check_csv/{place_name}/check_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "# model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "\n",
    "# dir_path = \"check_csv/{place_name}/pred/pred_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "# pred_concat_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     gain_index=0\n",
    "#     model_gain_arr=[0]*len(result_test_df)\n",
    "#     for _, s in trans_df.iterrows():\n",
    "#         if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "#             count+=1#的中回数\n",
    "#             model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "#         gain_index+=1\n",
    "#     #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "#     gain_arr[index]=sum(model_gain_arr)\n",
    "#     accuracy_arr[index]=(count/num_1)*100\n",
    "#     try:\n",
    "#         pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "#     except:\n",
    "#         pred_0[index]=0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
