{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#収益計算の部分のバグを探す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.decomposition import PCA  #次元削減用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "import time\n",
    "import datetime\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "from sklearn.preprocessing import StandardScaler#モデルの評価用に標準化する関数\n",
    "import scipy.stats#モデルの評価用に標準化する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下収益計算部分のチェックを行うコードの雑処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     1,
     12,
     29,
     36,
     63,
     90,
     183
    ]
   },
   "outputs": [],
   "source": [
    "#バージョンとは関係のない、データの加工関数内で使う関数====================================================================================================================================================================================================\n",
    "def make_PCA_df(PCA_arr):#PCAで削減したものは二次元配列で帰ってくるので、それをデータフレームにして返す関数\n",
    "    X=[0]*len(PCA_arr)\n",
    "    Y=[0]*len(PCA_arr)\n",
    "    index=0\n",
    "    for arr in PCA_arr:\n",
    "        X[index]=arr[0]\n",
    "        Y[index]=arr[1]\n",
    "        index+=1\n",
    "    return pd.DataFrame({'X':X,'Y':Y})\n",
    "\n",
    "#dateのカラムを年だけに変換するやつ\n",
    "def trans_date_type(df):\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    df['year']=df['date'].dt.year\n",
    "    df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "#閾値を渡して、その値以上を1、未満を0に置き変える。\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def get_place_master():\n",
    "    place_dict={\"01\":\"kiryu\"\n",
    "                ,\"02\":\"toda\"\n",
    "                ,\"03\":\"edogawa\"\n",
    "                ,\"04\":\"heiwazima\"\n",
    "                ,\"05\":\"tamagawa\"\n",
    "                ,\"06\":\"hamanako\"\n",
    "                ,\"07\":\"gamagori\"\n",
    "                ,\"08\":\"tokoname\"\n",
    "                ,\"09\":\"tu\"\n",
    "                ,\"10\":\"mikuni\"\n",
    "                ,\"11\":\"biwako\"\n",
    "                ,\"12\":\"suminoe\"\n",
    "                ,\"13\":\"amagasaki\"\n",
    "                ,\"14\":\"naruto\"\n",
    "                ,\"15\":\"marugame\"\n",
    "                ,\"16\":\"kozima\"\n",
    "                ,\"17\":\"miyazima\"\n",
    "                ,\"18\":\"tokuyama\"\n",
    "                ,\"19\":\"simonoseki\"\n",
    "                ,\"20\":\"wakamatu\"\n",
    "                ,\"21\":\"asiya\"\n",
    "                ,\"22\":\"fukuoka\"\n",
    "                ,\"23\":\"karatu\"\n",
    "                ,\"24\":\"omura\"}\n",
    "    return place_dict\n",
    "\n",
    "def get_place_num_master():\n",
    "    place_num_dict={1:\"kiryu\"\n",
    "                ,2:\"toda\"\n",
    "                ,3:\"edogawa\"\n",
    "                ,4:\"heiwazima\"\n",
    "                ,5:\"tamagawa\"\n",
    "                ,6:\"hamanako\"\n",
    "                ,7:\"gamagori\"\n",
    "                ,8:\"tokoname\"\n",
    "                ,9:\"tu\"\n",
    "                ,10:\"mikuni\"\n",
    "                ,11:\"biwako\"\n",
    "                ,12:\"suminoe\"\n",
    "                ,13:\"amagasaki\"\n",
    "                ,14:\"naruto\"\n",
    "                ,15:\"marugame\"\n",
    "                ,16:\"kozima\"\n",
    "                ,17:\"miyazima\"\n",
    "                ,18:\"tokuyama\"\n",
    "                ,19:\"simonoseki\"\n",
    "                ,20:\"wakamatu\"\n",
    "                ,21:\"asiya\"\n",
    "                ,22:\"fukuoka\"\n",
    "                ,23:\"karatu\"\n",
    "                ,24:\"omura\"}\n",
    "    return place_num_dict\n",
    "\n",
    "def data_making_clustar(df):#クラスタリングあり、モータ番号、艇番号なし\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22})#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02})#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02})\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col])#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020]#2020のデータを完全に切り離す。\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values#\n",
    "    money=result_df_dummie['money'].values#\n",
    "    years=result_df_dummie['year'].values#\n",
    "\n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['year']==2018) | ((result_df_dummie['year']==2019) )].copy()#2019,2020のデータを検証用データに。\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['year']!=2018) & ((result_df_dummie['year']!=2019) )].copy()#そのほかを学習データに\n",
    "\n",
    "    #年の情報だけ切り分けに使ったからここで消す。\n",
    "    clustar_test_df=clustar_test_df.drop('year',axis=1)\n",
    "    clustar_train_df=clustar_train_df.drop('year',axis=1)\n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df])\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df\n",
    "    return model_df\n",
    "\n",
    "#モデルのパラメータ探索関数(XGboost)\n",
    "def model_score_XGboost_th(version,place_name,result_df):#XGboostの出力を確率のやつを使用したバージョン、閾値の探索も行う。\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2018) | ((result_df['year']==2019) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2018) & ((result_df['year']!=2019) )]#そのほかを学習データに\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=test_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    gain_mean=gain_mean.sort_index()\n",
    "\n",
    "    gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    gain_median=gain_median.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "                                'result_com_num':result_com_s.values,\n",
    "                                'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "                                'gain_mean':gain_mean.values,\n",
    "                                'gain_median':gain_median.values,})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "\n",
    "        gain_th=10#利益率の閾値\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        #depths_arr=[5,6,8]\n",
    "        depths_arr=[5,8]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=100+(sum_target_per*2)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1)\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "                target_x_test=result_test_df.drop('money',axis=1)\n",
    "                target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test=result_test_df['result_com']\n",
    "                train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2, shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "                #XGboostのデータ型に変換する\n",
    "                train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "                valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "                test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "\n",
    "                #xgb.config_context(verbosity=0)\n",
    "                param = {'max_depth': depth, #パラメータの設定\n",
    "                                 #'eta': 1.8,\n",
    "                                 #'eta': 0.8,\n",
    "                                 'eta': 1.3,\n",
    "                                 #'eta': 0.2,\n",
    "                                 #'objective': 'binary:hinge',\n",
    "                                 'objective': 'binary:logistic',#確率で出力\n",
    "                                 'eval_metric': 'logloss',\n",
    "                                 'verbosity':0,\n",
    "                                 'subsample':0.8,\n",
    "                                 'nthread':10,\n",
    "                                 'gpu_id':0,\n",
    "                                 'seed':7,\n",
    "                                 'tree_method':'gpu_hist'\n",
    "                                }\n",
    "                evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "                #bst = xgb.train(param, train,num_boost_round=1000,early_stopping_rounds=30)\n",
    "                #num_round = 10000\n",
    "                num_round = 400\n",
    "                bst = xgb.train(param, train,num_round,evallist, early_stopping_rounds=30, verbose_eval=0 )\n",
    "                #bst = xgb.train(param, train,num_round,evallist, verbose=100,early_stopping_rounds=30 )\n",
    "                #RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth)\n",
    "                #RF = RF.fit(target_x_train,target_y_train)\n",
    "\n",
    "\n",
    "                # 未知データに対する予測値\n",
    "                #predict_y_test = RF.predict(target_x_test)\n",
    "                predict_y_test=bst.predict(test)\n",
    "\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る\n",
    "                pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                          , 'test':target_y_test})\n",
    "\n",
    "                #th_arr=[0.1,0.3,0.5,0.6,0.7,0.8,0.9]\n",
    "                #th_arr=[0.01,0.03,0.05,0.07,0.9,0.1,0.13]#探索結果待ち、、、、、\n",
    "                th_arr=[0.85,0.9,0.92]\n",
    "                for th in th_arr:\n",
    "                    trans_df=pred_th_trans(pred_test_df,th)\n",
    "                    num_1=len(trans_df[trans_df['test']==1])\n",
    "                    count=0\n",
    "                    #追加　配当金の情報も考慮する。\n",
    "                    gain_index=0\n",
    "                    model_gain_arr=[0]*len(result_test_df)\n",
    "                    for _, s in trans_df.iterrows():\n",
    "                        if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count+=1#的中回数\n",
    "                            model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "                        gain_index+=1\n",
    "                    #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "                    gain_arr[index]=sum(model_gain_arr)\n",
    "                    accuracy_arr[index]=(count/num_1)*100\n",
    "                    try:\n",
    "                        pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "                    except:\n",
    "                        pred_0[index]=0\n",
    "                    #scoreのseriesに情報書き込み==================\n",
    "                    model_score_s['threshold']=th\n",
    "                    model_score_s['総収益']=sum(model_gain_arr)\n",
    "                    #model_score_s['投資金額']=100*sum(predict_y_test)\n",
    "                    model_score_s['投資金額']=100*trans_df['pred'].sum()\n",
    "                    model_score_s['出現数']=sum(target_y_test)\n",
    "                    #model_score_s['購買予測数']=sum(predict_y_test)\n",
    "                    model_score_s['購買予測数']=trans_df['pred'].sum()\n",
    "                    model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "                    model_score_s['購買的中率']=(count/trans_df['pred'].sum())*100\n",
    "                    model_score_s['的中数']=count\n",
    "                    model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "    #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "    dir_path = \"check_csv/{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"check_csv/{place_name}/pred/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実際のpickleモデルを使い、かつ収益計算部分に手を加えないで予測を行う。また予測もcsv/pred内に1/0で出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_check_V2_1_2(result_base_df,use_model_df,place_name,version):#pickleを使った時の予測内容のチェックをする\n",
    "    #探査結果から学習したモデルを保存する関数、\n",
    "    print(place_name)\n",
    "    \n",
    "    #==============================================================================\n",
    "    #学習関数で場所ごとにバージョンに対応した学習データを作る\n",
    "    result_df=data_making_clustar(result_base_df)\n",
    "    #==============================================================================\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "    pred_concat_df=pd.DataFrame(columns=use_model_df['target_com'].values,index=test_df.index)#予測データをまとめて持つdf\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "    for index, model_row in use_model_df.iterrows():\n",
    "        #パラメータ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "        #======================================================================================\n",
    "        #result_com=int(model_row['target_com'])\n",
    "        result_com=int(model_row['target_com'])\n",
    "        depth=int(model_row['depth'])\n",
    "        target_per=int(model_row['target_per'])\n",
    "        th=float(model_row['threshold'])\n",
    "\n",
    "        #======================================================================================\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "\n",
    "\n",
    "        #モデルの評価指標値を格納するseries======================\n",
    "        model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "        model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "        model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "        model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "        #======================\n",
    "        #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "        # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "        target_df=result_train_df#ベースのデータフレームをコピー\n",
    "        target_df=target_df.sample(frac=1,random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "        target_1_df=target_df[target_df['result_com']==1]\n",
    "        len_1=len(target_1_df)\n",
    "        target_0_df=target_df[target_df['result_com']==0]\n",
    "        len_0=len(target_0_df)\n",
    "        target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "        target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "        #学習＆予測ぱーと========================================================================\n",
    "        #==========================================================================================================================================\n",
    "        #データの切り分け\n",
    "        target_x_train=target_train_df.drop('money',axis=1)\n",
    "        target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "        target_x_test=result_test_df.drop('money',axis=1)\n",
    "        target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "        target_y_train=target_train_df['result_com']\n",
    "        target_y_test=result_test_df['result_com']\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2,shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "        #XGboostのデータ型に変換する\n",
    "        train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "        valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "        test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "\n",
    "        #xgb.config_context(verbosity=0)\n",
    "        param = {'max_depth': depth, #パラメータの設定\n",
    "                         #'eta': 1.8,\n",
    "                         #'eta': 0.8,\n",
    "                         'eta': 1.3,\n",
    "                         #'eta': 0.2,\n",
    "                         #'objective': 'binary:hinge',\n",
    "                         'objective': 'binary:logistic',#確率で出力\n",
    "                         'eval_metric': 'logloss',\n",
    "                         'verbosity':0,\n",
    "                         'subsample':0.8,\n",
    "                         'nthread':10,\n",
    "                         'gpu_id':0,\n",
    "                         'seed':7,\n",
    "                         'tree_method':'gpu_hist'\n",
    "                        }\n",
    "        evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "        num_round = 400\n",
    "        \n",
    "        \n",
    "        pickle_path=\"check_pickle/{place_name}/com{com}_{depth}_{target_per}_{th}_{place_name}.sav\".format(place_name=place_name,com=result_com,depth=depth,target_per=target_per,th=th)#モデルのdirs\n",
    "        bst = pickle.load(open(pickle_path, 'rb'))\n",
    "        # 未知データに対する予測値\n",
    "        #predict_y_test = RF.predict(target_x_test)\n",
    "        predict_y_test=bst.predict(test)\n",
    "\n",
    "        #[1]の正答率を見る\n",
    "        pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                  , 'test':target_y_test})\n",
    "        trans_df=pred_th_trans(pred_test_df,th)\n",
    "        num_1=len(trans_df[trans_df['test']==1])\n",
    "        count=0\n",
    "        #追加　配当金の情報も考慮する。\n",
    "        gain_index=0\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        for _, s in trans_df.iterrows():\n",
    "            if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                count+=1#的中回数\n",
    "                model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "            gain_index+=1\n",
    "        #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "        gain_arr[index]=sum(model_gain_arr)\n",
    "        accuracy_arr[index]=(count/num_1)*100\n",
    "        try:\n",
    "            pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "        except:\n",
    "            pred_0[index]=0\n",
    "            \n",
    "        pred_concat_df[result_com]=trans_df['test'].values#組の予測を結合\n",
    "        \n",
    "        #scoreのseriesに情報書き込み==================\n",
    "        model_score_s['threshold']=th\n",
    "        model_score_s['総収益']=sum(model_gain_arr)\n",
    "        #model_score_s['投資金額']=100*sum(predict_y_test)\n",
    "        model_score_s['投資金額']=100*trans_df['pred'].sum()\n",
    "        model_score_s['出現数']=sum(target_y_test)\n",
    "        #model_score_s['購買予測数']=sum(predict_y_test)\n",
    "        model_score_s['購買予測数']=trans_df['pred'].sum()\n",
    "        model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "        model_score_s['購買的中率']=(count/trans_df['pred'].sum())*100\n",
    "        model_score_s['的中数']=count\n",
    "        model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "    #model_score_df.to_csv('{}_model_score.csv'.format(place), encoding='utf_8_sig')\n",
    "    dir_path = \"check_csv/{place_name}/check_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    \n",
    "    dir_path = \"check_csv/{place_name}/pred/pred_pickle_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "    pred_concat_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    \n",
    "    #return None\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n",
      "[14:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[14:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n",
      "[14:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[14:35:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n",
      "[14:35:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[14:35:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n",
      "[14:35:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:355: Loading from a raw memory buffer on CPU only machine.  Changing tree_method to hist.\n",
      "[14:35:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:223: No visible GPU is found, setting `gpu_id` to -1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "\n",
    "place_name=\"kiryu\"\n",
    "place_master=get_place_master()\n",
    "#for place in tqdm(place_master.items()):\n",
    "    #place_name=place[1]\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "dir_path = \"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#学習のためのベースになるリザルトデータ\n",
    "result_base_df=pd.read_csv(dir_path)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "#各会場の成績の良かったモデルのスコアの読み込み\n",
    "model_dir_path = \"check_csv/{place_name}/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "use_model_df=pd.read_csv(model_dir_path)\n",
    "use_model_df=use_model_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "pickle_check_V2_1_2(result_base_df,use_model_df,place_name,version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pickleを使わずにtrainして予測を出力、正常にpickleが作動しているか同課の確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_check_V2_1_2(result_base_df,use_model_df,place_name,version):\n",
    "    #探査結果から学習したモデルを保存する関数、\n",
    "    print(place_name)\n",
    "    #==============================================================================\n",
    "    #学習関数で場所ごとにバージョンに対応した学習データを作る\n",
    "    result_df=data_making_clustar(result_base_df)\n",
    "    #==============================================================================\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    test_df = result_df[(result_df['year']==2019) | ((result_df['year']==2020) )]#2019,2020のデータを検証用データに。\n",
    "    train_df =  result_df[(result_df['year']!=2019) & ((result_df['year']!=2020) )]#そのほかを学習データに\n",
    "    pred_concat_df=pd.DataFrame(columns=use_model_df['target_com'].values,index=test_df.index)#予測データをまとめて持つdf\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "\n",
    "    # #x,yへの切り分け\n",
    "    # #出現数の分布\n",
    "    # result_com_s=test_df['result_com'].value_counts()\n",
    "    # result_com_s=result_com_s.sort_index()\n",
    "    # gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    # gain_mean=gain_mean.sort_index()\n",
    "    #\n",
    "    # gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    # gain_median=gain_median.sort_index()\n",
    "    # result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "    #                             'result_com_num':result_com_s.values,\n",
    "    #                             'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "    #                             'gain_mean':gain_mean.values,\n",
    "    #                             'gain_median':gain_median.values,})\n",
    "    # result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "    for index, model_row in use_model_df.iterrows():\n",
    "        #パラメータ＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝＝\n",
    "        #======================================================================================\n",
    "        #result_com=int(model_row['target_com'])\n",
    "        result_com=int(model_row['target_com'])\n",
    "        depth=int(model_row['depth'])\n",
    "        target_per=int(model_row['target_per'])\n",
    "        th=float(model_row['threshold'])\n",
    "\n",
    "        #======================================================================================\n",
    "        #======================================================================================\n",
    "        #======================================================================================\n",
    "\n",
    "\n",
    "        # gain_th=10#利益率の閾値\n",
    "        # result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        # buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        # num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        result_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(result_test_df)\n",
    "        i=0\n",
    "        for result in result_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "\n",
    "        result_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        result_test_df['money']=test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        test_gain_arr=test_money.values\n",
    "\n",
    "\n",
    "        #モデルの評価指標値を格納するseries======================\n",
    "        model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','総収益', '投資金額','出現数','購買予測数','利益率','購買的中率','的中数'], dtype='float64')\n",
    "        model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "        model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "        model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "        #======================\n",
    "        #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "        # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "        target_df=result_train_df#ベースのデータフレームをコピー\n",
    "        target_df=target_df.sample(frac=1,random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "        target_1_df=target_df[target_df['result_com']==1]\n",
    "        len_1=len(target_1_df)\n",
    "        target_0_df=target_df[target_df['result_com']==0]\n",
    "        len_0=len(target_0_df)\n",
    "        target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "        target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "        #学習＆予測ぱーと========================================================================\n",
    "        #==========================================================================================================================================\n",
    "        #データの切り分け\n",
    "        target_x_train=target_train_df.drop('money',axis=1)\n",
    "        target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "        target_x_test=result_test_df.drop('money',axis=1)\n",
    "        target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "        target_y_train=target_train_df['result_com']\n",
    "        target_y_test=result_test_df['result_com']\n",
    "        train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2,shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "        #XGboostのデータ型に変換する\n",
    "        train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "        valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "        test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "\n",
    "        #xgb.config_context(verbosity=0)\n",
    "        param = {'max_depth': depth, #パラメータの設定\n",
    "                         #'eta': 1.8,\n",
    "                         #'eta': 0.8,\n",
    "                         'eta': 1.3,\n",
    "                         #'eta': 0.2,\n",
    "                         #'objective': 'binary:hinge',\n",
    "                         'objective': 'binary:logistic',#確率で出力\n",
    "                         'eval_metric': 'logloss',\n",
    "                         'verbosity':0,\n",
    "                         'subsample':0.8,\n",
    "                         'nthread':10,\n",
    "                         'gpu_id':0,\n",
    "                         'seed':7,\n",
    "                         'tree_method':'gpu_hist'\n",
    "                        }\n",
    "        evallist = [(valid, 'eval'), (train, 'train')]#学習時にバリデーションを監視するデータの指定。\n",
    "        num_round = 400\n",
    "        bst = xgb.train(param, train,num_round,evallist, early_stopping_rounds=30, verbose_eval=0 )\n",
    "\n",
    "        # 未知データに対する予測値\n",
    "        #predict_y_test = RF.predict(target_x_test)\n",
    "        predict_y_test=bst.predict(test)\n",
    "        #[1]の正答率を見る\n",
    "        pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                  , 'test':target_y_test})\n",
    "        trans_df=pred_th_trans(pred_test_df,th)\n",
    "        num_1=len(trans_df[trans_df['test']==1])\n",
    "        count=0\n",
    "        #追加　配当金の情報も考慮する。\n",
    "        gain_index=0\n",
    "        model_gain_arr=[0]*len(result_test_df)\n",
    "        for _, s in trans_df.iterrows():\n",
    "            if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                count+=1#的中回数\n",
    "                model_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "            gain_index+=1\n",
    "        #print('test accyracy: {}'.format((count/num_1)*100))\n",
    "        gain_arr[index]=sum(model_gain_arr)\n",
    "        accuracy_arr[index]=(count/num_1)*100\n",
    "        try:\n",
    "            pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "        except:\n",
    "            pred_0[index]=0\n",
    "            pred_concat_df[result_com]=trans_df['test'].values#組の予測を結合\n",
    "        #scoreのseriesに情報書き込み==================\n",
    "        model_score_s['threshold']=th\n",
    "        model_score_s['総収益']=sum(model_gain_arr)\n",
    "        #model_score_s['投資金額']=100*sum(predict_y_test)\n",
    "        model_score_s['投資金額']=100*trans_df['pred'].sum()\n",
    "        model_score_s['出現数']=sum(target_y_test)\n",
    "        #model_score_s['購買予測数']=sum(predict_y_test)\n",
    "        model_score_s['購買予測数']=trans_df['pred'].sum()\n",
    "        model_score_s['利益率']=(model_score_s['総収益']/model_score_s['投資金額'])*100\n",
    "        model_score_s['購買的中率']=(count/trans_df['pred'].sum())*100\n",
    "        model_score_s['的中数']=count\n",
    "        model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "\n",
    "    \n",
    "    dir_path = \"check_csv/{place_name}/check_traine_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    \n",
    "    dir_path = \"check_csv/{place_name}/pred/pred_train_{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#予測の書き込み\n",
    "    pred_concat_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[14:41:51] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:548: Check failed: common::AllVisibleGPUs() >= 1 (0 vs. 1) : No visible GPU is found for XGBoost.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-bd8158edd6cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[0muse_model_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_model_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Unnamed: 0\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain_check_V2_1_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_base_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0muse_model_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mplace_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-5eb5cb4d38d5>\u001b[0m in \u001b[0;36mtrain_check_V2_1_2\u001b[1;34m(result_base_df, use_model_df, place_name, version)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[0mevallist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eval'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;31m#学習時にバリデーションを監視するデータの指定。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mnum_round\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m400\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mbst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mevallist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[1;31m# 未知データに対する予測値\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nabe0\\desktop\\nabepy\\env\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    195\u001b[0m                           \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m                           \u001b[0mmaximize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m                           early_stopping_rounds=early_stopping_rounds)\n\u001b[0m\u001b[0;32m    198\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbst\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nabe0\\desktop\\nabepy\\env\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks, evals_result, maximize, verbose_eval, early_stopping_rounds)\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nabe0\\desktop\\nabepy\\env\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1496\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1497\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1498\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1499\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\nabe0\\desktop\\nabepy\\env\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \"\"\"\n\u001b[0;32m    209\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: [14:41:51] C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/gbm/gbtree.cc:548: Check failed: common::AllVisibleGPUs() >= 1 (0 vs. 1) : No visible GPU is found for XGBoost."
     ]
    }
   ],
   "source": [
    "\n",
    "#データ格納用のディレクトリ作り\n",
    "version='V2_1_2'#バージョン\n",
    "\n",
    "place_name=\"kiryu\"\n",
    "place_master=get_place_master()\n",
    "#for place in tqdm(place_master.items()):\n",
    "    #place_name=place[1]\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "################################################========================================================================================================================\n",
    "dir_path = \"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#学習のためのベースになるリザルトデータ\n",
    "result_base_df=pd.read_csv(dir_path)\n",
    "result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "\n",
    "#各会場の成績の良かったモデルのスコアの読み込み\n",
    "model_dir_path = \"check_csv/{place_name}/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#使用するモデルのパラメータ読み込み\n",
    "use_model_df=pd.read_csv(model_dir_path)\n",
    "use_model_df=use_model_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "\n",
    "train_check_V2_1_2(result_base_df,use_model_df,place_name,version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 予測比較用の関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pred_arr(pred1_df,pred2_df):\n",
    "    for col_name1,col1,col_name2,col2 in zip(pred1_df.iteritems(),pred2_df.iteritems()):\n",
    "        if col1.values==col2.values:\n",
    "            print(col_name1,'  and  ',col_name2,'  is same pred \\n')\n",
    "    return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
