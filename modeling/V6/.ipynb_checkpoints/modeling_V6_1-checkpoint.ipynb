{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optimum-compatibility",
   "metadata": {},
   "source": [
    "# V6系統:betの金額をprobaをもとに可変する\n",
    "## V6_1:アルゴリズム:randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "figured-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from sklearn import preprocessing\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "import warnings\n",
    "#自作のモジュールのインポート\n",
    "sys.path.append(\"../..\")\n",
    "import module.master as master\n",
    "import module.modeling_scores as modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-assault",
   "metadata": {},
   "source": [
    "### モデルのスコア保存のための箱作り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "piano-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "version='V6_1'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#V6_系列の特殊処理\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/v6_score/{version}/\".format(place_name=place_name,version=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "important-dominant",
   "metadata": {},
   "source": [
    "## 実行する関数内で使用されている関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "perceived-blowing",
   "metadata": {
    "code_folding": [
     0,
     73,
     80,
     107
    ]
   },
   "outputs": [],
   "source": [
    "def get_event_info(result_base_df):\n",
    "    df=result_base_df.copy()\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    df['year']=df['date'].dt.year\n",
    "    df['month']=df['date'].dt.month\n",
    "    df['day']=df['date'].dt.day\n",
    "\n",
    "    num_date=1\n",
    "    num_date_arr=[]\n",
    "    last_race_date=df['date'].values[0]#前レースの日付(処理開始時用にtarainのデータの一番初めのdateを仮に入力しておく)\n",
    "    for index,row in df.iterrows():\n",
    "        today_date=row['date']\n",
    "        if today_date==last_race_date:#同じ日のレースだったらおなじレース日を配列に追加、次の日の日付を出力（ほぼ無操作みたいなもん）\n",
    "            next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "            num_date_arr.append(num_date)\n",
    "        else:#日にちが変わった時\n",
    "            if today_date==next_date:#想定していた日付（次の日のレース）だったら,レース日を一日足して、そのレース日番号を加算\n",
    "                num_date+=1\n",
    "                num_date_arr.append(num_date)\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "                #print(next_date)\n",
    "            else:#想定していた日付でない(違う大会になった)場合はレース日をリセット\n",
    "                num_date=1\n",
    "                num_date_arr.append(num_date)\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "    df['num_date']=num_date_arr\n",
    "\n",
    "    range_races=0#大会中の取得できたレースの数\n",
    "    range_date=1#大会の開催日数\n",
    "    range_date_arr=[]\n",
    "    range_date_arr_2=[]#for文中で繰り返し上書きさせる用の配列\n",
    "    last_race_date=df['date'].values[0]#前レースの日付(処理開始時用にtrainのデータの一番初めのdateを仮に入力しておく)\n",
    "    for index,row in df.iterrows():\n",
    "        today_date=row['date']\n",
    "        if today_date==last_race_date:#同じ日のレースだったらおなじレース日を配列に追加、次の日の日付を出力（ほぼ無操作みたいなもん）\n",
    "            range_races+=1\n",
    "            next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "            #num_date_arr.append(num_date)\n",
    "        else:#日にちが変わった時\n",
    "            if today_date==next_date:#想定していた日付（次の日のレース）だったら,レース日を一日足して終了\n",
    "                range_date+=1\n",
    "                range_races+=1\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日次の日\n",
    "            else:#想定していた日付でない(違う大会になった)場合は現在のrange_dateをもとに前の大会のレースに大会開催日数を持たせる。\n",
    "\n",
    "                range_date_arr_2=[range_date]*range_races\n",
    "                for num in range_date_arr_2:\n",
    "                    range_date_arr.append(num)\n",
    "                range_races=1#大会中の取得できたレースの数\n",
    "                range_date=1#大会の開催日数\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "    range_date_arr_2=[range_date]*range_races#最後の日は日付の変わり絵が発生しないので特別処理\n",
    "    for num in range_date_arr_2:\n",
    "        range_date_arr.append(num)\n",
    "    df['range_date']=range_date_arr\n",
    "\n",
    "    #四半期カラムの作成\n",
    "    df['season']=df['month']\n",
    "    df['season']=df['season'].replace([3,4,5],'sp')#春\n",
    "    df['season']=df['season'].replace([6,7,8],'su')#夏\n",
    "    df['season']=df['season'].replace([9,10,11],'au')#秋\n",
    "    df['season']=df['season'].replace([12,1,2],'wi')#冬\n",
    "    #df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_monthly_analysis(pred_df):#予測に加えて，配当，開催情報が結合されたdfを渡すことで月ごと関連の分析を行ってくれる関数\n",
    "    cols=['month','use','get','income','income_per','num_hit','buy_hit_per','mean_income','median_income']\n",
    "    monthly_analysis_df= pd.DataFrame(columns=cols)#月別収益結果の入る箱\n",
    "    months=pred_df['month'].value_counts(sort=False).index\n",
    "    for month in months:\n",
    "        monthly_df=pred_df[pred_df['month']==month].copy()\n",
    "        use_m=100*monthly_df['pred'].sum()\n",
    "        get_m=monthly_df['gain'].sum()\n",
    "        income=get_m-use_m\n",
    "        income_per=(get_m/use_m)*100\n",
    "        \n",
    "        \n",
    "        num_hit=monthly_df['hit_flag'].sum()\n",
    "        num_pred=pred_df['pred'].sum()\n",
    "        buy_hit_per=(num_hit/num_pred)*100\n",
    "        if num_hit==0:#警告文削除用\n",
    "            mean_income=0\n",
    "            median_income=0\n",
    "        else:\n",
    "            mean_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].mean()#１回の的中あたりの平均配当\n",
    "            median_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].median()#１回の的中あたりの中央配当\n",
    "        \n",
    "        append_arr=[month,use_m,get_m,income,income_per,num_hit,buy_hit_per,mean_income,median_income]\n",
    "        append_s=pd.Series(append_arr,index=cols)\n",
    "        monthly_analysis_df=monthly_analysis_df.append(append_s, ignore_index=True)\n",
    "    return monthly_analysis_df\n",
    "\n",
    "def get_season_date(now_date):#日付(datetime型)を渡すと，その日付で購買予測を行う際に使用するデータの区間を返す関数\n",
    "    use_data_year=now_date.year\n",
    "    if (now_date.month>=1)and(now_date.month<4):\n",
    "        use_data_month=1\n",
    "    elif (now_date.month>=4)and(now_date.month<7):\n",
    "        use_data_month=4\n",
    "    elif (now_date.month>=7)and(now_date.month<10):\n",
    "        use_data_month=7\n",
    "    elif (now_date.month>=10):\n",
    "        use_data_month=10\n",
    "    else:\n",
    "        print('what???????')\n",
    "    use_data_date=datetime.datetime(year=use_data_year, month=use_data_month,day=1)\n",
    "    return use_data_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "public-boulder",
   "metadata": {},
   "source": [
    "## 実行する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dressed-adventure",
   "metadata": {
    "code_folding": [
     1,
     7,
     71,
     112,
     135
    ]
   },
   "outputs": [],
   "source": [
    "#dateのカラムを年だけに変換するやつ\n",
    "def trans_date_type(df):#注意！！！こっちでは日付のデータの削除は行わない！！！！\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    df['year']=df['date'].dt.year\n",
    "    #df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "def data_making_mo_bo(df):#クラスタリングなし、ボート、艇番号あり\n",
    "    warnings.simplefilter('ignore',  category=pd.errors.PerformanceWarning)#解決できない警告は無視\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22})#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02})#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02})\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02})\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col])#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "    #boat もmoterも番号は1~99とする\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        for number in numbers:\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        moter_dummie_df=pd.get_dummies(result_df_dummie[col])#モータ番号をダミー化\n",
    "        for column, val in moter_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "\n",
    "    #boat番号をダミー化\n",
    "    for col in boat_cols:\n",
    "        for number in numbers:\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr\n",
    "        boat_dummie_df=pd.get_dummies(result_df_dummie[col])#boat番号をダミー化\n",
    "        for column, val in boat_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1)\n",
    "    clustar_target_df=result_df_dummie\n",
    "    clustaring_df=clustar_target_df\n",
    "    model_df=clustaring_df\n",
    "    model_df=trans_date_type(model_df)\n",
    "    return model_df\n",
    "\n",
    "def regulation_pred_proba_scale(to_pred_df,sample_proba_df):\n",
    "    #購買対象のレースの予測値を持ったdfと，その日の日付を渡す\n",
    "    #渡したdfを昨年の予測値でーたにまぜて，予測値の差が見えやすいようにスケーリングする．関数\n",
    "\n",
    "    #year:予測対象のレースが存在する年\n",
    "    pred_df_len=len(to_pred_df)\n",
    "    pred_proba_df=to_pred_df.copy()#何も加工をしていないprobaを保存しておく用にコピーを作っておく\n",
    "    pred_proba_df=pred_proba_df.loc[:, pred_proba_df.columns.str.contains('pred')]#予測に関する列のみを抽出\n",
    "    pred_proba_df=pred_proba_df.set_axis([col.replace('pred','proba') for col in pred_proba_df.columns],axis=1).copy()#購買金額に関連する列とわかるように名前を振りなおす\n",
    "    #display(pred_proba_df)\n",
    "    \n",
    "    #今回は大型のモデリングなので，一年前のレース結果に混ぜる方式は行わない\n",
    "    #year=target_date.year#予測対象のレースの年情報だけ切り抜く\n",
    "    #sample_year=year-1#一年前のprobaのデータに混ぜてスケーリングを行う\n",
    "    #sample_proba_df=pd.read_csv('test_csv/proba_get_use_{}.csv'.format(sample_year))\n",
    "    #num_race_cal=pred_df['num_race']\n",
    "    #sample_proba_df=pd.read_csv('test_csv/proba_get_use_{}.csv'.format(sample_year))\n",
    "#     num_race=np.arange(1,12)\n",
    "    sample_proba_df=sample_proba_df.loc[:, sample_proba_df.columns.str.contains('pred')].copy()#予測に関する列のみを抽出\n",
    "    concat_target_proba_df=pd.concat([sample_proba_df, to_pred_df], axis=0)\n",
    "    #probaをスケーリングして最大値1,最小値0にスケーリングする（pickleのモデルを使ってスケール変換機の保存を行う）\n",
    "    for col in sample_proba_df.columns:\n",
    "        concat_target_proba_df[col]=preprocessing.minmax_scale(concat_target_proba_df[col].values)\n",
    "    #trans_target_proba_df=concat_target_proba_df[concat_target_proba_df['date']==target_date].copy()\n",
    "    trans_target_proba_df=concat_target_proba_df[len(concat_target_proba_df)-pred_df_len:].copy()\n",
    "    #trans_proba_df=trans_proba_df.mask(trans_proba_df>=0.5,1)\n",
    "    #th=0.8#中心点が0.5とは限らないっぽい，なんとなく見ながら変更をしていく(前まではこの方式で可変してた)\n",
    "    th=trans_target_proba_df.describe().at['50%','pred']#切り捨てを50パーセンタイル未満に変更\n",
    "    #th=trans_target_proba_df.describe().at['75%','pred_proba']#切り捨てを50パーセンタイル未満に変更\n",
    "    #print(th)\n",
    "    #trans_target_proba_df=trans_target_proba_df.loc[:, trans_target_proba_df.columns.str.contains('pred')].copy()\n",
    "    #trans_target_proba_df.drop('num_race', axis=1)\n",
    "    trans_target_proba_df=trans_target_proba_df.mask(trans_target_proba_df<th,0)#閾値未満は購買を行わないので，切り捨てる\n",
    "    for col in sample_proba_df.columns:\n",
    "        trans_target_proba_df[col]=((trans_target_proba_df[col]-th)*2)\n",
    "    #trans_target_proba_df=(((trans_target_proba_df-th)*2)*10000)機能の製作時はここで係数をかけたが，実装時にはやらない.\n",
    "    trans_target_proba_df=trans_target_proba_df.mask(trans_target_proba_df<=0,0)#上記の計算式だと購買を行わないものはみんな-1000となるので０に置換する\n",
    "    #trans_target_proba_df['num_race']=num_race_cal\n",
    "    trans_target_proba_df=pd.concat([pred_proba_df,trans_target_proba_df],axis=1)\n",
    "    return trans_target_proba_df\n",
    "\n",
    "def add_BetMoney_BetFlag(regulation_df,bet_coefficient):#スケーリングしたprobaの予測値のdfと，購買金額の係数を渡して，実際の購買金額と，購買を行ったかどうかのフラグを付けてくれる関数\n",
    "    bet_regulation_df=regulation_df.copy()\n",
    "    #bet_regulation_df=bet_regulation_df.drop('num_race', axis=1)\n",
    "    bet_regulation_df=bet_regulation_df.set_axis([col.replace('pred','bet') for col in bet_regulation_df.columns],axis=1).copy()#購買金額に関連する列とわかるように名前を振りなおす\n",
    "    bet_regulation_df=bet_regulation_df.loc[:, bet_regulation_df.columns.str.contains('bet')]#購買金額に関する列のみを抽出\n",
    "    \n",
    "    #bet_regulation_df=((bet_regulation_df)*10000)\n",
    "    bet_regulation_df=((bet_regulation_df)*bet_coefficient)\n",
    "    bet_regulation_df=bet_regulation_df.round(-2)#投票時を想定して，桁を百円単位に丸める（四捨五入）\n",
    "    \n",
    "    #bet_proba_df=bet_proba_df.mask(bet_proba_df<=0,0)#上記の計算式だと購買を行わないものはみんな-1000となるので０に置換する\n",
    "    bet_flag_df=regulation_df.copy()\n",
    "    \n",
    "    #bet_flag_df=bet_flag_df.drop('num_race', axis=1)\n",
    "    #bet_flag_df=bet_flag_df.mask(bet_flag_df>=th,1).copy()#データの中心は変わらない,かつ中心以上により購買を行ったものにはフラグ付けを行う\n",
    "    bet_flag_df=bet_flag_df.set_axis([col.replace('pred','buy_flag') for col in bet_flag_df.columns],axis=1)#購買フラグに関連する列とわかるように名前を振りなおす\n",
    "    bet_flag_df=bet_flag_df.loc[:, bet_flag_df.columns.str.contains('buy_flag')]#予測の有無に関する列のみを抽出\n",
    "    bet_flag_df=bet_flag_df.mask(bet_flag_df>0,1).copy()#データの中心は変わらない,かつ中心以上により購買を行ったものにはフラグ付けを行う\n",
    "    proba_bet_flag_df=pd.concat([regulation_df,bet_regulation_df],axis=1)\n",
    "    proba_bet_flag_df=pd.concat([proba_bet_flag_df,bet_flag_df],axis=1)\n",
    "    #あたったレースにフラグを付ける＆獲得できた配当金の計算（レース単位でユニーク．前に出てきたものとしょりは　似ているが同じではない．）\n",
    "    return proba_bet_flag_df\n",
    "\n",
    "def calc_monthly_analysis_V6(pred_df):#予測に加えて，配当，開催情報が結合されたdfを渡すことで月ごと関連の分析を行ってくれる関数\n",
    "    cols=['month','use','get','income','income_per','num_hit','buy_hit_per','mean_income','median_income']\n",
    "    monthly_analysis_df= pd.DataFrame(columns=cols)#月別収益結果の入る箱\n",
    "    months=pred_df['month'].value_counts(sort=False).index\n",
    "    for month in months:\n",
    "        monthly_df=pred_df[pred_df['month']==month].copy()\n",
    "        use_m=monthly_df['bet'].sum()\n",
    "        get_m=monthly_df['gain'].sum()\n",
    "        income=get_m-use_m\n",
    "        income_per=(get_m/use_m)*100\n",
    "        \n",
    "        \n",
    "        num_hit=monthly_df['hit_flag'].sum()\n",
    "        num_pred=pred_df['buy_flag'].sum()\n",
    "        buy_hit_per=(num_hit/num_pred)*100\n",
    "        if num_hit==0:#警告文削除用\n",
    "            mean_income=0\n",
    "            median_income=0\n",
    "        else:\n",
    "            mean_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].mean()#１回の的中あたりの平均配当\n",
    "            median_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].median()#１回の的中あたりの中央配当\n",
    "        \n",
    "        append_arr=[month,use_m,get_m,income,income_per,num_hit,buy_hit_per,mean_income,median_income]\n",
    "        append_s=pd.Series(append_arr,index=cols)\n",
    "        #monthly_analysis_df=pd.concat([monthly_analysis_df, append_s], axis=0)\n",
    "        monthly_analysis_df=monthly_analysis_df.append(append_s, ignore_index=True)\n",
    "    return monthly_analysis_df\n",
    "\n",
    "def model_score_proba_RondomForest_has_final(version,place_name,result_df,target_year):#probaをもとに購買金額を可変する・アルゴリズムにrondomforestを用いている(本来であれば購買を行う区間であるfinalあり版，実際のモデル保存用とは別もの)\n",
    "    print(place_name)\n",
    "    #3年間で行う（test:2年前,test2:昨年,final:今年（本来であればここが実購買区間にあたる，テスト時（has_final）のみ存在））\n",
    "    #target_year:実際に購買を行う区間を指定（has_final版ではここは仮定の年となる），これをもとにtest1,test2はそれぞれ-1,-2年されて区間が決定する\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    #model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','total_get_test', 'total_use_test','num_com_test','num_pred_test','gain_test','gain_std_test','num_hit_test','buy_hit_per_test','buy_hit_per_std_test','plus_month_num_test','diff_mea_med_test'])#スコアを格納するdf\n",
    "    model_score_df=pd.DataFrame()\n",
    "    #学習データの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #result_df['date']=pd.to_datetime(result_df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    #result_df['year']=result_df['date'].dt.year\n",
    "    test1_year=target_year-2\n",
    "    test2_year=target_year-1\n",
    "    final_year=target_year\n",
    "    \n",
    "    result_df=result_df[result_df['year']<=target_year].copy()#先の年は削除する\n",
    "    test1_df = result_df[result_df['year']==test1_year].copy()#指標算出用のテストデータ\n",
    "    test2_df = result_df[result_df['year']==test2_year].copy()#指標算出用のテストデータ\n",
    "    final_df = result_df[result_df['year']==target_year].copy()#実購買予定区間テストデータ\n",
    "    train_df = result_df[(result_df['year']<test1_year)].copy()#そのほかを学習データに\n",
    "    #学習データを切り分けたらyear,dateはいらないから削除する\n",
    "    final_df=final_df.drop(['year'],axis=1)\n",
    "    test1_df=test1_df.drop(['year'],axis=1)\n",
    "    test2_df=test2_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "    test_dfs_dates={'test1': test1_df['date'].values,\n",
    "                   'test2': test2_df['date'].values,\n",
    "                   'final': final_df['date'].values\n",
    "                    }\n",
    "    final_df=final_df.drop(['date'],axis=1)\n",
    "    test1_df=test1_df.drop(['date'],axis=1)\n",
    "    test2_df=test2_df.drop(['date'],axis=1)\n",
    "    train_df=train_df.drop(['date'],axis=1)\n",
    "    \n",
    "    \n",
    "    train_money=train_df['money'].values\n",
    "    test1_money=test1_df['money'].values\n",
    "    test2_money=test2_df['money'].values\n",
    "    final_money=final_df['money'].values\n",
    "    \n",
    "    #final_test_gain_arr=final_test_money.values\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=train_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    target_result_coms=result_com_s.index[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "    for result_com_number in tqdm(target_result_coms):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "        #========================================\n",
    "        target_test1_df=test1_df.copy()\n",
    "        result_arr=[0]*len(target_test1_df)\n",
    "        i=0\n",
    "        for result in target_test1_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        target_test1_df['result_com']=result_arr\n",
    "        #========================================\n",
    "        target_test2_df=test2_df.copy()\n",
    "        result_arr=[0]*len(target_test2_df)\n",
    "        i=0\n",
    "        for result in target_test2_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        target_test2_df['result_com']=result_arr\n",
    "        #========================================\n",
    "        target_final_df=final_df.copy()\n",
    "        result_arr=[0]*len(target_final_df)\n",
    "        i=0\n",
    "        for result in target_final_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        target_final_df['result_com']=result_arr\n",
    "        #========================================\n",
    "        result_train_df['money']=train_money\n",
    "        target_test1_df['money']=test1_money\n",
    "        target_test2_df['money']=test2_money\n",
    "        target_final_df['money']=final_money\n",
    "        label_transed_dfs={'test1': target_test1_df,\n",
    "                            'test2': target_test2_df,\n",
    "                            'final': target_final_df\n",
    "                           }\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        #depths_arr=[5,6,8]\n",
    "        depths_arr=[5,7]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=80+(sum_target_per)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                #final区間がない用のseries\n",
    "                #model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','total_get_test', 'total_use_test','num_com_test','num_pred_test','gain_test','gain_std_test','num_hit_test','buy_hit_per_test','buy_hit_per_std_test','plus_month_num_test','diff_mea_med_test'], dtype='float64')\n",
    "                model_score_s=pd.Series(dtype='float64')\n",
    "                \n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1).copy()\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "\n",
    "                target_x_test1=target_test1_df.drop('money',axis=1).copy()\n",
    "                target_x_test1=target_x_test1.drop('result_com',axis=1)\n",
    "                \n",
    "                target_x_test2=target_test2_df.drop('money',axis=1).copy()\n",
    "                target_x_test2=target_x_test2.drop('result_com',axis=1)\n",
    "\n",
    "                target_x_final=target_final_df.drop('money',axis=1).copy()\n",
    "                target_x_final=target_x_final.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test1=target_test1_df['result_com']\n",
    "                target_y_test2=target_test2_df['result_com']\n",
    "                target_y_final=target_final_df['result_com']\n",
    "                #target_y_final=target_final_test_df['result_com']\n",
    "                train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2, shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "                #RF = RandomForestClassifier(random_state=7,n_estimators=1000,max_depth=depth,n_jobs=5)\n",
    "                RF = RandomForestClassifier(random_state=7,n_estimators=1000,max_depth=depth,n_jobs=-1)\n",
    "                RF = RF.fit(target_x_train,target_y_train)\n",
    "                #RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth)\n",
    "                #既知データに対する予測値\n",
    "                predict_y_train_proba_arr = RF.predict_proba(target_x_train)#まだ多次元リスト\n",
    "                predict_y_train=[proba_arr[1] for proba_arr in predict_y_train_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                # 未知データに対する予測値\n",
    "                predict_y_test1_proba_arr = RF.predict_proba(target_x_test1)#まだ多次元リスト\n",
    "                predict_y_test1=[proba_arr[1] for proba_arr in predict_y_test1_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                predict_y_test2_proba_arr = RF.predict_proba(target_x_test2)#まだ多次元リスト\n",
    "                predict_y_test2=[proba_arr[1] for proba_arr in predict_y_test2_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                predict_y_final_proba_arr = RF.predict_proba(target_x_final)#まだ多次元リスト\n",
    "                predict_y_final=[proba_arr[1] for proba_arr in predict_y_final_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る(resultはresult_comとは別物であることに注意！！すでにラベル変換が終わったのちのものである！！！)\n",
    "                pred_train_df=pd.DataFrame({'pred':predict_y_train#確率分布での出力\n",
    "                                          , 'result':target_y_train})\n",
    "                pred_test1_df=pd.DataFrame({'pred':predict_y_test1#確率分布での出力\n",
    "                                          , 'result':target_y_test1})\n",
    "                pred_test2_df=pd.DataFrame({'pred':predict_y_test2#確率分布での出力\n",
    "                                          , 'result':target_y_test2})\n",
    "                pred_final_test_df=pd.DataFrame({'pred':predict_y_final#確率分布での出力\n",
    "                                         , 'result':target_y_final})\n",
    "                sample_proba_df=pred_train_df.copy()#trainのprobaの分布をサンプルとして扱う\n",
    "                \n",
    "                #各区間の予測をディクショナリにまとめて一気にスケーリングとベットのフラグ付けを行う\n",
    "                pred_dfs = {'test1': pred_test1_df,\n",
    "                            'test2': pred_test2_df,\n",
    "                            'final': pred_final_test_df\n",
    "                           }\n",
    "                trans_pred_dfs={}\n",
    "                pred_keys=list(pred_dfs.keys())\n",
    "                for key, target_pred_df in pred_dfs.items():\n",
    "                    pred_regelated_proba_df=regulation_pred_proba_scale(target_pred_df,sample_proba_df)\n",
    "                    trans_proba_bet_df=add_BetMoney_BetFlag(pred_regelated_proba_df,bet_coefficient)\n",
    "                    trans_pred_dfs[key] = trans_proba_bet_df    \n",
    "                #display(trans_pred_dfs[pred_keys[0]])#チェック用\n",
    "                #このバージョンでは’確からしさ’をもとに購買金額を可変するので閾値でのバイナリへの変換は行わない．\n",
    "                #追加　配当金の情報も考慮する。\n",
    "                for key,target_df in trans_pred_dfs.items():\n",
    "                    model_analysis_df=target_df.copy()#集計結果を格納するためのdf(書き換えが怖い)               \n",
    "                    model_analysis_df[\"money\"]=label_transed_dfs[key]['money'].values\n",
    "                    count_test=0\n",
    "                    gain_index=0\n",
    "                    model_test_gain_arr=[0]*len(target_df)\n",
    "                    return_gains=model_analysis_df['money'].values#実際の配当金情報\n",
    "                    test_hit_arr=[0]*len(target_df)#あたっているかあたっていないかのフラグを格納した配列\n",
    "                    for _, s in target_df.iterrows():\n",
    "                        if ((s['buy_flag']==1) and (s['result']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count_test+=1#的中回数\n",
    "                            model_test_gain_arr[gain_index]=return_gains[gain_index]*(s['bet']/100)\n",
    "                            test_hit_arr[gain_index]=1\n",
    "                        gain_index+=1\n",
    "                    model_analysis_df['gain']=model_test_gain_arr\n",
    "                    #=================================================\n",
    "                    #(月での集計を加えて，comごとでの安定性について確認する)\n",
    "                    model_analysis_df['date']=test_dfs_dates[key]#月ごとの分析のために日付のデータを結合する\n",
    "                    model_analysis_df['month']=model_analysis_df['date'].dt.month#月ごとの分析のために日付のデータを月に変更する\n",
    "                    #集計のための情報を結合する\n",
    "                    model_analysis_df['hit_flag']=test_hit_arr#的中時に１のフラグを結合する\n",
    "                    model_analysis_df['gain']=model_test_gain_arr#的中して得られたゲインの金額を格納した配列\n",
    "                    \n",
    "                    #model_analysis_df['date']=test_dates\n",
    "                    model_analysis_df=get_event_info(model_analysis_df)#開催情報を取得\n",
    "                    test_m_analysis=calc_monthly_analysis_V6(model_analysis_df)#月別の分析結果を取得\n",
    "                    test_m_analysis=test_m_analysis.astype(float)\n",
    "                    #final_test_m_analysis=calc_monthly_analysis(final_test_analysis_df)#月別の分析結果を取得\n",
    "                    test_m_desc=test_m_analysis.describe()\n",
    "                    #final_test_m_desc=final_test_m_analysis.describe()\n",
    "                    #scoreのseriesに情報書き込み==================\n",
    "                    #テストデータ=============================================================\n",
    "                    #総収益\n",
    "                    model_score_s['total_get_{}'.format(key)]=sum(model_test_gain_arr)\n",
    "                    #投資金額\n",
    "                    model_score_s['total_use_{}'.format(key)]=model_analysis_df['bet'].sum()\n",
    "                    #出現数\n",
    "                    model_score_s['num_com_{}'.format(key)]=sum(model_analysis_df['result'])\n",
    "                    #購買予測数\n",
    "                    model_score_s['num_pred_{}'.format(key)]=model_analysis_df['buy_flag'].sum()\n",
    "                    #利益率\n",
    "                    model_score_s['gain_{}'.format(key)]=(model_score_s['total_get_{}'.format(key)]/model_score_s['total_use_{}'.format(key)])*100\n",
    "                    #利益率の標準偏差\n",
    "                    model_score_s['gain_std_{}'.format(key)]=test_m_desc.loc['std','income_per']\n",
    "                    #的中数\n",
    "                    model_score_s['num_hit_{}'.format(key)]=count_test\n",
    "                    #購買的中率\n",
    "                    model_score_s['buy_hit_per_{}'.format(key)]=(count_test/model_analysis_df['buy_flag'].sum())*100\n",
    "                    #購買的中率の標準偏差\n",
    "                    model_score_s['buy_hit_per_std_{}'.format(key)]=test_m_desc.loc['std','buy_hit_per']\n",
    "                    #配当がプラスになった月の数\n",
    "                    model_score_s['plus_month_num_{}'.format(key)]=len(test_m_analysis[test_m_analysis['income']>0])\n",
    "                    #得られた配当の中央値と平均の差(中央値-平均，つまりマイナスが大きいほど高い配当が引っ張っている)\n",
    "                    model_score_s['diff_mea_med_{}'.format(key)]=(test_m_desc.loc['mean','mean_income'])-(test_m_desc.loc['mean','median_income'])\n",
    "                #dfに書き込み\n",
    "                #model_score_df=pd.concat([model_score_df, model_score_s], axis=0)\n",
    "                model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "                model_score_df.to_csv('check_2.csv')\n",
    "                model_analysis_df.to_csv('check.csv')\n",
    "    #モデルの「スコアを保存\n",
    "                dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/v6_score/{V}/{place_name}_model_score_proba_{V}_{th}.csv\".format(place_name=place_name,V=version,th='50per')#作成したデータの書き込み先#使用するデータの読み込み\n",
    "                model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-general",
   "metadata": {},
   "source": [
    "# モデリング実行部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "activated-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=master.get_place_master()\n",
    "place_names=[place_name for place_name in place_master.values()]#会場名のみを収納した配列\n",
    "version='V6_1'#バージョン\n",
    "# place_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "gorgeous-bikini",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:23:02<00:00, 435.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:22:26<00:00, 433.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edogawa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:18:25<00:00, 425.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heiwazima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:20:51<00:00, 430.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamagawa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:23:14<00:00, 435.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamanako\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:29:05<00:00, 448.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamagori\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:29:24<00:00, 448.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokoname\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:29:47<00:00, 449.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:22:41<00:00, 434.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mikuni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:27:08<00:00, 443.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biwako\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:26:07<00:00, 441.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suminoe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:25:40<00:00, 440.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amagasaki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:26:01<00:00, 441.49s/it]\n",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naruto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:22:20<00:00, 433.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marugame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:30:32<00:00, 451.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kozima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:32:26<00:00, 455.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miyazima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:30:35<00:00, 451.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokuyama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:31:28<00:00, 453.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simonoseki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:25:59<00:00, 441.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wakamatu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:28:10<00:00, 446.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asiya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:31:43<00:00, 453.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fukuoka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:24:26<00:00, 438.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karatu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:27:44<00:00, 445.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omura\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:33:46<00:00, 458.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "elapsed time::297894.79719281197==================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#チェック\n",
    "time_sta = time.time()\n",
    "target_year=2021\n",
    "bet_coefficient=10000#投票金額を決定する係数 \n",
    "for place_name in place_names:\n",
    "#for place_name in place_names[5:10]:\n",
    "\n",
    "    #print(place_name)\n",
    "    result_filepath=\"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    result_base_df=pd.read_csv(result_filepath)\n",
    "    result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "    result_df=data_making_mo_bo(result_base_df)#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）    \n",
    "    #ディレクトリ要チェックや！！！（カチカチ）\n",
    "    model_score_proba_RondomForest_has_final(version,place_name,result_df,target_year)\n",
    "# 時間計測終了\n",
    "time_end = time.time()\n",
    "# 経過時間（秒）\n",
    "tim = time_end- time_sta\n",
    "print('DONE')\n",
    "print('elapsed time::{}=================================================================================================='.format(tim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "powered-variation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>result_com</th>\n",
       "      <th>money</th>\n",
       "      <th>racer_1_rank</th>\n",
       "      <th>racer_1_age</th>\n",
       "      <th>racer_1_doub</th>\n",
       "      <th>racer_1_ave_st</th>\n",
       "      <th>racer_2_rank</th>\n",
       "      <th>racer_2_age</th>\n",
       "      <th>racer_2_doub</th>\n",
       "      <th>...</th>\n",
       "      <th>racer_6_bo_91</th>\n",
       "      <th>racer_6_bo_92</th>\n",
       "      <th>racer_6_bo_93</th>\n",
       "      <th>racer_6_bo_94</th>\n",
       "      <th>racer_6_bo_95</th>\n",
       "      <th>racer_6_bo_96</th>\n",
       "      <th>racer_6_bo_97</th>\n",
       "      <th>racer_6_bo_98</th>\n",
       "      <th>racer_6_bo_99</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>6</td>\n",
       "      <td>920.0</td>\n",
       "      <td>4</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.295</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2</td>\n",
       "      <td>460.0</td>\n",
       "      <td>3</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.390</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.340</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>10</td>\n",
       "      <td>5210.0</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.270</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2</td>\n",
       "      <td>970.0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0.562</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>73</td>\n",
       "      <td>2940.0</td>\n",
       "      <td>3</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.252</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22185</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>7</td>\n",
       "      <td>840.0</td>\n",
       "      <td>4</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.245</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22186</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>97</td>\n",
       "      <td>35530.0</td>\n",
       "      <td>4</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.299</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22187</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>4</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>4</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0.579</td>\n",
       "      <td>0.14</td>\n",
       "      <td>3</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.378</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22188</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>1</td>\n",
       "      <td>720.0</td>\n",
       "      <td>4</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.16</td>\n",
       "      <td>4</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.572</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22189</th>\n",
       "      <td>2022-03-31</td>\n",
       "      <td>9</td>\n",
       "      <td>1290.0</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.349</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.027</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22190 rows × 1240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            date  result_com    money  racer_1_rank  racer_1_age  \\\n",
       "0     2012-04-01           6    920.0             4         39.0   \n",
       "1     2012-04-01           2    460.0             3         55.0   \n",
       "2     2012-04-01          10   5210.0             2         45.0   \n",
       "3     2012-04-01           2    970.0             3         32.0   \n",
       "4     2012-04-01          73   2940.0             3         37.0   \n",
       "...          ...         ...      ...           ...          ...   \n",
       "22185 2022-03-31           7    840.0             4         54.0   \n",
       "22186 2022-03-31          97  35530.0             4         25.0   \n",
       "22187 2022-03-31           4   1050.0             4         37.0   \n",
       "22188 2022-03-31           1    720.0             4         27.0   \n",
       "22189 2022-03-31           9   1290.0             3         60.0   \n",
       "\n",
       "       racer_1_doub  racer_1_ave_st  racer_2_rank  racer_2_age  racer_2_doub  \\\n",
       "0             0.445            0.18             2         35.0         0.295   \n",
       "1             0.390            0.18             2         42.0         0.340   \n",
       "2             0.333            0.19             2         29.0         0.270   \n",
       "3             0.339            0.16             4         39.0         0.562   \n",
       "4             0.344            0.15             2         42.0         0.252   \n",
       "...             ...             ...           ...          ...           ...   \n",
       "22185         0.622            0.14             2         45.0         0.245   \n",
       "22186         0.462            0.16             2         33.0         0.299   \n",
       "22187         0.579            0.14             3         34.0         0.378   \n",
       "22188         0.435            0.16             4         38.0         0.572   \n",
       "22189         0.349            0.17             1         20.0         0.027   \n",
       "\n",
       "       ...  racer_6_bo_91  racer_6_bo_92  racer_6_bo_93  racer_6_bo_94  \\\n",
       "0      ...              0              0              0              0   \n",
       "1      ...              0              0              0              0   \n",
       "2      ...              0              0              0              0   \n",
       "3      ...              0              0              0              0   \n",
       "4      ...              0              0              0              0   \n",
       "...    ...            ...            ...            ...            ...   \n",
       "22185  ...              0              0              0              0   \n",
       "22186  ...              0              0              0              0   \n",
       "22187  ...              0              0              0              0   \n",
       "22188  ...              0              0              0              0   \n",
       "22189  ...              0              0              0              0   \n",
       "\n",
       "       racer_6_bo_95  racer_6_bo_96  racer_6_bo_97  racer_6_bo_98  \\\n",
       "0                  0              0              0              0   \n",
       "1                  0              0              0              0   \n",
       "2                  0              0              0              0   \n",
       "3                  0              0              0              0   \n",
       "4                  0              0              0              0   \n",
       "...              ...            ...            ...            ...   \n",
       "22185              0              0              0              0   \n",
       "22186              0              0              0              0   \n",
       "22187              0              0              0              0   \n",
       "22188              0              0              0              0   \n",
       "22189              0              0              0              0   \n",
       "\n",
       "       racer_6_bo_99  year  \n",
       "0                  0  2012  \n",
       "1                  0  2012  \n",
       "2                  0  2012  \n",
       "3                  0  2012  \n",
       "4                  0  2012  \n",
       "...              ...   ...  \n",
       "22185              0  2022  \n",
       "22186              0  2022  \n",
       "22187              0  2022  \n",
       "22188              0  2022  \n",
       "22189              0  2022  \n",
       "\n",
       "[22190 rows x 1240 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "hearing-mystery",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\takuma\\nabepy\\env\\lib\\site-packages\\ipykernel\\ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.resetwarnings()\n",
    "\n",
    "warnings.simplefilter('ignore', FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-parallel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-butterfly",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
