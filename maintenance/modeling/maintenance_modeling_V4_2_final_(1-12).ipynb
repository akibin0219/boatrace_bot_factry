{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "white-penguin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "terminal-northeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "#自作のモジュールのインポート\n",
    "sys.path.append(\"../..\")\n",
    "import module.master as master\n",
    "import module.modeling_scores as modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "younger-germany",
   "metadata": {},
   "source": [
    "### モデルのスコア保存のための箱作り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "minus-oklahoma",
   "metadata": {},
   "outputs": [],
   "source": [
    "version='V4_2'#バージョン\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#V4_系列の特殊処理\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/v4_score/{version}/\".format(place_name=place_name,version=version)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-consumer",
   "metadata": {},
   "source": [
    "## 実行する関数内で使用されている関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "pretty-tragedy",
   "metadata": {
    "code_folding": [
     0,
     73,
     80,
     107
    ]
   },
   "outputs": [],
   "source": [
    "def get_event_info(result_base_df):\n",
    "    df=result_base_df.copy()\n",
    "    df['date']=pd.to_datetime(df['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    df['year']=df['date'].dt.year\n",
    "    df['month']=df['date'].dt.month\n",
    "    df['day']=df['date'].dt.day\n",
    "\n",
    "    num_date=1\n",
    "    num_date_arr=[]\n",
    "    last_race_date=df['date'].values[0]#前レースの日付(処理開始時用にtarainのデータの一番初めのdateを仮に入力しておく)\n",
    "    for index,row in df.iterrows():\n",
    "        today_date=row['date']\n",
    "        if today_date==last_race_date:#同じ日のレースだったらおなじレース日を配列に追加、次の日の日付を出力（ほぼ無操作みたいなもん）\n",
    "            next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "            num_date_arr.append(num_date)\n",
    "        else:#日にちが変わった時\n",
    "            if today_date==next_date:#想定していた日付（次の日のレース）だったら,レース日を一日足して、そのレース日番号を加算\n",
    "                num_date+=1\n",
    "                num_date_arr.append(num_date)\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "                #print(next_date)\n",
    "            else:#想定していた日付でない(違う大会になった)場合はレース日をリセット\n",
    "                num_date=1\n",
    "                num_date_arr.append(num_date)\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "    df['num_date']=num_date_arr\n",
    "\n",
    "    range_races=0#大会中の取得できたレースの数\n",
    "    range_date=1#大会の開催日数\n",
    "    range_date_arr=[]\n",
    "    range_date_arr_2=[]#for文中で繰り返し上書きさせる用の配列\n",
    "    last_race_date=df['date'].values[0]#前レースの日付(処理開始時用にtrainのデータの一番初めのdateを仮に入力しておく)\n",
    "    for index,row in df.iterrows():\n",
    "        today_date=row['date']\n",
    "        if today_date==last_race_date:#同じ日のレースだったらおなじレース日を配列に追加、次の日の日付を出力（ほぼ無操作みたいなもん）\n",
    "            range_races+=1\n",
    "            next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "            #num_date_arr.append(num_date)\n",
    "        else:#日にちが変わった時\n",
    "            if today_date==next_date:#想定していた日付（次の日のレース）だったら,レース日を一日足して終了\n",
    "                range_date+=1\n",
    "                range_races+=1\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日次の日\n",
    "            else:#想定していた日付でない(違う大会になった)場合は現在のrange_dateをもとに前の大会のレースに大会開催日数を持たせる。\n",
    "\n",
    "                range_date_arr_2=[range_date]*range_races\n",
    "                for num in range_date_arr_2:\n",
    "                    range_date_arr.append(num)\n",
    "                range_races=1#大会中の取得できたレースの数\n",
    "                range_date=1#大会の開催日数\n",
    "                last_race_date=row['date']#前回レース日を上書き\n",
    "                #next_date=train_df[train_df['date']==row['date'] + datetime.timedelta(days=1)]#次の日\n",
    "                next_date=row['date'] + datetime.timedelta(days=1)#次の日\n",
    "    range_date_arr_2=[range_date]*range_races#最後の日は日付の変わり絵が発生しないので特別処理\n",
    "    for num in range_date_arr_2:\n",
    "        range_date_arr.append(num)\n",
    "    df['range_date']=range_date_arr\n",
    "\n",
    "    #四半期カラムの作成\n",
    "    df['season']=df['month']\n",
    "    df['season']=df['season'].replace([3,4,5],'sp')#春\n",
    "    df['season']=df['season'].replace([6,7,8],'su')#夏\n",
    "    df['season']=df['season'].replace([9,10,11],'au')#秋\n",
    "    df['season']=df['season'].replace([12,1,2],'wi')#冬\n",
    "    #df=df.drop('date',axis=1)\n",
    "    return df\n",
    "\n",
    "def pred_th_trans(pred_df,th):\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_monthly_analysis(pred_df):#予測に加えて，配当，開催情報が結合されたdfを渡すことで月ごと関連の分析を行ってくれる関数\n",
    "    cols=['month','use','get','income','income_per','num_hit','buy_hit_per','mean_income','median_income']\n",
    "    monthly_analysis_df= pd.DataFrame(columns=cols)#月別収益結果の入る箱\n",
    "    months=pred_df['month'].value_counts(sort=False).index\n",
    "    for month in months:\n",
    "        monthly_df=pred_df[pred_df['month']==month].copy()\n",
    "        use_m=100*monthly_df['pred'].sum()\n",
    "        get_m=monthly_df['gain'].sum()\n",
    "        income=get_m-use_m\n",
    "        income_per=(get_m/use_m)*100\n",
    "        \n",
    "        \n",
    "        num_hit=monthly_df['hit_flag'].sum()\n",
    "        num_pred=pred_df['pred'].sum()\n",
    "        buy_hit_per=(num_hit/num_pred)*100\n",
    "        if num_hit==0:#警告文削除用\n",
    "            mean_income=0\n",
    "            median_income=0\n",
    "        else:\n",
    "            mean_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].mean()#１回の的中あたりの平均配当\n",
    "            median_income=monthly_df[monthly_df['hit_flag']==1][\"gain\"].median()#１回の的中あたりの中央配当\n",
    "        \n",
    "        append_arr=[month,use_m,get_m,income,income_per,num_hit,buy_hit_per,mean_income,median_income]\n",
    "        append_s=pd.Series(append_arr,index=cols)\n",
    "        monthly_analysis_df=monthly_analysis_df.append(append_s, ignore_index=True)\n",
    "    return monthly_analysis_df\n",
    "\n",
    "def get_season_date(now_date):#日付(datetime型)を渡すと，その日付で購買予測を行う際に使用するデータの区間を返す関数\n",
    "    use_data_year=now_date.year\n",
    "    if (now_date.month>=1)and(now_date.month<4):\n",
    "        use_data_month=1\n",
    "    elif (now_date.month>=4)and(now_date.month<7):\n",
    "        use_data_month=4\n",
    "    elif (now_date.month>=7)and(now_date.month<10):\n",
    "        use_data_month=7\n",
    "    elif (now_date.month>=10):\n",
    "        use_data_month=10\n",
    "    else:\n",
    "        print('what???????')\n",
    "    use_data_date=datetime.datetime(year=use_data_year, month=use_data_month,day=1)\n",
    "    return use_data_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-unknown",
   "metadata": {},
   "source": [
    "### 実行する関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "afraid-improvement",
   "metadata": {
    "code_folding": [
     0,
     122,
     244,
     534
    ]
   },
   "outputs": [],
   "source": [
    "def data_making_clustar_section_has_final(df,now_ym,range_test_m,range_final_m):#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）final(実運用バックテスト区間)あり版\n",
    "    #V4系列で使用する加工関数，年と月の情報を使って直近のデータを使って性能検証を行う（区間を使ってデータを作っている）\n",
    "    #validデータを作成するバージョンなので実装する際はこれをそのまま使わず，final_test部分の処理は消してくだちい\n",
    "    #yearが使わないと思うけど一応残してあるから邪魔だと思ったら消して下さい\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22}).copy()#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02}).copy()#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02}).copy()\n",
    "\n",
    "    #result_base_df=result_df.copy()\n",
    "    #result_base_df=get_event_info(result_base_df)#開催の情報について付与する関数(年月日に加えて，何日間の開催かどうかも教えてくれる)\n",
    "\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df.copy()\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr.copy()\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col]).copy()#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val.copy()\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    \n",
    "    #==========================================================================\n",
    "    #result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020].copy()#2020のデータを完全に切り離す。\n",
    "    #==========================================================================\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #now_ym:開始の月\n",
    "    test_st_date = now_ym - relativedelta(months=range_test_m)#テストデータに使用する区間を決める\n",
    "    final_test_en_date = now_ym + relativedelta(months=range_final_m)#最終テストの終了日\n",
    "    \n",
    "    clustar_final_test_df=result_df_dummie[(result_df_dummie['date']>=now_ym) & (result_df_dummie['date']<final_test_en_date) ].copy()#今の日付以降を最終チェックデータ(予測のターゲット)に。\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['date']<now_ym) & ((result_df_dummie['date']>=test_st_date) )].copy()#今日に日より前の，指定した区間でのテストデータ\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['date']<test_st_date)].copy()#そのほかを学習データに\n",
    "    result_df_dummie=result_df_dummie[result_df_dummie['date']<final_test_en_date]\n",
    "    #年の情報だけ切り分けに使ったからここで消す。\n",
    "    clustar_final_test_df=clustar_final_test_df.drop('date',axis=1).copy()\n",
    "    clustar_test_df=clustar_test_df.drop('date',axis=1).copy()\n",
    "    clustar_train_df=clustar_train_df.drop('date',axis=1).copy()\n",
    "\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values.copy()#\n",
    "    money=result_df_dummie['money'].values.copy()#\n",
    "    years=result_df_dummie['year'].values.copy()#\n",
    "    dates=result_df_dummie['date'].values.copy()#\n",
    "    \n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('year',axis=1)\n",
    "    #esult_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "#     clustar_final_test_df['check']='final'#確認用\n",
    "#     clustar_test_df['check']='test'#確認用\n",
    "#     clustar_train_df['check']='train'#確認用\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['date']=dates\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df.copy()\n",
    "    return model_df\n",
    "\n",
    "def data_making_clustar_section(df,now_ym,range_test_m):#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）final(実運用バックテスト区間)無し版\n",
    "    #V4系列で使用する加工関数，年と月の情報を使って直近のデータを使って性能検証を行う（区間を使ってデータを作っている）\n",
    "    #validデータを作成するバージョンなので実装する際はこれをそのまま使わず，final_test部分の処理は消してくだちい\n",
    "    #yearが使わないと思うけど一応残してあるから邪魔だと思ったら消して下さい\n",
    "    result_df=df\n",
    "    result_df=result_df.drop([\"racer_1_ID\",\"racer_2_ID\",\"racer_3_ID\",\"racer_4_ID\",\"racer_5_ID\",\"racer_6_ID\",],axis=1)#IDはいらないので削除\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_ave_st_time\":0.22}).copy()#新人のave_st_timeを0.22に\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_ave_st_time\":0.22}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_1_doub_win\":0.02}).copy()#新人の着に絡む確率ave_st_timeを0.02に(新人の半期の偏差から導出)\n",
    "    result_df=result_df.replace(0.0000,{\"racer_2_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_3_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_4_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_5_doub_win\":0.02}).copy()\n",
    "    result_df=result_df.replace(0.0000,{\"racer_6_doub_win\":0.02}).copy()\n",
    "\n",
    "    #result_base_df=result_df.copy()\n",
    "    #result_base_df=get_event_info(result_base_df)#開催の情報について付与する関数(年月日に加えて，何日間の開催かどうかも教えてくれる)\n",
    "\n",
    "    #ダミー変数化\n",
    "    result_df_dummie=result_df.copy()\n",
    "    race_dummie_df=pd.get_dummies(result_df_dummie['number_race'])#number_raceをダミー化\n",
    "    for column, val in race_dummie_df.iteritems():\n",
    "        result_df_dummie['race_{}'.format(int(column))]=val\n",
    "    result_df_dummie=result_df_dummie.drop('number_race',axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "    male_cols=[s for s in cols if 'male' in s]#性別を示すカラムを取り出す\n",
    "\n",
    "    #===========================新規、性別の取り出し機能が良くなかったため作り直す\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in male_cols:\n",
    "        for number in np.arange(0,2,1):\n",
    "              result_df_dummie['{}_{}'.format(col,int(number))]=empty_arr.copy()\n",
    "        male_dummie_df=pd.get_dummies(result_df_dummie[col]).copy()#性別をダミー化\n",
    "        for column, val in male_dummie_df.iteritems():\n",
    "              result_df_dummie['{}_{}'.format(col,int(column))]=val.copy()\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    cols=list(result_df_dummie.columns)\n",
    "\n",
    "\n",
    "\n",
    "    moter_cols=[s for s in cols if '_mo' in s]#モーター番号を示すカラムを取り出す\n",
    "    boat_cols=[s for s in cols if '_bo' in s]#ボート番号を示すカラムを取り出す\n",
    "\n",
    "    #boat、moterの情報は使わない、\n",
    "    numbers=np.arange(1, 100, 1)\n",
    "    empty_arr=[0]*len(result_df_dummie)\n",
    "    for col in moter_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "    for col in boat_cols:\n",
    "        result_df_dummie=result_df_dummie.drop('{}'.format(col),axis=1).copy()\n",
    "\n",
    "    #クラスタリング\n",
    "    #分けてみるクラスタの数は[3,5,7,9]の4個\n",
    "    #cluster_target_df　　trainのデータからリザルトと配当金を取り除いたもの\n",
    "    #学習データのdateを年に変換\n",
    "    result_df_dummie['date']=pd.to_datetime(result_df_dummie['date'])#日付が文字列なのでdateを日付型に変換\n",
    "    result_df_dummie['year']=result_df_dummie['date'].dt.year\n",
    "    \n",
    "    #==========================================================================\n",
    "    #result_df_dummie=result_df_dummie[result_df_dummie['year']!=2020].copy()#2020のデータを完全に切り離す。\n",
    "    #==========================================================================\n",
    "    #クラアスタリング用の学習、予測用のデータの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #now_ym:開始の月\n",
    "    test_st_date = now_ym - relativedelta(months=range_test_m)#テストデータに使用する区間を決める\n",
    "    #final_test_en_date = now_ym + relativedelta(months=range_final_m)#最終テストの終了日\n",
    "    \n",
    "    #clustar_final_test_df=result_df_dummie[(result_df_dummie['date']>=now_ym) & (result_df_dummie['date']<final_test_en_date) ].copy()#今の日付以降を最終チェックデータ(予測のターゲット)に。\n",
    "    clustar_test_df = result_df_dummie[(result_df_dummie['date']<now_ym) & ((result_df_dummie['date']>=test_st_date) )].copy()#今日より前の，指定した区間でのテストデータ\n",
    "    clustar_train_df =  result_df_dummie[(result_df_dummie['date']<test_st_date)].copy()#そのほかを学習データに\n",
    "    result_df_dummie=result_df_dummie[result_df_dummie['date']<now_ym]\n",
    "    #年の情報だけ切り分けに使ったからここで消す。\n",
    "    #clustar_final_test_df=clustar_final_test_df.drop('date',axis=1).copy()\n",
    "    clustar_test_df=clustar_test_df.drop('date',axis=1).copy()\n",
    "    clustar_train_df=clustar_train_df.drop('date',axis=1).copy()\n",
    "\n",
    "    #クラスタリングに邪魔だから消したいけど、後々使うものはいったんよけておく\n",
    "    result=result_df_dummie['result_com'].values.copy()#\n",
    "    money=result_df_dummie['money'].values.copy()#\n",
    "    years=result_df_dummie['year'].values.copy()#\n",
    "    dates=result_df_dummie['date'].values.copy()#\n",
    "    \n",
    "    #安全なところに移したら削除する\n",
    "    result_df_dummie=result_df_dummie.drop('result_com',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('money',axis=1)\n",
    "    result_df_dummie=result_df_dummie.drop('year',axis=1)\n",
    "    #esult_df_dummie=result_df_dummie.drop('date',axis=1)\n",
    "    \n",
    "    \n",
    "\n",
    "    target_num_cluster=[3,5,7,9]#分けるクラスタ数によってモデルの名前を変える\n",
    "    for num_cluster in target_num_cluster:\n",
    "        Km = KMeans(random_state=7,n_clusters=num_cluster).fit(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #final_test_pred =Km.predict(clustar_final_test_df)#rondom_stateはラッキーセブン\n",
    "        test_pred =Km.predict(clustar_test_df)#rondom_stateはラッキーセブン\n",
    "        train_pred = Km.predict(clustar_train_df)#rondom_stateはラッキーセブン\n",
    "        #Km=========================実査に使うときはこれのモデルを会場ごとに保存して使用。\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        #clustar_final_test_df['num={}'.format(num_cluster)]=final_test_pred\n",
    "        clustar_test_df['num={}'.format(num_cluster)]=test_pred\n",
    "        clustar_train_df['num={}'.format(num_cluster)]=train_pred\n",
    "\n",
    "    #結合して元の形に戻す。\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "#     clustar_final_test_df['check']='final'#確認用\n",
    "#     clustar_test_df['check']='test'#確認用\n",
    "#     clustar_train_df['check']='train'#確認用\n",
    "    #clustar_df=pd.concat([clustar_train_df, clustar_test_df,clustar_final_test_df]).copy()\n",
    "    clustar_df=pd.concat([clustar_train_df, clustar_test_df]).copy()\n",
    "    clustar_df['year']=years\n",
    "    clustar_df['date']=dates\n",
    "    clustar_df['money']=money\n",
    "    clustar_df['result_com']=result\n",
    "    model_df=clustar_df.copy()\n",
    "    return model_df\n",
    "\n",
    "def model_score_rondom_th_section_has_final(version,place_name,result_df,now_ym,range_test_m,range_final_m):#rondomforestの出力を確率のやつを使用したバージョン、閾値の探索も行う。final(実運用バックテスト区間)あり版＿＿※[注意]ここでのnow_ymは区間の開始日を示していて，今日の日付ではないことに注意\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','total_get_test', 'total_use_test','num_com_test','num_pred_test','gain_test','gain_std_test','num_hit_test','buy_hit_per_test','buy_hit_per_std_test','plus_month_num_test','diff_mea_med_test','total_get_final', 'total_use_final','num_com_final','num_pred_final','gain_final','gain_std_final','num_hit_final','buy_hit_per_final','buy_hit_per_std_final','plus_month_num_final','diff_mea_med_final'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #now_ym:開始の月\n",
    "    test_st_date = now_ym - relativedelta(months=range_test_m)#テストデータに使用する区間を決める\n",
    "    final_test_en_date = now_ym + relativedelta(months=range_final_m)#最終テストの終了日\n",
    "    #以下学習データ\n",
    "    final_test_df=result_df[(result_df['date']>=now_ym) & (result_df['date']<final_test_en_date) ].copy()#今の日付以降を最終チェックデータ(予測のターゲット)に。\n",
    "    test_df = result_df[(result_df['date']<now_ym) & ((result_df['date']>=test_st_date) )].copy()#今日に日より前の，指定した区間でのテストデータ\n",
    "    train_df =  result_df[(result_df['date']<test_st_date)].copy()#そのほかを学習データに\n",
    "\n",
    "    #V4ではdateは性能評価用のdf作成時に使用するので別でとっておく(最終的にtransしたpredのdfに結合する)\n",
    "    final_test_dates=final_test_df['date'].values\n",
    "    test_dates=test_df['date'].values\n",
    "    train_dates=train_df['date'].values\n",
    "\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    final_test_df=final_test_df.drop(['year'],axis=1)\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "    final_test_df=final_test_df.drop(['date'],axis=1)\n",
    "    test_df=test_df.drop(['date'],axis=1)\n",
    "    train_df=train_df.drop(['date'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "    final_test_money=pd.Series(final_test_df['money'])\n",
    "    test_gain_arr=test_money.values\n",
    "    final_test_gain_arr=final_test_money.values\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=test_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    gain_mean=gain_mean.sort_index()\n",
    "\n",
    "    gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    gain_median=gain_median.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "                                'result_com_num':result_com_s.values,\n",
    "                                'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "                                'gain_mean':gain_mean.values,\n",
    "                                'gain_median':gain_median.values,})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "\n",
    "        gain_th=10#利益率の閾値\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "\n",
    "        target_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(target_test_df)\n",
    "        i=0\n",
    "        for result in target_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        target_test_df['result_com']=result_arr\n",
    "\n",
    "        target_final_test_df=final_test_df.copy()\n",
    "        result_arr=[0]*len(target_final_test_df)\n",
    "        i=0\n",
    "        for result in target_final_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        target_final_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        target_test_df['money']=test_money\n",
    "        target_final_test_df['money']=final_test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(target_test_df)\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        #depths_arr=[5,6,8]\n",
    "        depths_arr=[5,7]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=80+(sum_target_per)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','total_get_test', 'total_use_test','num_com_test','num_pred_test','gain_test','gain_std_test','num_hit_test','buy_hit_per_test','buy_hit_per_std_test','plus_month_num_test','diff_mea_med_test','total_get_final', 'total_use_final','num_com_final','num_pred_final','gain_final','gain_std_final','num_hit_final','buy_hit_per_final','buy_hit_per_std_final','plus_month_num_final','diff_mea_med_final'], dtype='float64')\n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1).copy()\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "\n",
    "                target_x_test=target_test_df.drop('money',axis=1).copy()\n",
    "                target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "                target_x_final=target_final_test_df.drop('money',axis=1).copy()\n",
    "                target_x_final=target_x_final.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test=target_test_df['result_com']\n",
    "                target_y_final=target_final_test_df['result_com']\n",
    "                train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2, shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "                #XGboostのデータ型に変換する\n",
    "#                 train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "#                 valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "#                 test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "#                 final = xgb.DMatrix(target_x_final, label=target_y_final)#最終確認用の区間\n",
    "\n",
    "                #RF = RandomForestClassifier(random_state=7,n_estimators=1000,max_depth=depth,n_jobs=5)\n",
    "                RF = RandomForestClassifier(random_state=7,n_estimators=1000,max_depth=depth,n_jobs=1)\n",
    "                \n",
    "                RF = RF.fit(target_x_train,target_y_train)\n",
    "                #bst = xgb.train(param, train,num_round,evallist, verbose=100,early_stopping_rounds=30 )\n",
    "                #RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth)\n",
    "                #RF = RF.fit(target_x_train,target_y_train)\n",
    "\n",
    "\n",
    "                # 未知データに対する予測値\n",
    "                #predict_y_test = RF.predict(target_x_test)\n",
    "#                 predict_y_test=bst.predict(test)\n",
    "#                 predict_y_final=bst.predict(final)\n",
    "                predict_y_test_proba_arr = RF.predict_proba(target_x_test)#まだ多次元リスト\n",
    "                predict_y_test=[proba_arr[1] for proba_arr in predict_y_test_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                predict_y_final_proba_arr = RF.predict_proba(target_x_final)#まだ多次元リスト\n",
    "                predict_y_final=[proba_arr[1] for proba_arr in predict_y_final_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る\n",
    "                pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                          , 'test':target_y_test})\n",
    "                pred_final_test_df=pd.DataFrame({'pred_proba':predict_y_final#確率分布での出力\n",
    "                                          , 'test':target_y_final})\n",
    "\n",
    "\n",
    "                th_arr=[0.5,0.54,0.58]\n",
    "                for th in th_arr:\n",
    "                    trans_test_df=pred_th_trans(pred_test_df,th)\n",
    "                    trans_final_test_df=pred_th_trans(pred_final_test_df,th)\n",
    "                    #num_1=len(trans_df[trans_df['test']==1])\n",
    "                    #追加　配当金の情報も考慮する。\n",
    "                    count_test=0\n",
    "                    gain_index=0\n",
    "                    model_test_gain_arr=[0]*len(test_df)\n",
    "                    test_hit_arr=[0]*len(test_df)#あたっているかあたっていないかのフラグを格納した配列\n",
    "                    for _, s in trans_test_df.iterrows():\n",
    "                        if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count_test+=1#的中回数\n",
    "                            model_test_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "                            test_hit_arr[gain_index]=1\n",
    "                        gain_index+=1\n",
    "\n",
    "                    count_final=0\n",
    "                    gain_index=0\n",
    "                    model_final_test_gain_arr=[0]*len(final_test_df)\n",
    "                    final_test_hit_arr=[0]*len(final_test_df)#あたっているかあたっていないかのフラグを格納した配列\n",
    "                    for _, s in trans_final_test_df.iterrows():\n",
    "                        if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count_final+=1#的中回数\n",
    "                            model_final_test_gain_arr[gain_index]=final_test_gain_arr[gain_index]\n",
    "                            final_test_hit_arr[gain_index]=1\n",
    "                        gain_index+=1\n",
    "\n",
    "    #                     gain_arr[index]=sum(model_gain_arr)\n",
    "    #                     accuracy_arr[index]=(count/num_1)*100\n",
    "                    #=================================================\n",
    "                    try:\n",
    "                        pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "                    except:\n",
    "                        pred_0[index]=0\n",
    "\n",
    "                    #V4から増やした集計(月での集計を加えて，comごとでの安定性について確認する)\n",
    "                    test_analysis_df=trans_test_df.copy()#集計結果を格納するためのdf\n",
    "                    final_test_analysis_df=trans_final_test_df.copy()#集計結果を格納するためのdf\n",
    "                    #集計のための情報を結合する\n",
    "                    test_analysis_df['hit_flag']=test_hit_arr#的中時に１のフラグを結合する\n",
    "                    test_analysis_df['gain']=model_test_gain_arr#的中して得られたゲインの金額を格納した配列\n",
    "                    test_analysis_df['date']=test_dates#月ごとの分析のために日付のデータを結合する\n",
    "                    test_analysis_df=get_event_info(test_analysis_df)#開催情報を取得\n",
    "\n",
    "                    final_test_analysis_df['hit_flag']=final_test_hit_arr#的中時に１のフラグを結合する\n",
    "                    final_test_analysis_df['gain']=model_final_test_gain_arr#的中して得られたゲインの金額を格納した配列\n",
    "                    final_test_analysis_df['date']=final_test_dates#月ごとの分析のために日付のデータを結合する\n",
    "                    final_test_analysis_df=get_event_info(final_test_analysis_df)#開催情報を取得\n",
    "\n",
    "                    test_m_analysis=calc_monthly_analysis(test_analysis_df)#月別の分析結果を取得\n",
    "                    final_test_m_analysis=calc_monthly_analysis(final_test_analysis_df)#月別の分析結果を取得\n",
    "                    test_m_desc=test_m_analysis.describe()\n",
    "                    final_test_m_desc=final_test_m_analysis.describe()\n",
    "                    #scoreのseriesに情報書き込み==================\n",
    "                    model_score_s['threshold']=th\n",
    "                    #テストデータ=============================================================\n",
    "                    #総収益\n",
    "                    model_score_s['total_get_test']=sum(model_test_gain_arr)\n",
    "                    #投資金額\n",
    "                    model_score_s['total_use_test']=100*trans_test_df['pred'].sum()\n",
    "                    #出現数\n",
    "                    model_score_s['num_com_test']=sum(target_y_test)\n",
    "                    #購買予測数\n",
    "                    model_score_s['num_pred_test']=trans_test_df['pred'].sum()\n",
    "                    #利益率\n",
    "                    model_score_s['gain_test']=(model_score_s['total_get_test']/model_score_s['total_use_test'])*100\n",
    "                    #利益率の標準偏差\n",
    "                    model_score_s['gain_std_test']=test_m_desc.loc['std','income_per']\n",
    "                    #的中数\n",
    "                    model_score_s['num_hit_test']=count_test\n",
    "                    #購買的中率\n",
    "                    model_score_s['buy_hit_per_test']=(count_test/trans_test_df['pred'].sum())*100\n",
    "                    #購買的中率の標準偏差\n",
    "                    model_score_s['buy_hit_per_std_test']=test_m_desc.loc['std','buy_hit_per']\n",
    "                    #配当がプラスになった月の数\n",
    "                    model_score_s['plus_month_num_test']=len(test_m_analysis[test_m_analysis['income']>0])\n",
    "                    #得られた配当の中央値と平均の差(中央値-平均，つまりマイナスが大きいほど高い配当が引っ張っている)\n",
    "                    model_score_s['diff_mea_med_test']=(test_m_desc.loc['mean','mean_income'])-(test_m_desc.loc['mean','median_income'])\n",
    "\n",
    "                    #最終テストデータ(カラム名の説明に関してはテストと同じなので割愛させていただきます)=============================================================\n",
    "                    model_score_s['total_get_final']=sum(model_final_test_gain_arr)\n",
    "                    model_score_s['total_use_final']=100*trans_final_test_df['pred'].sum()\n",
    "                    model_score_s['num_com_final']=sum(target_y_final)\n",
    "                    model_score_s['num_pred_final']=trans_final_test_df['pred'].sum()\n",
    "                    model_score_s['gain_final']=(model_score_s['total_get_final']/model_score_s['total_use_final'])*100\n",
    "                    model_score_s['gain_std_final']=final_test_m_desc.loc['std','income_per']\n",
    "\n",
    "                    model_score_s['num_hit_final']=count_final\n",
    "                    model_score_s['buy_hit_per_final']=(count_final/trans_final_test_df['pred'].sum())*100\n",
    "                    model_score_s['buy_hit_per_std_final']=final_test_m_desc.loc['std','buy_hit_per']\n",
    "                    model_score_s['plus_month_num_final']=len(final_test_m_analysis[final_test_m_analysis['income']>0])\n",
    "                    model_score_s['diff_mea_med_final']=(final_test_m_desc.loc['mean','mean_income'])-(final_test_m_desc.loc['mean','median_income'])\n",
    "\n",
    "                    #dfに書き込み\n",
    "                    model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "    #dir_path = \"../../bot_database/{place_name}/model_score_{place_name}/v4_score/{V}/{place_name}_model_score_st{stDate}_finalM{finalM}_{V}.csv\".format(place_name=place_name,V=version,stDate=now_ym.strftime('%Y%m%d'),finalM=range_final_m)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/v4_score/{V}/{place_name}_model_score_st{stDate}_finalM{finalM}_{V}.csv\".format(place_name=place_name,V=version,stDate=now_ym.strftime('%Y%m%d'),finalM=range_final_m)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    \n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n",
    "\n",
    "def model_score_rondom_th_section(version,place_name,result_df,now_ym,range_test_m):#rondomforestの出力を確率のやつを使用したバージョン、閾値の探索も行う。final(実運用バックテスト区間)無し版__※[注意]ここでのnow_ymは区間の開始日を示していて，今日の日付ではないことに注意\n",
    "    print(place_name)\n",
    "    #result_dfは加工関数にて分けられたものを渡す。\n",
    "    model_score_df=pd.DataFrame(columns=['target_com','depth','target_per','threshold','total_get_test', 'total_use_test','num_com_test','num_pred_test','gain_test','gain_std_test','num_hit_test','buy_hit_per_test','buy_hit_per_std_test','plus_month_num_test','diff_mea_med_test'])#スコアを格納するdf\n",
    "\n",
    "    #学習データの切り分け\n",
    "    #年，月，日とかの取得\n",
    "    #now_ym:開始の月\n",
    "    test_st_date = now_ym - relativedelta(months=range_test_m)#テストデータに使用する区間を決める\n",
    "    #final_test_en_date = now_ym + relativedelta(months=range_final_m)#最終テストの終了日\n",
    "    #以下学習データ\n",
    "    #final_test_df=result_df[(result_df['date']>=now_ym) & (result_df['date']<final_test_en_date) ].copy()#今の日付以降を最終チェックデータ(予測のターゲット)に。\n",
    "    test_df = result_df[(result_df['date']<now_ym) & ((result_df['date']>=test_st_date) )].copy()#今日に日より前の，指定した区間でのテストデータ\n",
    "    train_df =  result_df[(result_df['date']<test_st_date)].copy()#そのほかを学習データに\n",
    "    #V4ではdateは性能評価用のdf作成時に使用するので別でとっておく(最終的にtransしたpredのdfに結合する)\n",
    "    #final_test_dates=final_test_df['date'].values\n",
    "    test_dates=test_df['date'].values\n",
    "    train_dates=train_df['date'].values\n",
    "\n",
    "    #学習データを切り分けたらyearはいらないから削除する\n",
    "    #final_test_df=final_test_df.drop(['year'],axis=1)\n",
    "    test_df=test_df.drop(['year'],axis=1)\n",
    "    train_df=train_df.drop(['year'],axis=1)\n",
    "    #final_test_df=final_test_df.drop(['date'],axis=1)\n",
    "    test_df=test_df.drop(['date'],axis=1)\n",
    "    train_df=train_df.drop(['date'],axis=1)\n",
    "\n",
    "    train_money=pd.Series(train_df['money'])\n",
    "    test_money=pd.Series(test_df['money'])\n",
    "    #final_test_money=pd.Series(final_test_df['money'])\n",
    "    test_gain_arr=test_money.values\n",
    "    #final_test_gain_arr=final_test_money.values\n",
    "    #x,yへの切り分け\n",
    "    #出現数の分布\n",
    "    result_com_s=test_df['result_com'].value_counts()\n",
    "    result_com_s=result_com_s.sort_index()\n",
    "    gain_mean=test_df.groupby('result_com')['money'].mean()\n",
    "    gain_mean=gain_mean.sort_index()\n",
    "\n",
    "    gain_median=test_df.groupby('result_com')['money'].median()\n",
    "    gain_median=gain_median.sort_index()\n",
    "    result_com_df=pd.DataFrame({'result_com':result_com_s.index,\n",
    "                                'result_com_num':result_com_s.values,\n",
    "                                'result_com_per':result_com_s.values/sum(result_com_s.values)*100,\n",
    "                                'gain_mean':gain_mean.values,\n",
    "                                'gain_median':gain_median.values,})\n",
    "    result_com_df=result_com_df.iloc[0:28]#探索的に探すにも最後のほうは役にモデルなのはわかっているため\n",
    "    for result_com_number in tqdm(result_com_df['result_com'].values):\n",
    "        #print(result_com_number)\n",
    "        result_com=result_com_number\n",
    "        #result_comごとの閾値の決定========================================================================\n",
    "\n",
    "        gain_th=10#利益率の閾値\n",
    "        result_s=result_com_df[result_com_df['result_com']==result_com]\n",
    "        buy_accuracy_th=result_s['result_com_per'].values[0]*1.1#買ったうちの的中率の閾値\n",
    "        num_tp_th=result_s['result_com_num'].values[0]*0.2#あたった回数の閾値(出現回数の20%が的中)\n",
    "        #===============================================================================\n",
    "        #学習データのラベル変換==========================================================\n",
    "        result_train_df=train_df.copy()\n",
    "        result_arr=[0]*len(result_train_df)\n",
    "        i=0\n",
    "        for result in result_train_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        result_train_df['result_com']=result_arr\n",
    "\n",
    "        target_test_df=test_df.copy()\n",
    "        result_arr=[0]*len(target_test_df)\n",
    "        i=0\n",
    "        for result in target_test_df['result_com']:\n",
    "            if ((result==result_com)):\n",
    "                result_arr[i]=1\n",
    "            else:\n",
    "                result_arr[i]=0\n",
    "            i+=1\n",
    "        target_test_df['result_com']=result_arr\n",
    "\n",
    "#         target_final_test_df=final_test_df.copy()\n",
    "#         result_arr=[0]*len(target_final_test_df)\n",
    "#         i=0\n",
    "#         for result in target_final_test_df['result_com']:\n",
    "#             if ((result==result_com)):\n",
    "#                 result_arr[i]=1\n",
    "#             else:\n",
    "#                 result_arr[i]=0\n",
    "#             i+=1\n",
    "#         target_final_test_df['result_com']=result_arr\n",
    "\n",
    "        result_train_df['money']=train_money\n",
    "        target_test_df['money']=test_money\n",
    "        #target_final_test_df['money']=final_test_money\n",
    "        #学習データラベル変換終わり============================================\n",
    "\n",
    "        for_arr=np.arange(1,85)\n",
    "        #for_arr=np.arange(1,100,1)\n",
    "        accuracy_arr=[0]*len(for_arr)\n",
    "        target_per_arr=[0]*len(for_arr)\n",
    "        pred_0=[0]*len(for_arr)\n",
    "        gain_arr=[0]*len(for_arr)\n",
    "        model_gain_arr=[0]*len(target_test_df)\n",
    "        #depths_arr=[4,5,6,7,8]\n",
    "        #depths_arr=[5,6,8]\n",
    "        depths_arr=[5,7]\n",
    "        for depth in depths_arr:\n",
    "            for sum_target_per in for_arr:\n",
    "\n",
    "                index=sum_target_per-1\n",
    "                #target_per=50+sum_target_per\n",
    "                target_per=80+(sum_target_per)\n",
    "                target_per_arr[index]=target_per\n",
    "\n",
    "                #モデルの評価指標値を格納するseries======================\n",
    "                #final区間がない用のseries\n",
    "                model_score_s=pd.Series(index=['target_com','depth','target_per','threshold','total_get_test', 'total_use_test','num_com_test','num_pred_test','gain_test','gain_std_test','num_hit_test','buy_hit_per_test','buy_hit_per_std_test','plus_month_num_test','diff_mea_med_test'], dtype='float64')\n",
    "                model_score_s['target_com']=result_com#目標としているresult_comラベル番号\n",
    "                model_score_s['depth']=depth#ハイパーパラメータ＿木の深さ\n",
    "                model_score_s['target_per']=target_per#学習データ_1に対してどの程度の0のデータを持たせるか。\n",
    "                #======================\n",
    "                #trainの[0]に対して、target_perの割合の量[1]を持った学習データの作成\n",
    "                # 一層目の判別機のtrainデータ　:terget_result_df\n",
    "                target_df=result_train_df#ベースのデータフレームをコピー\n",
    "                target_df=target_df.sample(frac=1, random_state=7)#シャッフル、時系列の偏りを無くす\n",
    "                target_1_df=target_df[target_df['result_com']==1]\n",
    "                len_1=len(target_1_df)\n",
    "                target_0_df=target_df[target_df['result_com']==0]\n",
    "                len_0=len(target_0_df)\n",
    "                target_0_df=target_0_df.iloc[(len_0-int(len_1*(target_per/100))):len_0]#1に対する目標の割合ぶん0の結果だったレースを抽出（後ろから抽出）\n",
    "                target_train_df=pd.concat([target_1_df, target_0_df])\n",
    "                #学習＆予測ぱーと========================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #データの切り分け\n",
    "                target_x_train=target_train_df.drop('money',axis=1).copy()\n",
    "                target_x_train=target_x_train.drop('result_com',axis=1)\n",
    "\n",
    "                target_x_test=target_test_df.drop('money',axis=1).copy()\n",
    "                target_x_test=target_x_test.drop('result_com',axis=1)\n",
    "\n",
    "                #target_x_final=target_final_test_df.drop('money',axis=1).copy()\n",
    "                #target_x_final=target_x_final.drop('result_com',axis=1)\n",
    "\n",
    "                target_y_train=target_train_df['result_com']\n",
    "                target_y_test=target_test_df['result_com']\n",
    "                #target_y_final=target_final_test_df['result_com']\n",
    "                train_x, valid_x, train_y, valid_y = train_test_split(target_x_train, target_y_train, test_size=0.2, shuffle=True, random_state=7)#学習データ内でさらに分割してロスをもとに修正をする。\n",
    "\n",
    "                #XGboostのデータ型に変換する\n",
    "#                 train = xgb.DMatrix(train_x, label=train_y)#学習用\n",
    "#                 valid = xgb.DMatrix(valid_x, label=valid_y)#学習時のロス修正用\n",
    "#                 test = xgb.DMatrix(target_x_test, label=target_y_test)#実際に使った時の利益率の算出用\n",
    "#                 final = xgb.DMatrix(target_x_final, label=target_y_final)#最終確認用の区間\n",
    "                #RF = RandomForestClassifier(random_state=7,n_estimators=1000,max_depth=depth,n_jobs=5)\n",
    "                RF = RandomForestClassifier(random_state=7,n_estimators=1000,max_depth=depth,n_jobs=1)\n",
    "                RF = RF.fit(target_x_train,target_y_train)\n",
    "                #bst = xgb.train(param, train,num_round,evallist, verbose=100,early_stopping_rounds=30 )\n",
    "                #RF = RandomForestClassifier(random_state=1,n_estimators=1000,max_depth=depth)\n",
    "                #RF = RF.fit(target_x_train,target_y_train)\n",
    "\n",
    "\n",
    "                # 未知データに対する予測値\n",
    "                #predict_y_test = RF.predict(target_x_test)\n",
    "#                 predict_y_test=bst.predict(test)\n",
    "#                 predict_y_final=bst.predict(final)\n",
    "                predict_y_test_proba_arr = RF.predict_proba(target_x_test)#まだ多次元リスト\n",
    "                predict_y_test=[proba_arr[1] for proba_arr in predict_y_test_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                #predict_y_final_proba_arr = RF.predict_proba(target_x_final)#まだ多次元リスト\n",
    "                #predict_y_final=[proba_arr[1] for proba_arr in predict_y_final_proba_arr]#1にあたる部分の確率のみ出力\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "                #==========================================================================================================================================\n",
    "\n",
    "                #[1]の正答率を見る\n",
    "                pred_test_df=pd.DataFrame({'pred_proba':predict_y_test#確率分布での出力\n",
    "                                          , 'test':target_y_test})\n",
    "                #pred_final_test_df=pd.DataFrame({'pred_proba':predict_y_final#確率分布での出力\n",
    "                #                          , 'test':target_y_final})\n",
    "\n",
    "\n",
    "                th_arr=[0.5,0.54,0.58]\n",
    "                for th in th_arr:\n",
    "                    trans_test_df=pred_th_trans(pred_test_df,th)\n",
    "                    #trans_final_test_df=pred_th_trans(pred_final_test_df,th)\n",
    "                    #num_1=len(trans_df[trans_df['test']==1])\n",
    "                    #追加　配当金の情報も考慮する。\n",
    "                    count_test=0\n",
    "                    gain_index=0\n",
    "                    model_test_gain_arr=[0]*len(test_df)\n",
    "                    test_hit_arr=[0]*len(test_df)#あたっているかあたっていないかのフラグを格納した配列\n",
    "                    for _, s in trans_test_df.iterrows():\n",
    "                        if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "                            count_test+=1#的中回数\n",
    "                            model_test_gain_arr[gain_index]=test_gain_arr[gain_index]\n",
    "                            test_hit_arr[gain_index]=1\n",
    "                        gain_index+=1\n",
    "\n",
    "#                     count_final=0\n",
    "#                     gain_index=0\n",
    "#                     model_final_test_gain_arr=[0]*len(final_test_df)\n",
    "#                     final_test_hit_arr=[0]*len(final_test_df)#あたっているかあたっていないかのフラグを格納した配列\n",
    "#                     for _, s in trans_final_test_df.iterrows():\n",
    "#                         if ((s['pred']==1) and (s['test']==1)):#もし購買しているかつ的中をしていたら・・・\n",
    "#                             count_final+=1#的中回数\n",
    "#                             model_final_test_gain_arr[gain_index]=final_test_gain_arr[gain_index]\n",
    "#                             final_test_hit_arr[gain_index]=1\n",
    "#                         gain_index+=1\n",
    "    #                     gain_arr[index]=sum(model_gain_arr)\n",
    "    #                     accuracy_arr[index]=(count/num_1)*100\n",
    "                    #=================================================\n",
    "                    try:\n",
    "                        pred_0[index]=trans_df['pred'].value_counts()[0]\n",
    "                    except:\n",
    "                        pred_0[index]=0\n",
    "\n",
    "                    #V4から増やした集計(月での集計を加えて，comごとでの安定性について確認する)\n",
    "                    test_analysis_df=trans_test_df.copy()#集計結果を格納するためのdf\n",
    "                    #final_test_analysis_df=trans_final_test_df.copy()#集計結果を格納するためのdf\n",
    "                    #集計のための情報を結合する\n",
    "                    test_analysis_df['hit_flag']=test_hit_arr#的中時に１のフラグを結合する\n",
    "                    test_analysis_df['gain']=model_test_gain_arr#的中して得られたゲインの金額を格納した配列\n",
    "                    test_analysis_df['date']=test_dates#月ごとの分析のために日付のデータを結合する\n",
    "                    test_analysis_df=get_event_info(test_analysis_df)#開催情報を取得\n",
    "#                     final_test_analysis_df['hit_flag']=final_test_hit_arr#的中時に１のフラグを結合する\n",
    "#                     final_test_analysis_df['gain']=model_final_test_gain_arr#的中して得られたゲインの金額を格納した配列\n",
    "#                     final_test_analysis_df['date']=final_test_dates#月ごとの分析のために日付のデータを結合する\n",
    "#                     final_test_analysis_df=get_event_info(final_test_analysis_df)#開催情報を取得\n",
    "\n",
    "                    test_m_analysis=calc_monthly_analysis(test_analysis_df)#月別の分析結果を取得\n",
    "                    #final_test_m_analysis=calc_monthly_analysis(final_test_analysis_df)#月別の分析結果を取得\n",
    "                    test_m_desc=test_m_analysis.describe()\n",
    "                    #final_test_m_desc=final_test_m_analysis.describe()\n",
    "                    #scoreのseriesに情報書き込み==================\n",
    "                    model_score_s['threshold']=th\n",
    "                    #テストデータ=============================================================\n",
    "                    #総収益\n",
    "                    model_score_s['total_get_test']=sum(model_test_gain_arr)\n",
    "                    #投資金額\n",
    "                    model_score_s['total_use_test']=100*trans_test_df['pred'].sum()\n",
    "                    #出現数\n",
    "                    model_score_s['num_com_test']=sum(target_y_test)\n",
    "                    #購買予測数\n",
    "                    model_score_s['num_pred_test']=trans_test_df['pred'].sum()\n",
    "                    #利益率\n",
    "                    model_score_s['gain_test']=(model_score_s['total_get_test']/model_score_s['total_use_test'])*100\n",
    "                    #利益率の標準偏差\n",
    "                    model_score_s['gain_std_test']=test_m_desc.loc['std','income_per']\n",
    "                    #的中数\n",
    "                    model_score_s['num_hit_test']=count_test\n",
    "                    #購買的中率\n",
    "                    model_score_s['buy_hit_per_test']=(count_test/trans_test_df['pred'].sum())*100\n",
    "                    #購買的中率の標準偏差\n",
    "                    model_score_s['buy_hit_per_std_test']=test_m_desc.loc['std','buy_hit_per']\n",
    "                    #配当がプラスになった月の数\n",
    "                    model_score_s['plus_month_num_test']=len(test_m_analysis[test_m_analysis['income']>0])\n",
    "                    #得られた配当の中央値と平均の差(中央値-平均，つまりマイナスが大きいほど高い配当が引っ張っている)\n",
    "                    model_score_s['diff_mea_med_test']=(test_m_desc.loc['mean','mean_income'])-(test_m_desc.loc['mean','median_income'])\n",
    "\n",
    "                    #最終テストデータ(カラム名の説明に関してはテストと同じなので割愛させていただきます)=============================================================\n",
    "#                     model_score_s['total_get_final']=sum(model_final_test_gain_arr)\n",
    "#                     model_score_s['total_use_final']=100*trans_final_test_df['pred'].sum()\n",
    "#                     model_score_s['num_com_final']=sum(target_y_final)\n",
    "#                     model_score_s['num_pred_final']=trans_final_test_df['pred'].sum()\n",
    "#                     model_score_s['gain_final']=(model_score_s['total_get_final']/model_score_s['total_use_final'])*100\n",
    "#                     model_score_s['gain_std_final']=final_test_m_desc.loc['std','income_per']\n",
    "#                     model_score_s['num_hit_final']=count_final\n",
    "#                     model_score_s['buy_hit_per_final']=(count_final/trans_final_test_df['pred'].sum())*100\n",
    "#                     model_score_s['buy_hit_per_std_final']=final_test_m_desc.loc['std','buy_hit_per']\n",
    "#                     model_score_s['plus_month_num_final']=len(final_test_m_analysis[final_test_m_analysis['income']>0])\n",
    "#                     model_score_s['diff_mea_med_final']=(final_test_m_desc.loc['mean','mean_income'])-(final_test_m_desc.loc['mean','median_income'])\n",
    "\n",
    "                    #dfに書き込み\n",
    "                    model_score_df=model_score_df.append(model_score_s,ignore_index=True)\n",
    "    #モデルの「スコアを保存\n",
    "    dir_path = \"../../../bot_database/{place_name}/model_score_{place_name}/v4_score/{V}/{place_name}_model_score_st{stDate}_{V}.csv\".format(place_name=place_name,V=version,stDate=now_ym.strftime('%Y%m%d'))#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    model_score_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twenty-switch",
   "metadata": {},
   "source": [
    "## 芦屋で良さげだったので他の会場でも同じものを作る(final区間あり版)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "monetary-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=master.get_place_master()\n",
    "place_names=[place_name for place_name in place_master.values()]#会場名のみを収納した配列\n",
    "version='V4_2'#バージョン\n",
    "# place_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-picture",
   "metadata": {},
   "source": [
    "### 12会場(suminoe)まで動かす"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "stylish-poetry",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:15:48<00:00, 419.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toda\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:15:00<00:00, 417.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edogawa\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edogawa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:15:50<00:00, 419.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heiwazima\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heiwazima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:11:45<00:00, 410.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamagawa\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamagawa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:13:28<00:00, 414.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamanako\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hamanako\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:18:56<00:00, 426.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamagori\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamagori\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:24:34<00:00, 438.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokoname\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokoname\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:26:03<00:00, 441.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tu\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:17:57<00:00, 424.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mikuni\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mikuni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:17:00<00:00, 422.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "elapsed time::118622.25692915916==================================================================================================\n"
     ]
    }
   ],
   "source": [
    "#チェック\n",
    "time_sta = time.time()\n",
    "today=datetime.date.today()#今日の日付の取得\n",
    "now_ym=get_season_date(today)#区間の始まり日付を取得する\n",
    "#now_ym=datetime.datetime(year=2021, month=1,day=1)#このスクリプトでの区間を手動で設定\n",
    "for place_name in place_names[:12]:\n",
    "    print(place_name)\n",
    "    result_filepath=\"../../../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    result_base_df=pd.read_csv(result_filepath)\n",
    "    result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "    print('st_date:',now_ym,'=======================================')\n",
    "    #now_ym=datetime.datetime(year=2020, month=1,day=1)#スタート地点(今)の年月を指定，これを基準にさかのぼっていってテスト,学習データを決める(配列の中身は[year,month])\n",
    "    range_test_m=5#テストデータに使用する月の数を指定\n",
    "    range_final_m=3#最終的な評価に使うデータの数(今はここで５となっているが実際に評価をする際に必要な数は大きい値位にしておけば後で調整ができる)\n",
    "    result_df=data_making_clustar_section_has_final(result_base_df,now_ym,range_test_m,range_final_m)#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）\n",
    "    #result_df=data_making_clustar_section(result_base_df,now_ym,range_test_m)#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）    \n",
    "    #print('result_df:',result_df['year'].value_counts())\n",
    "    #print('result_df_datemax:',result_df['date'].max())\n",
    "    #ディレクトリ要チェックや！！！（カチカチ）\n",
    "    model_score_rondom_th_section_has_final(version,place_name,result_df,now_ym,range_test_m,range_final_m)\n",
    "    #model_score_rondom_th_section(version,place_name,result_df,now_ym,range_test_m)\n",
    "# 時間計測終了\n",
    "time_end = time.time()\n",
    "# 経過時間（秒）\n",
    "tim = time_end- time_sta\n",
    "print('DONE')\n",
    "print('elapsed time::{}=================================================================================================='.format(tim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-letter",
   "metadata": {},
   "source": [
    "## 琵琶湖（11）から最後（大村）まで"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "universal-training",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biwako\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "biwako\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:11:09<00:00, 409.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suminoe\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suminoe\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:18:30<00:00, 425.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amagasaki\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amagasaki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:20:59<00:00, 430.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naruto\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naruto\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:03:36<00:00, 393.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marugame\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marugame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:27:59<00:00, 445.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kozima\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kozima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:28:28<00:00, 446.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miyazima\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "miyazima\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:20:34<00:00, 429.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokuyama\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokuyama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:34:47<00:00, 460.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simonoseki\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simonoseki\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [4:30:17<00:00, 579.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wakamatu\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wakamatu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [3:23:43<00:00, 436.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asiya\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asiya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [2:42:47<00:00, 348.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fukuoka\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fukuoka\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [2:41:54<00:00, 346.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karatu\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "karatu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [2:37:39<00:00, 337.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omura\n",
      "st_date: 2021-01-01 00:00:00 =======================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omura\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 28/28 [2:36:56<00:00, 336.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n",
      "elapsed time::163207.665756464==================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# #チェック\n",
    "# time_sta = time.time()\n",
    "# #today=datetime.date.today()#今日の日付の取得\n",
    "# #now_ym=get_season_date()today#区間の始まり日付を取得する\n",
    "# now_ym=datetime.datetime(year=2021, month=1,day=1)#このスクリプトでの区間を手動で設定\n",
    "# for place_name in place_names[10:]:\n",
    "#     print(place_name)\n",
    "#     result_filepath=\"../../..//bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "#     result_base_df=pd.read_csv(result_filepath)\n",
    "#     result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "#     print('st_date:',now_ym,'=======================================')\n",
    "#     #now_ym=datetime.datetime(year=2020, month=1,day=1)#スタート地点(今)の年月を指定，これを基準にさかのぼっていってテスト,学習データを決める(配列の中身は[year,month])\n",
    "#     range_test_m=5#テストデータに使用する月の数を指定\n",
    "#     range_final_m=3#最終的な評価に使うデータの数(今はここで５となっているが実際に評価をする際に必要な数は大きい値位にしておけば後で調整ができる)\n",
    "#     result_df=data_making_clustar_section_has_final(result_base_df,now_ym,range_test_m,range_final_m)#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）\n",
    "#     #result_df=data_making_clustar_section(result_base_df,now_ym,range_test_m)#モデル関連に使用するdfの作成関数(クラスタリングあり、モータ番号、艇番号なし)（加工関数）    \n",
    "# #     print('result_df:',result_df['year'].value_counts())\n",
    "# #     print('result_df_datemax:',result_df['date'].max())\n",
    "#     #ディレクトリ要チェックや！！！（カチカチ）\n",
    "#     model_score_rondom_th_section_has_final(version,place_name,result_df,now_ym,range_test_m,range_final_m)\n",
    "#     #model_score_rondom_th_section(version,place_name,result_df,now_ym,range_test_m)\n",
    "# # 時間計測終了\n",
    "# time_end = time.time()\n",
    "# # 経過時間（秒）\n",
    "# tim = time_end- time_sta\n",
    "# print('DONE')\n",
    "# print('elapsed time::{}=================================================================================================='.format(tim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "strong-bullet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:====================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-122298038a12>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train:===================='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#print(train_df['year'].value_counts)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'min:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'max:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test:==================='\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "#     print('train:====================')\n",
    "#     #print(train_df['year'].value_counts)\n",
    "#     print('min:',train_df['date'].min())\n",
    "#     print('max:',train_df['date'].max())\n",
    "#     print('test:===================')\n",
    "#     print('min:',test_df['date'].min())\n",
    "#     print('max:',test_df['date'].max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
