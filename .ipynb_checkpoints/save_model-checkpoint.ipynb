{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "level-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "from pandas import DataFrame\n",
    "from tqdm import tqdm\n",
    "from imblearn.under_sampling import RandomUnderSampler #アンダーサンプリング用\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "# 機械学習用\n",
    "from sklearn.cluster import KMeans #クラスタリング用\n",
    "from sklearn.ensemble import RandomForestClassifier#ランダムフォレスト\n",
    "from copy import deepcopy as cp\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "import datetime\n",
    "import os #ディレクトリ作成用\n",
    "import xgboost as xgb\n",
    "import sys\n",
    "#自作のモジュールのインポート\n",
    "#sys.path.append(\"..\")\n",
    "import module.master as master\n",
    "import module.model_select_save as select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "systematic-september",
   "metadata": {},
   "outputs": [],
   "source": [
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    #print(place[0],place[1],'\\n')\n",
    "    place_name=place[1]\n",
    "    dir_path = \"../../bot_database/{place_name}/model_score_{place_name}/\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    if os.path.exists(dir_path)==False:\n",
    "        os.makedirs(dir_path)\n",
    "    else:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-component",
   "metadata": {},
   "source": [
    "# use_model_dfの作成と保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opposite-database",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================================================\n",
      "kiryu\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.9009881500728317__test:0.8872618485277909\n",
      "CHECK::::train_precision:84.51160280626013__test_precision:78.88631090487239\n",
      "=========================================================================================================================\n",
      "toda\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.873627022558167__test:0.8401826484018264\n",
      "CHECK::::train_precision:90.87677725118483__test_precision:77.97619047619048\n",
      "=========================================================================================================================\n",
      "edogawa\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.861658989803551__test:0.8502598016060463\n",
      "CHECK::::train_precision:89.18128654970761__test_precision:83.06092124814265\n",
      "=========================================================================================================================\n",
      "heiwazima\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8900830675957639__test:0.8554558337269721\n",
      "CHECK::::train_precision:92.62672811059907__test_precision:77.27272727272727\n",
      "=========================================================================================================================\n",
      "tamagawa\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8327231211369631__test:0.7970398362462604\n",
      "CHECK::::train_precision:91.26460132046724__test_precision:76.36363636363637\n",
      "=========================================================================================================================\n",
      "hamanako\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8988622495177355__test:0.8767123287671232\n",
      "CHECK::::train_precision:89.00091659028413__test_precision:79.89756722151088\n",
      "=========================================================================================================================\n",
      "gamagori\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8523680170072044__test:0.8230199968508897\n",
      "CHECK::::train_precision:90.58866279069767__test_precision:80.42253521126761\n",
      "=========================================================================================================================\n",
      "tokoname\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8860281091295619__test:0.8562431113210518\n",
      "CHECK::::train_precision:88.56296296296297__test_precision:78.9873417721519\n",
      "=========================================================================================================================\n",
      "tu\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8855163182551868__test:0.8647457093371123\n",
      "CHECK::::train_precision:91.97733373098717__test_precision:89.87012987012987\n",
      "=========================================================================================================================\n",
      "mikuni\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8815794653753789__test:0.8444339474098567\n",
      "CHECK::::train_precision:88.99700598802394__test_precision:77.05479452054794\n",
      "=========================================================================================================================\n",
      "biwako\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.9033502618007165__test:0.8849000157455519\n",
      "CHECK::::train_precision:85.42116630669547__test_precision:73.93225331369662\n",
      "=========================================================================================================================\n",
      "suminoe\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8646903665210031__test:0.8441190363722249\n",
      "CHECK::::train_precision:93.88059701492537__test_precision:79.86111111111111\n",
      "=========================================================================================================================\n",
      "amagasaki\n",
      "input_expand:31752_____droped_expand:31750\n",
      "input:31752_____droped:31752\n",
      "train:0.8730708661417322__test:0.8516535433070866\n",
      "CHECK::::train_precision:92.01252936570086__test_precision:87.90233074361821\n",
      "=========================================================================================================================\n",
      "naruto\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8737844966733593__test:0.848527790899071\n",
      "CHECK::::train_precision:92.63836646963998__test_precision:89.33189655172413\n",
      "=========================================================================================================================\n",
      "marugame\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8975630880673989__test:0.8672649976381672\n",
      "CHECK::::train_precision:90.53591790193842__test_precision:86.3914373088685\n",
      "=========================================================================================================================\n",
      "kozima\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8872485335223023__test:0.8464808691544639\n",
      "CHECK::::train_precision:91.0546875__test_precision:79.63272120200334\n",
      "=========================================================================================================================\n",
      "miyazima\n",
      "input_expand:31752_____droped_expand:31750\n",
      "input:31752_____droped:31747\n",
      "train:0.8768503937007874__test:0.8562204724409449\n",
      "CHECK::::train_precision:92.69141531322505__test_precision:77.24867724867724\n",
      "=========================================================================================================================\n",
      "tokuyama\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8463052635723003__test:0.8097937332703511\n",
      "CHECK::::train_precision:93.7914691943128__test_precision:76.40692640692642\n",
      "=========================================================================================================================\n",
      "simonoseki\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.858155190740522__test:0.8302629507164226\n",
      "CHECK::::train_precision:89.1826923076923__test_precision:84.01937046004842\n",
      "=========================================================================================================================\n",
      "wakamatu\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8861462147159561__test:0.8549834671705243\n",
      "CHECK::::train_precision:89.76840649844452__test_precision:78.65771812080537\n",
      "=========================================================================================================================\n",
      "asiya\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31752\n",
      "train:0.8783512460139364__test:0.8449063139663046\n",
      "CHECK::::train_precision:89.61619967035554__test_precision:80.68506184586109\n",
      "=========================================================================================================================\n",
      "fukuoka\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31751\n",
      "train:0.8919333884492736__test:0.8649031648559282\n",
      "CHECK::::train_precision:91.90578158458244__test_precision:80.85106382978722\n",
      "=========================================================================================================================\n",
      "karatu\n",
      "input_expand:31752_____droped_expand:31752\n",
      "input:31752_____droped:31751\n",
      "train:0.8483524270698004__test:0.8055424342623209\n",
      "CHECK::::train_precision:90.7510173880873__test_precision:75.47169811320755\n",
      "=========================================================================================================================\n",
      "omura\n",
      "input_expand:31752_____droped_expand:31746\n",
      "input:31752_____droped:31740\n",
      "train:0.8473381634903134__test:0.82\n",
      "CHECK::::train_precision:90.33158813263526__test_precision:82.55813953488372\n"
     ]
    }
   ],
   "source": [
    "#データ格納用のディレクトリ作り\n",
    "version='V3_1'#バージョン\n",
    "years=[2018,2019,2020]#使用する三年分のデータ（古い順で！！！）\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    place_name=place[1]\n",
    "    print('=========================================================================================================================')\n",
    "    print(place_name)\n",
    "    ################################################========================================================================================================================\n",
    "    #各会場の学習データの読み込みと加工\n",
    "    expand_filepath=\"../bot_database/{place_name}/model_score_{place_name}/{place_name}_model_score_3year_expand_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    expand_score_df=pd.read_csv(expand_filepath)#3年分のデータの読み込み\n",
    "    expand_score_df=expand_score_df.drop([\"Unnamed: 0\"],axis=1)#余計な列の削除\n",
    "    input_expand_score=len(expand_score_df)#データの件数\n",
    "    expand_score_df=expand_score_df.dropna()#収益の計算ができない部分がnanになって排出されてる臭いので削除\n",
    "    after_expand_score=len(expand_score_df)#データの件数\n",
    "    \n",
    "    score_filepath=\"../bot_database/{place_name}/model_score_{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "    score_df=pd.read_csv(score_filepath)#２年分の本番予測用のデータ\n",
    "    input_score=len(score_df)#データの件数\n",
    "    score_df=score_df.drop([\"Unnamed: 0\"],axis=1)#余計な列の削除\n",
    "    score_df=score_df.drop([\"gain_year3\"],axis=1)#余計な列の削除\n",
    "    score_df=score_df.dropna()#収益の計算ができない部分がnanになって排出されてる臭いので削除\n",
    "    after_score=len(score_df)#データの件数\n",
    "    print(\"input_expand:{}_____droped_expand:{}\".format(input_expand_score,after_expand_score))\n",
    "    print(\"input:{}_____droped:{}\".format(input_score,after_score))\n",
    "    #select.model_selection_save(expand_score_df,score_df,place_name,version,th=0.75)\n",
    "    select.model_selection_save(expand_score_df,score_df,place_name,version,th=0.60)\n",
    "    #select.model_selection_save(expand_score_df,score_df,place_name,version,th=0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-austin",
   "metadata": {},
   "source": [
    "# 予測時の学習データ作成のためのクラスタリングモデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defined-parish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n",
      "toda\n",
      "edogawa\n",
      "heiwazima\n",
      "tamagawa\n",
      "hamanako\n",
      "gamagori\n",
      "tokoname\n",
      "tu\n",
      "mikuni\n",
      "biwako\n",
      "suminoe\n",
      "amagasaki\n",
      "naruto\n",
      "marugame\n",
      "kozima\n",
      "miyazima\n",
      "tokuyama\n",
      "simonoseki\n",
      "wakamatu\n",
      "asiya\n",
      "fukuoka\n",
      "karatu\n",
      "omura\n"
     ]
    }
   ],
   "source": [
    "#データ格納用のディレクトリ作り\n",
    "version='V3_1'#バージョン\n",
    "years=[2018,2019,2020]#使用する三年分のデータ（古い順で！！！）\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    place_name=place[1]\n",
    "    print(place_name)\n",
    "    ################################################========================================================================================================================\n",
    "    #各会場の学習データの読み込みと加工\n",
    "    result_filepath=\"../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    result_base_df=pd.read_csv(result_filepath)\n",
    "    result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "    select.save_clustar_model(result_base_df,place_name,version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-chester",
   "metadata": {},
   "source": [
    "# 購買予測に使うもでる（XGboost）の保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wound-navigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kiryu\n",
      "toda\n",
      "edogawa\n",
      "heiwazima\n",
      "tamagawa\n",
      "hamanako\n",
      "gamagori\n",
      "tokoname\n",
      "tu\n",
      "mikuni\n",
      "biwako\n",
      "suminoe\n",
      "amagasaki\n",
      "naruto\n",
      "marugame\n",
      "kozima\n",
      "miyazima\n",
      "tokuyama\n",
      "simonoseki\n",
      "wakamatu\n",
      "asiya\n",
      "fukuoka\n",
      "karatu\n",
      "omura\n"
     ]
    }
   ],
   "source": [
    "#データ格納用のディレクトリ作り\n",
    "version='V3_1'#バージョン\n",
    "years=[2018,2019,2020]#使用する三年分のデータ（古い順で！！！）\n",
    "\n",
    "place_master=master.get_place_master()\n",
    "for place in place_master.items():\n",
    "    place_name=place[1]\n",
    "    ################################################========================================================================================================================\n",
    "    #各会場の学習データの読み込みと加工\n",
    "    result_filepath=\"../bot_database/{place_name}/{place_name}_train/train_{place_name}.csv\".format(place_name=place_name)#作成したデータの書き込み先#使用するデータの読み込み\n",
    "    result_base_df=pd.read_csv(result_filepath)\n",
    "    result_base_df=result_base_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "    \n",
    "    use_model_filepath=dir_path =  \"../bot_database/{place_name}/model_score_{place_name}/use_model/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#選定されたモデルのリストを出力\n",
    "    use_model_df=pd.read_csv(use_model_filepath)\n",
    "    use_model_df=use_model_df.drop([\"Unnamed: 0\"],axis=1)\n",
    "    \n",
    "    \n",
    "    select.save_model_XGboost_V3_1(result_base_df,use_model_df,years,place_name,version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-tennis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "japanese-greece",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "normal-action",
   "metadata": {
    "code_folding": [
     0,
     15,
     22,
     28,
     40,
     48,
     55,
     102
    ]
   },
   "outputs": [],
   "source": [
    "def trans_result_com(target_com,trans_base_df):#comをターゲットに合わせて0,1の二値に変換する。\n",
    "    #学習データのラベル変換==========================================================\n",
    "    trans_df=trans_base_df.copy()\n",
    "    #result_train_df=trans_base_df.copy()\n",
    "    result_arr=[0]*len(trans_df)\n",
    "    i=0\n",
    "    for result in trans_df['result_com']:#\n",
    "        if ((result==target_com)):\n",
    "            result_arr[i]=1\n",
    "        else:\n",
    "            result_arr[i]=0\n",
    "        i+=1\n",
    "    trans_df['result_com']=result_arr\n",
    "    return trans_df\n",
    "\n",
    "def pred_th_trans(pred_df,th):#引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def pred_th_trans_com(pred_df,th,target_com):#指定の組のカラムのみを置換。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_{}'.format(target_com)] >= th, 'pred_{}'.format(target_com)] = 1\n",
    "    trans_df.loc[~(trans_df['pred_{}'.format(target_com)] >=  th), 'pred_{}'.format(target_com)] = 0\n",
    "    return trans_df\n",
    "\n",
    "def calc_gain(pred_gain_df):#レース単位であたっているか同課の判別と、当たった場合に得られた配当金を計算する関数\n",
    "    pred_true_df=pred_gain_df[(pred_gain_df['pred']==1)&(pred_gain_df['trans_result']==1)].copy()\n",
    "    pred_true_df['hit']=1\n",
    "    calc_base_df=pred_gain_df.copy()\n",
    "    calc_base_df['hit']=pred_true_df['hit']\n",
    "    calc_base_df['gain']=pred_true_df['money']\n",
    "    calc_base_df=calc_base_df.fillna(0)\n",
    "    #\n",
    "    #calc_base_df:予測、変換積みの結果、実際の結果、配当金、収益をすべて表したdf,合計操作は行っていない。\n",
    "    #\n",
    "    return calc_base_df\n",
    "\n",
    "def check_pred_arr(pred1_df,pred2_df):#カラムの中身が同じか比較する関数\n",
    "    pred_1_vals=[pred1_df[col] for col in pred1_df.columns]\n",
    "    pred_2_vals=[pred2_df[col] for col in pred2_df.columns]\n",
    "    for col_name1,col1,col_name2,col2 in zip(pred1_df.columns,pred_1_vals,pred2_df.columns,pred_2_vals):\n",
    "        if list(col1.values)==list(col1.values):\n",
    "            print(col_name1,'  and  ',col_name2,'  is same pred \\n')\n",
    "    return None\n",
    "\n",
    "def pred_th_trans(pred_df,th):#閾値を渡して、その値以上を1、未満を0に置き変える。\n",
    "    #引数として予測結果のdeと、変換したい閾値を渡す。\n",
    "    trans_df=pred_df.copy()\n",
    "    trans_df.loc[trans_df['pred_proba'] >= th, 'pred'] = 1\n",
    "    trans_df.loc[~(trans_df['pred_proba']  >=  th), 'pred'] = 0\n",
    "    return trans_df\n",
    "\n",
    "def add_result_class(model,score_df):#モデルでの予測結果を学習時に使用したパラメータシートに追加して返す関数\n",
    "    #out_figはオプション、グラフの出力の有り無し\n",
    "    score_sheet_base=score_df.copy()\n",
    "    score_sheet_shuffle=score_sheet_base.sample(frac=1, random_state=7)#行をシャッフル\n",
    "    num_data=len(score_sheet_shuffle)#データの件数\n",
    "\n",
    "    #前処理    これはクラス分類なので閾値で最新収益を1,0に変換する\n",
    "    score_sheet_shuffle['gain_label']=0#実際の収益の有無を確認(110を利益有り無しの閾値とする )\n",
    "    score_sheet_shuffle.loc[score_sheet_shuffle['gain_year3'] < 110, 'gain_label'] =0\n",
    "    score_sheet_shuffle.loc[score_sheet_shuffle['gain_year3'] >= 110, 'gain_label'] =1\n",
    "    ex_df=score_sheet_shuffle.copy()#gain_year3の切り抜き用\n",
    "    score_sheet_shuffle=score_sheet_shuffle.drop(['gain_year3'],axis=1).copy()\n",
    "\n",
    "    #データの分割========================================================================================================\n",
    "    test_df = score_sheet_shuffle[int(num_data*0.8):].copy()#20%のデータを検証用データに。\n",
    "    train_df =  score_sheet_shuffle[:int(num_data*0.8)].copy()#そのほかを学習データに\n",
    "    gain_s=ex_df[int(num_data*0.8):]['gain_year3'].copy()#検証用データの実収益部分を切り抜く。\n",
    "\n",
    "    test_x=test_df.drop(['gain_label'],axis=1).copy()\n",
    "    train_x=train_df.drop(['gain_label'],axis=1).copy()\n",
    "    test_y=test_df['gain_label']\n",
    "    train_y=train_df['gain_label']\n",
    "    test_pred_df=test_df.copy()#予測の結合先を作っておく\n",
    "    train_pred_df=train_df.copy()#予測の結合先を作っておく\n",
    "\n",
    "    #予測を出力(確率分布)\n",
    "    train_pred_proba_arr=model.predict_proba(train_x)\n",
    "    train_pred_proba=[arr[1] for arr in train_pred_proba_arr]#確率分布の二次元配列なので[1]の部分だけ取り出す\n",
    "    test_pred_proba_arr = model.predict_proba(test_x)\n",
    "    test_pred_proba=[arr[1] for arr in test_pred_proba_arr]#確率分布の二次元配列なので[1]の部分だけ取り出す\n",
    "    test_pred_df[\"pred_proba\"]=test_pred_proba#予測を結合\n",
    "    train_pred_df[\"pred_proba\"]=train_pred_proba#予測を結合\n",
    "\n",
    "    #確率分布の閾値で予測をバイナリに変換(仮)\n",
    "    #test_pred_df=pred_th_trans(test_pred_df,0.5)\n",
    "    #train_pred_df=pred_th_trans(train_pred_df,0.5)\n",
    "\n",
    "#     #モデルのスコアの計算\n",
    "#     col1='pred'\n",
    "#     col2='gain_label'\n",
    "#     cross_df,train_precision=making_cross(col1,col2 ,train_pred_df)\n",
    "#     cross_df,test_precision=making_cross(col1,col2 ,test_pred_df)\n",
    "#     print(\"train_precision:{}__test_precision:{}\".format(train_precision,test_precision))\n",
    "\n",
    "    test_pred_df['gain_year3']=gain_s#実際の収益の情報を追加する\n",
    "    return test_pred_df\n",
    "\n",
    "def making_cross(col1,col2,base_df,out_cross=0):#クロス収益図を作成してpurecisionを算出する関数\n",
    "    cross_df=pd.DataFrame(columns=[\"{}_1\".format(col2),\"{}_0\".format(col2),'sum']\n",
    "                         ,index=[\"{}_1\".format(col1),\"{}_0\".format(col1),'sum'])#クロス集計の結果の格納df\n",
    "    cross_df.at[\"{}_1\".format(col1), \"{}_1\".format(col2)]=len(base_df[(base_df[col1]==1) & (base_df[col2]==1)])#左上\n",
    "    cross_df.at[\"{}_1\".format(col1), \"{}_0\".format(col2)]=len(base_df[(base_df[col1]==1) & (base_df[col2]==0)])#右上\n",
    "    cross_df.at[\"{}_0\".format(col1), \"{}_1\".format(col2)]=len(base_df[(base_df[col1]==0) & (base_df[col2]==1)])#左下\n",
    "    cross_df.at[\"{}_0\".format(col1), \"{}_0\".format(col2)]=len(base_df[(base_df[col1]==0) & (base_df[col2]==0)])#右下\n",
    "\n",
    "    cross_df.at[\"{}_1\".format(col1), \"sum\"]=cross_df.at[\"{}_1\".format(col1), \"{}_1\".format(col2)]+cross_df.at[\"{}_1\".format(col1), \"{}_0\".format(col2)]\n",
    "    cross_df.at[\"{}_0\".format(col1), \"sum\"]=cross_df.at[\"{}_0\".format(col1), \"{}_1\".format(col2)]+cross_df.at[\"{}_0\".format(col1), \"{}_0\".format(col2)]\n",
    "    cross_df.at[\"sum\", \"{}_1\".format(col2)]=cross_df.at[\"{}_1\".format(col1), \"{}_1\".format(col2)]+cross_df.at[\"{}_0\".format(col1), \"{}_1\".format(col2)]\n",
    "    cross_df.at[\"sum\", \"{}_0\".format(col2)]=cross_df.at[\"{}_1\".format(col1), \"{}_0\".format(col2)]+cross_df.at[\"{}_0\".format(col1), \"{}_0\".format(col2)]\n",
    "    if out_cross==1:\n",
    "        display(cross_df)\n",
    "    #precisionを算出\n",
    "    try:\n",
    "        #precision=(cross_df.at[\"{}_1\".format(col1), \"{}_1\".format(col2)]/(cross_df.at[\"{}_1\".format(col1), \"{}_1\".format(col2)]+cross_df.at[\"{}_1\".format(col1), \"{}_0\".format(col2)]))*100\n",
    "        precision=(cross_df.at[\"{}_1\".format(col1), \"{}_1\".format(col2)]/(cross_df.at[\"{}_1\".format(col1),'sum']))*100\n",
    "    except ZeroDivisionError:\n",
    "        precision=0\n",
    "\n",
    "    return cross_df,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "framed-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_selection_save(expand_score_df,score_df,place_name,version,th=0.8):#三年分のスコアシートを与えたら[モデル選定モデル]を作成して保存、かつ実際に使用する購買予測モデルのスコアシートを吐き出してくれる関数(use_model)(モデルの保存も同時に行う。)\n",
    "#     #expand_score_df:::モデル選定のためのモデル作成の学習データ用の三年間を横に展開したスコアシート\n",
    "#     #score_df:::実際にも本番で使うモデルを選ぶためのシート（答えなし）\n",
    "#     score_sheet_base=expand_score_df.copy()\n",
    "#     score_sheet_shuffle=score_sheet_base.sample(frac=1, random_state=7)#行をシャッフル\n",
    "#     num_data=len(score_sheet_shuffle)#データの件数\n",
    "\n",
    "#     #前処理    これはクラス分類なので閾値で最新収益を1,0に変換する\n",
    "#     score_sheet_shuffle['gain_label']=0#実際の収益の有無を確認(110を利益有り無しの閾値とする )\n",
    "#     score_sheet_shuffle.loc[score_sheet_shuffle['gain_year3'] < 110, 'gain_label'] =0\n",
    "#     score_sheet_shuffle.loc[score_sheet_shuffle['gain_year3'] >= 110, 'gain_label'] =1\n",
    "#     ex_df=score_sheet_shuffle.copy()#gain_year3の切り抜き用\n",
    "#     score_sheet_shuffle=score_sheet_shuffle.drop(['gain_year3'],axis=1).copy()\n",
    "#     #選定モデルの学習、保存パート============================================================================================================================================================================\n",
    "#     #選定モデルの学習、保存パート============================================================================================================================================================================\n",
    "#     #選定モデルの学習、保存パート============================================================================================================================================================================\n",
    "#     #選定モデルの学習、保存パート============================================================================================================================================================================\n",
    "#     #選定モデルの学習、保存パート============================================================================================================================================================================\n",
    "\n",
    "#     #データの分割========================================================================================================\n",
    "#     test_df = score_sheet_shuffle[int(num_data*0.8):].copy()#20%のデータを検証用データに。\n",
    "#     train_df =  score_sheet_shuffle[:int(num_data*0.8)].copy()#そのほかを学習データに\n",
    "#     gain_s=ex_df[int(num_data*0.8):]['gain_year3'].copy()#検証用データの実収益部分を切り抜く。\n",
    "\n",
    "#     test_x=test_df.drop(['gain_label'],axis=1).copy()\n",
    "#     train_x=train_df.drop(['gain_label'],axis=1).copy()\n",
    "#     test_y=test_df['gain_label']\n",
    "#     train_y=train_df['gain_label']\n",
    "#     test_pred_df=test_df.copy()#予測の結合先を作っておく\n",
    "#     train_pred_df=train_df.copy()#予測の結合先を作っておく\n",
    "\n",
    "#     #ざっと学習\n",
    "#     rc = RandomForestClassifier(n_jobs=8, random_state=7,n_estimators=100,max_depth=10)\n",
    "#     rc.fit(train_x,train_y)\n",
    "#     pickle_path=\"../bot_database/{place_name}/model_pickle_{place_name}/model_selection_{place_name}_{V}.sav\".format(place_name=place_name,V=version)#モデルを保存\n",
    "#     #pickle_path=\"check_selection.sav\"\n",
    "#     pickle.dump(rc, open(pickle_path, \"wb\"))#モデルの保存\n",
    "#     clf=pickle.load(open(pickle_path, 'rb'))#モデルを格納読み込む\n",
    "\n",
    "#     #予測を出力(確率分布)\n",
    "#     train_pred_proba_arr=clf.predict_proba(train_x)\n",
    "#     train_pred_proba=[arr[1] for arr in train_pred_proba_arr]#確率分布の二次元配列なので[1]の部分だけ取り出す\n",
    "#     test_pred_proba_arr = clf.predict_proba(test_x)\n",
    "#     test_pred_proba=[arr[1] for arr in test_pred_proba_arr]#確率分布の二次元配列なので[1]の部分だけ取り出す\n",
    "#     test_pred_df[\"pred_proba\"]=test_pred_proba#予測を結合\n",
    "#     train_pred_df[\"pred_proba\"]=train_pred_proba#予測を結合\n",
    "\n",
    "#     #確率分布の閾値で予測をバイナリに変換(閾値はいったん決めで0.8にする(引数で簡単に変えられるからよろしくうううう))\n",
    "#     test_pred_df=pred_th_trans(test_pred_df,th)\n",
    "#     train_pred_df=pred_th_trans(train_pred_df,th)\n",
    "#     #モデルのスコアの計算\n",
    "#     train_score=rc.score(train_x, train_y)\n",
    "#     test_score=rc.score(test_x, test_y)\n",
    "#     print(\"train:{}__test:{}\".format(train_score,test_score))\n",
    "\n",
    "#     col1='pred'\n",
    "#     col2='gain_label'\n",
    "#     cross_df,train_precision=making_cross(col1,col2 ,train_pred_df)\n",
    "#     cross_df,test_precision=making_cross(col1,col2 ,test_pred_df)\n",
    "#     print(\"CHECK::::train_precision:{}__test_precision:{}\".format(train_precision,test_precision))#精度も一応算出しておく（閾値は0.8）\n",
    "\n",
    "#     #使うモデルの選定を行う。\n",
    "#     use_model_check_df=pd.DataFrame(columns=test_pred_df.columns)#最終的に使うと判定されたモデルのパラメータを格納するDF\n",
    "#     selection_df=test_pred_df.copy()\n",
    "#     selection_df['gain_year3']=gain_s#実際の収益の情報を追加する\n",
    "#     selection_df=selection_df[selection_df['pred_proba']>=th].copy()#閾値はいったん決めで区切る\n",
    "#     candidate_com=selection_df['target_com'].value_counts().index#閾値を超え、利益が出やすいと判断されたパラメータのあるcomを重複なく抜き出す\n",
    "#     for com in candidate_com:\n",
    "#         com_selection_df=selection_df[selection_df['target_com']==com].copy()\n",
    "#         com_selection_df=com_selection_df.sort_values('pred_proba', ascending=False).iloc[:1]#各組の一番probaが高かったものを残す。\n",
    "#         use_model_check_df=pd.concat([use_model_check_df, com_selection_df], axis=0)\n",
    "\n",
    "\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "#     #選定モデルを使っての本番で使うパラメータの選定パート============================================================================================================================================================================\n",
    "\n",
    "#     #expand_score_df:::モデル選定のためのモデル作成の学習データ用の三年間を横に展開したスコアシート\n",
    "#     #score_df:::実際にも本番で使うモデルを選ぶためのシート（答えなし）\n",
    "#     has_proba_score_df=score_df.copy()\n",
    "#     #予測を出力(確率分布)\n",
    "#     pred_proba_arr = clf.predict_proba(score_df)\n",
    "#     pred_proba=[arr[1] for arr in pred_proba_arr]#確率分布の二次元配列なので[1]の部分だけ取り出す\n",
    "#     has_proba_score_df[\"pred_proba\"]=pred_proba#予測を結合\n",
    "\n",
    "#     #確率分布の閾値で予測をバイナリに変換(閾値はいったん決めで0.8にする(引数で簡単に変えられるからよろしくうううう))\n",
    "#     has_proba_score_df=pred_th_trans(has_proba_score_df,th)\n",
    "\n",
    "#     #使うモデルの選定を行う。\n",
    "#     use_model_df=pd.DataFrame(columns=has_proba_score_df.columns)#最終的に使うと判定されたモデルのパラメータを格納するDF\n",
    "#     selection_df=has_proba_score_df.copy()\n",
    "#     selection_df=selection_df[selection_df['pred_proba']>=th].copy()#閾値はいったん決めで区切る\n",
    "#     candidate_com=selection_df['target_com'].value_counts().index#閾値を超え、利益が出やすいと判断されたパラメータのあるcomを重複なく抜き出す\n",
    "#     display(selection_df)\n",
    "#     for com in candidate_com:\n",
    "#         com_selection_df=selection_df[selection_df['target_com']==com].copy()\n",
    "#         com_selection_df=com_selection_df.sort_values('pred_proba', ascending=False).iloc[:1]#各組の一番probaが高かったものを残す。\n",
    "#         use_model_df=pd.concat([use_model_df, com_selection_df], axis=0)\n",
    "#     #dir_path =  \"../bot_database/{place_name}/model_score_{place_name}/use_model/use_model_{place_name}_{V}.csv\".format(place_name=place_name,V=version)#選定されたモデルのリストを出力\n",
    "#     use_model_df.to_csv(dir_path, encoding='utf_8_sig')\n",
    "\n",
    "\n",
    "#     return use_model_check_df,use_model_df#決まったパラメータでモデルを作成できてるかのチェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fabulous-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #データ格納用のディレクトリ作り\n",
    "# version='V3_1'#バージョン\n",
    "# years=[2018,2019,2020]#使用する三年分のデータ（古い順で！！！）\n",
    "\n",
    "# place_master=master.get_place_master()\n",
    "# for place in place_master.items():\n",
    "#     place_name=place[1]\n",
    "#     print('=========================================================================================================================')\n",
    "#     print(place_name)\n",
    "#     ################################################========================================================================================================================\n",
    "#     #各会場の学習データの読み込みと加工\n",
    "#     expand_filepath=\"../bot_database/{place_name}/model_score_{place_name}/{place_name}_model_score_3year_expand_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "#     expand_score_df=pd.read_csv(expand_filepath)#3年分のデータの読み込み\n",
    "#     expand_score_df=expand_score_df.drop([\"Unnamed: 0\"],axis=1)#余計な列の削除\n",
    "#     input_expand_score=len(expand_score_df)#データの件数\n",
    "#     expand_score_df=expand_score_df.dropna()#収益の計算ができない部分がnanになって排出されてる臭いので削除\n",
    "#     after_expand_score=len(expand_score_df)#データの件数\n",
    "    \n",
    "#     score_filepath=\"../bot_database/{place_name}/model_score_{place_name}/{place_name}_model_score_{V}.csv\".format(place_name=place_name,V=version)#作成したデータの書き込み先\n",
    "#     score_df=pd.read_csv(score_filepath)#２年分の本番予測用のデータ\n",
    "#     input_score=len(score_df)#データの件数\n",
    "#     score_df=score_df.drop([\"Unnamed: 0\"],axis=1)#余計な列の削除\n",
    "#     score_df=score_df.drop([\"gain_year3\"],axis=1)#余計な列の削除\n",
    "#     score_df=score_df.dropna()#収益の計算ができない部分がnanになって排出されてる臭いので削除\n",
    "#     after_score=len(score_df)#データの件数\n",
    "#     print(\"input_expand:{}_____droped_expand:{}\".format(input_expand_score,after_expand_score))\n",
    "#     print(\"input:{}_____droped:{}\".format(input_score,after_score))\n",
    "#     #select.model_selection_save(expand_score_df,score_df,place_name,version,th=0.75)\n",
    "#     model_selection_save(expand_score_df,score_df,place_name,version,th=0.65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-input",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
